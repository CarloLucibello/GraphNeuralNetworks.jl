var documenterSearchIndex = {"docs":
[{"location":"GNNlib/guides/messagepassing/#Message-Passing","page":"Message Passing","title":"Message Passing","text":"","category":"section"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"A generic message passing on graph takes the form","category":"page"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"beginaligned\nmathbfm_jto i = phi(mathbfx_i mathbfx_j mathbfe_jto i) \nbarmathbfm_i = square_jin N(i)  mathbfm_jto i \nmathbfx_i = gamma_x(mathbfx_i barmathbfm_i)\nmathbfe_jto i^prime =  gamma_e(mathbfe_j to imathbfm_j to i)\nendaligned","category":"page"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"where we refer to phi as to the message function,  and to gamma_x and gamma_e as to the node update and edge update function respectively. The aggregation square is over the neighborhood N(i) of node i,  and it is usually equal either to sum, to max or to a mean operation. ","category":"page"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"In GNNlib.jl, the message passing mechanism is exposed by the propagate function. propagate takes care of materializing the node features on each edge, applying the message function, performing the aggregation, and returning barmathbfm.  It is then left to the user to perform further node and edge updates, manipulating arrays of size D_node times num_nodes and    D_edge times num_edges.","category":"page"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"propagate is composed of two steps, also available as two independent methods:","category":"page"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"apply_edges materializes node features on edges and applies the message function. \naggregate_neighbors applies a reduction operator on the messages coming from the neighborhood of each node.","category":"page"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"The whole propagation mechanism internally relies on the NNlib.gather  and NNlib.scatter methods.","category":"page"},{"location":"GNNlib/guides/messagepassing/#Examples","page":"Message Passing","title":"Examples","text":"","category":"section"},{"location":"GNNlib/guides/messagepassing/#Basic-use-of-apply_edges-and-propagate","page":"Message Passing","title":"Basic use of apply_edges and propagate","text":"","category":"section"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"The function apply_edges can be used to broadcast node data on each edge and produce new edge data.","category":"page"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"julia> using GNNlib, Graphs, Statistics\n\njulia> g = rand_graph(10, 20)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 20\n\njulia> x = ones(2,10);\n\njulia> z = 2ones(2,10);\n\n# Return an edge features arrays (D × num_edges)\njulia> apply_edges((xi, xj, e) -> xi .+ xj, g, xi=x, xj=z)\n2×20 Matrix{Float64}:\n 3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0\n 3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0\n\n# now returning a named tuple\njulia> apply_edges((xi, xj, e) -> (a=xi .+ xj, b=xi .- xj), g, xi=x, xj=z)\n(a = [3.0 3.0 … 3.0 3.0; 3.0 3.0 … 3.0 3.0], b = [-1.0 -1.0 … -1.0 -1.0; -1.0 -1.0 … -1.0 -1.0])\n\n# Here we provide a named tuple input\njulia> apply_edges((xi, xj, e) -> xi.a + xi.b .* xj, g, xi=(a=x,b=z), xj=z)\n2×20 Matrix{Float64}:\n 5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0\n 5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0","category":"page"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"The function propagate instead performs the apply_edges operation but then also applies a reduction over each node's neighborhood (see aggregate_neighbors).","category":"page"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"julia> propagate((xi, xj, e) -> xi .+ xj, g, +, xi=x, xj=z)\n2×10 Matrix{Float64}:\n 3.0  6.0  9.0  9.0  0.0  6.0  6.0  3.0  15.0  3.0\n 3.0  6.0  9.0  9.0  0.0  6.0  6.0  3.0  15.0  3.0\n\n# Previous output can be understood by looking at the degree\njulia> degree(g)\n10-element Vector{Int64}:\n 1\n 2\n 3\n 3\n 0\n 2\n 2\n 1\n 5\n 1","category":"page"},{"location":"GNNlib/guides/messagepassing/#Implementing-a-custom-Graph-Convolutional-Layer-using-Flux.jl","page":"Message Passing","title":"Implementing a custom Graph Convolutional Layer using Flux.jl","text":"","category":"section"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"Let's implement a simple graph convolutional layer using the message passing framework using the machine learning framework Flux.jl. The convolution reads ","category":"page"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"mathbfx_i = W cdot sum_j in N(i)  mathbfx_j","category":"page"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"We will also add a bias and an activation function.","category":"page"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"using Flux, Graphs, GraphNeuralNetworks\n\nstruct GCN{A<:AbstractMatrix, B, F} <: GNNLayer\n    weight::A\n    bias::B\n    σ::F\nend\n\nFlux.@layer GCN # allow gpu movement, select trainable params etc...\n\nfunction GCN(ch::Pair{Int,Int}, σ=identity)\n    in, out = ch\n    W = Flux.glorot_uniform(out, in)\n    b = zeros(Float32, out)\n    GCN(W, b, σ)\nend\n\nfunction (l::GCN)(g::GNNGraph, x::AbstractMatrix{T}) where T\n    @assert size(x, 2) == g.num_nodes\n\n    # Computes messages from source/neighbour nodes (j) to target/root nodes (i).\n    # The message function will have to handle matrices of size (*, num_edges).\n    # In this simple case we just let the neighbor features go through.\n    message(xi, xj, e) = xj \n\n    # The + operator gives the sum aggregation.\n    # `mean`, `max`, `min`, and `*` are other possibilities.\n    x = propagate(message, g, +, xj=x) \n\n    return l.σ.(l.weight * x .+ l.bias)\nend","category":"page"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"See the GATConv implementation here for a more complex example.","category":"page"},{"location":"GNNlib/guides/messagepassing/#Built-in-message-functions","page":"Message Passing","title":"Built-in message functions","text":"","category":"section"},{"location":"GNNlib/guides/messagepassing/","page":"Message Passing","title":"Message Passing","text":"In order to exploit optimized specializations of the propagate, it is recommended  to use built-in message functions such as copy_xj whenever possible. ","category":"page"},{"location":"GNNGraphs/api/temporalgraph/","page":"TemporalSnapshotsGNNGraph","title":"TemporalSnapshotsGNNGraph","text":"CurrentModule = GNNGraphs\nCollapsedDocStrings = true","category":"page"},{"location":"GNNGraphs/api/temporalgraph/#Temporal-Graphs","page":"TemporalSnapshotsGNNGraph","title":"Temporal Graphs","text":"","category":"section"},{"location":"GNNGraphs/api/temporalgraph/#TemporalSnapshotsGNNGraph","page":"TemporalSnapshotsGNNGraph","title":"TemporalSnapshotsGNNGraph","text":"","category":"section"},{"location":"GNNGraphs/api/temporalgraph/","page":"TemporalSnapshotsGNNGraph","title":"TemporalSnapshotsGNNGraph","text":"Documentation page for the graph type TemporalSnapshotsGNNGraph and related methods, representing time varying graphs with time varying features.","category":"page"},{"location":"GNNGraphs/api/temporalgraph/","page":"TemporalSnapshotsGNNGraph","title":"TemporalSnapshotsGNNGraph","text":"Modules = [GNNGraphs]\nPages   = [\"temporalsnapshotsgnngraph.jl\"]\nPrivate = false","category":"page"},{"location":"GNNGraphs/api/temporalgraph/#GNNGraphs.TemporalSnapshotsGNNGraph","page":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.TemporalSnapshotsGNNGraph","text":"TemporalSnapshotsGNNGraph(snapshots::AbstractVector{<:GNNGraph})\n\nA type representing a temporal graph as a sequence of snapshots. In this case a snapshot is a GNNGraph.\n\nTemporalSnapshotsGNNGraph can store the feature array associated to the graph itself as a DataStore object,  and it uses the DataStore objects of each snapshot for the node and edge features. The features can be passed at construction time or added later.\n\nConstructor Arguments\n\nsnapshot: a vector of snapshots, where each snapshot must have the same number of nodes.\n\nExamples\n\njulia> using GNNGraphs\n\njulia> snapshots = [rand_graph(10,20) for i in 1:5];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [20, 20, 20, 20, 20]\n  num_snapshots: 5\n\njulia> tg.tgdata.x = rand(4); # add temporal graph feature\n\njulia> tg # show temporal graph with new feature\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [20, 20, 20, 20, 20]\n  num_snapshots: 5\n  tgdata:\n        x = 4-element Vector{Float64}\n\n\n\n\n\n","category":"type"},{"location":"GNNGraphs/api/temporalgraph/#GNNGraphs.add_snapshot-Tuple{TemporalSnapshotsGNNGraph, Int64, GNNGraph}","page":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.add_snapshot","text":"add_snapshot(tg::TemporalSnapshotsGNNGraph, t::Int, g::GNNGraph)\n\nReturn a TemporalSnapshotsGNNGraph created starting from tg by adding the snapshot g at time index t.\n\nExamples\n\njulia> using GNNGraphs\n\njulia> snapshots = [rand_graph(10, 20) for i in 1:5];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [20, 20, 20, 20, 20]\n  num_snapshots: 5\n\njulia> new_tg = add_snapshot(tg, 3, rand_graph(10, 16)) # add a new snapshot at time 3\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10, 10]\n  num_edges: [20, 20, 16, 20, 20, 20]\n  num_snapshots: 6\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/temporalgraph/#GNNGraphs.remove_snapshot-Tuple{TemporalSnapshotsGNNGraph, Int64}","page":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.remove_snapshot","text":"remove_snapshot(tg::TemporalSnapshotsGNNGraph, t::Int)\n\nReturn a TemporalSnapshotsGNNGraph created starting from tg by removing the snapshot at time index t.\n\nExamples\n\njulia> using GNNGraphs\n\njulia> snapshots = [rand_graph(10,20), rand_graph(10,14), rand_graph(10,22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> new_tg = remove_snapshot(tg, 2) # remove snapshot at time 2\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10]\n  num_edges: [20, 22]\n  num_snapshots: 2\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/temporalgraph/#Random-Generators","page":"TemporalSnapshotsGNNGraph","title":"Random Generators","text":"","category":"section"},{"location":"GNNGraphs/api/temporalgraph/","page":"TemporalSnapshotsGNNGraph","title":"TemporalSnapshotsGNNGraph","text":"rand_temporal_radius_graph\nrand_temporal_hyperbolic_graph","category":"page"},{"location":"GNNGraphs/api/temporalgraph/#GNNGraphs.rand_temporal_radius_graph","page":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.rand_temporal_radius_graph","text":"rand_temporal_radius_graph(number_nodes::Int, \n                           number_snapshots::Int,\n                           speed::AbstractFloat,\n                           r::AbstractFloat;\n                           self_loops = false,\n                           dir = :in,\n                           kws...)\n\nCreate a random temporal graph given number_nodes nodes and number_snapshots snapshots. First, the positions of the nodes are randomly generated in the unit square. Two nodes are connected if their distance is less than a given radius r. Each following snapshot is obtained by applying the same construction to new positions obtained as follows. For each snapshot, the new positions of the points are determined by applying random independent displacement vectors to the previous positions. The direction of the displacement is chosen uniformly at random and its length is chosen uniformly in [0, speed]. Then the connections are recomputed. If a point happens to move outside the boundary, its position is updated as if it had bounced off the boundary.\n\nArguments\n\nnumber_nodes: The number of nodes of each snapshot.\nnumber_snapshots: The number of snapshots.\nspeed: The speed to update the nodes.\nr: The radius of connection.\nself_loops: If true, consider the node itself among its neighbors, in which               case the graph will contain self-loops. \ndir: The direction of the edges. If dir=:in edges go from the        neighbors to the central node. If dir=:out we have the opposite        direction.\nkws: Further keyword arguments will be passed to the GNNGraph constructor of each snapshot.\n\nExample\n\njulia> n, snaps, s, r = 10, 5, 0.1, 1.5;\n\njulia> tg = rand_temporal_radius_graph(n,snaps,s,r) # complete graph at each snapshot\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [90, 90, 90, 90, 90]\n  num_snapshots: 5\n\n\n\n\n\n","category":"function"},{"location":"GNNGraphs/api/temporalgraph/#GNNGraphs.rand_temporal_hyperbolic_graph","page":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.rand_temporal_hyperbolic_graph","text":"rand_temporal_hyperbolic_graph(number_nodes::Int, \n                               number_snapshots::Int;\n                               α::Real,\n                               R::Real,\n                               speed::Real,\n                               ζ::Real=1,\n                               self_loop = false,\n                               kws...)\n\nCreate a random temporal graph given number_nodes nodes and number_snapshots snapshots. First, the positions of the nodes are generated with a quasi-uniform distribution (depending on the parameter α) in hyperbolic space within a disk of radius R. Two nodes are connected if their hyperbolic distance is less than R. Each following snapshot is created in order to keep the same initial distribution.\n\nArguments\n\nnumber_nodes: The number of nodes of each snapshot.\nnumber_snapshots: The number of snapshots.\nα: The parameter that controls the position of the points. If α=ζ, the points are uniformly distributed on the disk of radius R. If α>ζ, the points are more concentrated in the center of the disk. If α<ζ, the points are more concentrated at the boundary of the disk.\nR: The radius of the disk and of connection.\nspeed: The speed to update the nodes.\nζ: The parameter that controls the curvature of the disk.\nself_loops: If true, consider the node itself among its neighbors, in which               case the graph will contain self-loops.\nkws: Further keyword arguments will be passed to the GNNGraph constructor of each snapshot.\n\nExample\n\njulia> n, snaps, α, R, speed, ζ = 10, 5, 1.0, 4.0, 0.1, 1.0;\n\njulia> thg = rand_temporal_hyperbolic_graph(n, snaps; α, R, speed, ζ)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [44, 46, 48, 42, 38]\n  num_snapshots: 5\n\nReferences\n\nSection D of the paper Dynamic Hidden-Variable Network Models and the paper  Hyperbolic Geometry of Complex Networks\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/utils/","page":"Other Operators","title":"Other Operators","text":"CurrentModule = GNNlib\nCollapsedDocStrings = true","category":"page"},{"location":"GNNlib/api/utils/#Utility-Functions","page":"Other Operators","title":"Utility Functions","text":"","category":"section"},{"location":"GNNlib/api/utils/#Graph-wise-operations","page":"Other Operators","title":"Graph-wise operations","text":"","category":"section"},{"location":"GNNlib/api/utils/","page":"Other Operators","title":"Other Operators","text":"reduce_nodes\nreduce_edges\nsoftmax_nodes\nsoftmax_edges\nbroadcast_nodes\nbroadcast_edges","category":"page"},{"location":"GNNlib/api/utils/#GNNlib.reduce_nodes","page":"Other Operators","title":"GNNlib.reduce_nodes","text":"reduce_nodes(aggr, g, x)\n\nFor a batched graph g, return the graph-wise aggregation of the node features x. The aggregation operator aggr can be +, mean, max, or min. The returned array will have last dimension g.num_graphs.\n\nSee also: reduce_edges.\n\n\n\n\n\nreduce_nodes(aggr, indicator::AbstractVector, x)\n\nReturn the graph-wise aggregation of the node features x given the graph indicator indicator. The aggregation operator aggr can be +, mean, max, or min.\n\nSee also graph_indicator.\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/utils/#GNNlib.reduce_edges","page":"Other Operators","title":"GNNlib.reduce_edges","text":"reduce_edges(aggr, g, e)\n\nFor a batched graph g, return the graph-wise aggregation of the edge features e. The aggregation operator aggr can be +, mean, max, or min. The returned array will have last dimension g.num_graphs.\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/utils/#GNNlib.softmax_nodes","page":"Other Operators","title":"GNNlib.softmax_nodes","text":"softmax_nodes(g, x)\n\nGraph-wise softmax of the node features x.\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/utils/#GNNlib.softmax_edges","page":"Other Operators","title":"GNNlib.softmax_edges","text":"softmax_edges(g, e)\n\nGraph-wise softmax of the edge features e.\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/utils/#GNNlib.broadcast_nodes","page":"Other Operators","title":"GNNlib.broadcast_nodes","text":"broadcast_nodes(g, x)\n\nGraph-wise broadcast array x of size (*, g.num_graphs)  to size (*, g.num_nodes).\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/utils/#GNNlib.broadcast_edges","page":"Other Operators","title":"GNNlib.broadcast_edges","text":"broadcast_edges(g, x)\n\nGraph-wise broadcast array x of size (*, g.num_graphs)  to size (*, g.num_edges).\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/utils/#Neighborhood-operations","page":"Other Operators","title":"Neighborhood operations","text":"","category":"section"},{"location":"GNNlib/api/utils/","page":"Other Operators","title":"Other Operators","text":"softmax_edge_neighbors","category":"page"},{"location":"GNNlib/api/utils/#GNNlib.softmax_edge_neighbors","page":"Other Operators","title":"GNNlib.softmax_edge_neighbors","text":"softmax_edge_neighbors(g, e)\n\nSoftmax over each node's neighborhood of the edge features e.\n\nmathbfe_jto i = frace^mathbfe_jto i\n                    sum_jin N(i) e^mathbfe_jto i\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/utils/#NNlib's-gather-and-scatter-functions","page":"Other Operators","title":"NNlib's gather and scatter functions","text":"","category":"section"},{"location":"GNNlib/api/utils/","page":"Other Operators","title":"Other Operators","text":"Primitive functions for message passing implemented in NNlib.jl:","category":"page"},{"location":"GNNlib/api/utils/","page":"Other Operators","title":"Other Operators","text":"gather!\ngather\nscatter!\nscatter","category":"page"},{"location":"GNNGraphs/api/samplers/","page":"Samplers","title":"Samplers","text":"CurrentModule = GNNGraphs\nCollapsedDocStrings = true","category":"page"},{"location":"GNNGraphs/api/samplers/#Samplers","page":"Samplers","title":"Samplers","text":"","category":"section"},{"location":"GNNGraphs/api/samplers/","page":"Samplers","title":"Samplers","text":"Modules = [GNNGraphs]\nPages   = [\"samplers.jl\"]\nPrivate = false","category":"page"},{"location":"GNNGraphs/api/samplers/#GNNGraphs.NeighborLoader","page":"Samplers","title":"GNNGraphs.NeighborLoader","text":"NeighborLoader(graph; num_neighbors, input_nodes, num_layers, [batch_size])\n\nA data structure for sampling neighbors from a graph for training Graph Neural Networks (GNNs).  It supports multi-layer sampling of neighbors for a batch of input nodes, useful for mini-batch training originally introduced in [\"Inductive Representation Learning on Large Graphs\"}(https://arxiv.org/abs/1706.02216) paper.\n\nFields\n\ngraph::GNNGraph: The input graph.\nnum_neighbors::Vector{Int}: A vector specifying the number of neighbors to sample per node at each GNN layer.\ninput_nodes::Vector{Int}: A vector containing the starting nodes for neighbor sampling.\nnum_layers::Int: The number of layers for neighborhood expansion (how far to sample neighbors).\nbatch_size::Union{Int, Nothing}: The size of the batch. If not specified, it defaults to the number of input_nodes.\n\nExamples\n\njulia> loader = NeighborLoader(graph; num_neighbors=[10, 5], input_nodes=[1, 2, 3], num_layers=2)\n\njulia> batch_counter = 0\n\njulia> for mini_batch_gnn in loader\n            batch_counter += 1\n            println(\"Batch \", batch_counter, \": Nodes in mini-batch graph: \", nv(mini_batch_gnn))\n        end\n\n\n\n\n\n","category":"type"},{"location":"GNNGraphs/guides/temporalgraph/","page":"Temporal Graphs","title":"Temporal Graphs","text":"CurrentModule = GNNGraphs","category":"page"},{"location":"GNNGraphs/guides/temporalgraph/#Temporal-Graphs","page":"Temporal Graphs","title":"Temporal Graphs","text":"","category":"section"},{"location":"GNNGraphs/guides/temporalgraph/","page":"Temporal Graphs","title":"Temporal Graphs","text":"Temporal Graphs are graphs with time varying topologies and  features. In GNNGraphs.jl, temporal graphs with fixed number of nodes over time are supported by the TemporalSnapshotsGNNGraph type.","category":"page"},{"location":"GNNGraphs/guides/temporalgraph/#Creating-a-TemporalSnapshotsGNNGraph","page":"Temporal Graphs","title":"Creating a TemporalSnapshotsGNNGraph","text":"","category":"section"},{"location":"GNNGraphs/guides/temporalgraph/","page":"Temporal Graphs","title":"Temporal Graphs","text":"A temporal graph can be created by passing a list of snapshots to the constructor. Each snapshot is a GNNGraph. ","category":"page"},{"location":"GNNGraphs/guides/temporalgraph/","page":"Temporal Graphs","title":"Temporal Graphs","text":"julia> using GNNGraphs\n\njulia> snapshots = [rand_graph(10,20) for i in 1:5];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [20, 20, 20, 20, 20]\n  num_snapshots: 5","category":"page"},{"location":"GNNGraphs/guides/temporalgraph/","page":"Temporal Graphs","title":"Temporal Graphs","text":"A new temporal graph can be created by adding or removing snapshots to an existing temporal graph. ","category":"page"},{"location":"GNNGraphs/guides/temporalgraph/","page":"Temporal Graphs","title":"Temporal Graphs","text":"julia> new_tg = add_snapshot(tg, 3, rand_graph(10, 16)) # add a new snapshot at time 3\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10, 10]\n  num_edges: [20, 20, 16, 20, 20, 20]\n  num_snapshots: 6","category":"page"},{"location":"GNNGraphs/guides/temporalgraph/","page":"Temporal Graphs","title":"Temporal Graphs","text":"julia> snapshots = [rand_graph(10,20), rand_graph(10,14), rand_graph(10,22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> new_tg = remove_snapshot(tg, 2) # remove snapshot at time 2\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10]\n  num_edges: [20, 22]\n  num_snapshots: 2","category":"page"},{"location":"GNNGraphs/guides/temporalgraph/","page":"Temporal Graphs","title":"Temporal Graphs","text":"See rand_temporal_radius_graph and rand_temporal_hyperbolic_graph for generating random temporal graphs. ","category":"page"},{"location":"GNNGraphs/guides/temporalgraph/","page":"Temporal Graphs","title":"Temporal Graphs","text":"julia> tg = rand_temporal_radius_graph(10, 3, 0.1, 0.5)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [32, 30, 34]\n  num_snapshots: 3","category":"page"},{"location":"GNNGraphs/guides/temporalgraph/#Basic-Queries","page":"Temporal Graphs","title":"Basic Queries","text":"","category":"section"},{"location":"GNNGraphs/guides/temporalgraph/","page":"Temporal Graphs","title":"Temporal Graphs","text":"Basic queries are similar to those for GNNGraphs:","category":"page"},{"location":"GNNGraphs/guides/temporalgraph/","page":"Temporal Graphs","title":"Temporal Graphs","text":"julia> snapshots = [rand_graph(10,20), rand_graph(10,14), rand_graph(10,22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> tg.num_nodes         # number of nodes in each snapshot\n3-element Vector{Int64}:\n 10\n 10\n 10\n\njulia> tg.num_edges         # number of edges in each snapshot\n3-element Vector{Int64}:\n 20\n 14\n 22\n\njulia> tg.num_snapshots     # number of snapshots\n3\n\njulia> tg.snapshots         # list of snapshots\n3-element Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}:\n GNNGraph(10, 20) with no data\n GNNGraph(10, 14) with no data\n GNNGraph(10, 22) with no data\n\njulia> tg.snapshots[1]      # first snapshot, same as tg[1]\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20","category":"page"},{"location":"GNNGraphs/guides/temporalgraph/#Data-Features","page":"Temporal Graphs","title":"Data Features","text":"","category":"section"},{"location":"GNNGraphs/guides/temporalgraph/","page":"Temporal Graphs","title":"Temporal Graphs","text":"A temporal graph can store global feature for the entire time series in the tgdata filed. Also, each snapshot can store node, edge, and graph features in the ndata, edata, and gdata fields, respectively. ","category":"page"},{"location":"GNNGraphs/guides/temporalgraph/","page":"Temporal Graphs","title":"Temporal Graphs","text":"julia> snapshots = [rand_graph(10, 20; ndata = rand(Float32, 3, 10)), \n                    rand_graph(10, 14; ndata = rand(Float32, 4, 10)), \n                    rand_graph(10, 22; ndata = rand(Float32, 5, 10))]; # node features at construction time\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots);\n\njulia> tg.tgdata.y = rand(Float32, 3, 1); # add global features after construction\n\njulia> tg\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n  tgdata:\n        y = 3×1 Matrix{Float32}\n\njulia> tg.ndata # vector of DataStore containing node features for each snapshot\n3-element Vector{DataStore}:\n DataStore(10) with 1 element:\n  x = 3×10 Matrix{Float32}\n DataStore(10) with 1 element:\n  x = 4×10 Matrix{Float32}\n DataStore(10) with 1 element:\n  x = 5×10 Matrix{Float32}\n\njulia> [ds.x for ds in tg.ndata]; # vector containing the x feature of each snapshot\n\njulia> [g.x for g in tg.snapshots]; # same vector as above, now accessing \n                                   # the x feature directly from the snapshots","category":"page"},{"location":"GNNlib/api/messagepassing/","page":"Message Passing","title":"Message Passing","text":"CurrentModule = GNNlib\nCollapsedDocStrings = true","category":"page"},{"location":"GNNlib/api/messagepassing/#Message-Passing","page":"Message Passing","title":"Message Passing","text":"","category":"section"},{"location":"GNNlib/api/messagepassing/#Interface","page":"Message Passing","title":"Interface","text":"","category":"section"},{"location":"GNNlib/api/messagepassing/","page":"Message Passing","title":"Message Passing","text":"GNNlib.apply_edges\nGNNlib.aggregate_neighbors\nGNNlib.propagate","category":"page"},{"location":"GNNlib/api/messagepassing/#GNNlib.apply_edges","page":"Message Passing","title":"GNNlib.apply_edges","text":"apply_edges(fmsg, g; [xi, xj, e])\napply_edges(fmsg, g, xi, xj, e=nothing)\n\nReturns the message from node j to node i applying the message function fmsg on the edges in graph g. In the message-passing scheme, the incoming messages  from the neighborhood of i will later be aggregated in order to update the features of node i (see aggregate_neighbors).\n\nThe function fmsg operates on batches of edges, therefore xi, xj, and e are tensors whose last dimension is the batch size, or can be named tuples of  such tensors.\n\nArguments\n\ng: An AbstractGNNGraph.\nxi: An array or a named tuple containing arrays whose last dimension's size        is g.num_nodes. It will be appropriately materialized on the       target node of each edge (see also edge_index).\nxj: As xi, but now to be materialized on each edge's source node. \ne: An array or a named tuple containing arrays whose last dimension's size is g.num_edges.\nfmsg: A function that takes as inputs the edge-materialized xi, xj, and e.      These are arrays (or named tuples of arrays) whose last dimension' size is the size of      a batch of edges. The output of f has to be an array (or a named tuple of arrays)      with the same batch size. If also layer is passed to propagate,     the signature of fmsg has to be fmsg(layer, xi, xj, e)      instead of fmsg(xi, xj, e).\n\nSee also propagate and aggregate_neighbors.\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/messagepassing/#GNNlib.aggregate_neighbors","page":"Message Passing","title":"GNNlib.aggregate_neighbors","text":"aggregate_neighbors(g, aggr, m)\n\nGiven a graph g, edge features m, and an aggregation operator aggr (e.g +, min, max, mean), returns the new node features \n\nmathbfx_i = square_j in mathcalN(i) mathbfm_jto i\n\nNeighborhood aggregation is the second step of propagate,  where it comes after apply_edges.\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/messagepassing/#GNNlib.propagate","page":"Message Passing","title":"GNNlib.propagate","text":"propagate(fmsg, g, aggr; [xi, xj, e])\npropagate(fmsg, g, aggr xi, xj, e=nothing)\n\nPerforms message passing on graph g. Takes care of materializing the node features on each edge,  applying the message function fmsg, and returning an aggregated message barmathbfm  (depending on the return value of fmsg, an array or a named tuple of  arrays with last dimension's size g.num_nodes).\n\nIt can be decomposed in two steps:\n\nm = apply_edges(fmsg, g, xi, xj, e)\nm̄ = aggregate_neighbors(g, aggr, m)\n\nGNN layers typically call propagate in their forward pass, providing as input f a closure.  \n\nArguments\n\ng: A GNNGraph.\nxi: An array or a named tuple containing arrays whose last dimension's size        is g.num_nodes. It will be appropriately materialized on the       target node of each edge (see also edge_index).\nxj: As xj, but to be materialized on edges' sources. \ne: An array or a named tuple containing arrays whose last dimension's size is g.num_edges.\nfmsg: A generic function that will be passed over to apply_edges.      Has to take as inputs the edge-materialized xi, xj, and e      (arrays or named tuples of arrays whose last dimension' size is the size of      a batch of edges). Its output has to be an array or a named tuple of arrays     with the same batch size. If also layer is passed to propagate,     the signature of fmsg has to be fmsg(layer, xi, xj, e)      instead of fmsg(xi, xj, e).\naggr: Neighborhood aggregation operator. Use +, mean, max, or min. \n\nExamples\n\nusing GraphNeuralNetworks, Flux\n\nstruct GNNConv <: GNNLayer\n    W\n    b\n    σ\nend\n\nFlux.@layer GNNConv\n\nfunction GNNConv(ch::Pair{Int,Int}, σ=identity)\n    in, out = ch\n    W = Flux.glorot_uniform(out, in)\n    b = zeros(Float32, out)\n    GNNConv(W, b, σ)\nend\n\nfunction (l::GNNConv)(g::GNNGraph, x::AbstractMatrix)\n    message(xi, xj, e) = l.W * xj\n    m̄ = propagate(message, g, +, xj=x)\n    return l.σ.(m̄ .+ l.bias)\nend\n\nl = GNNConv(10 => 20)\nl(g, x)\n\nSee also apply_edges and aggregate_neighbors.\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/messagepassing/#Built-in-message-functions","page":"Message Passing","title":"Built-in message functions","text":"","category":"section"},{"location":"GNNlib/api/messagepassing/","page":"Message Passing","title":"Message Passing","text":"GNNlib.copy_xi\nGNNlib.copy_xj\nGNNlib.xi_dot_xj\nGNNlib.xi_sub_xj\nGNNlib.xj_sub_xi\nGNNlib.e_mul_xj\nGNNlib.w_mul_xj","category":"page"},{"location":"GNNlib/api/messagepassing/#GNNlib.copy_xi","page":"Message Passing","title":"GNNlib.copy_xi","text":"copy_xi(xi, xj, e) = xi\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/messagepassing/#GNNlib.copy_xj","page":"Message Passing","title":"GNNlib.copy_xj","text":"copy_xj(xi, xj, e) = xj\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/messagepassing/#GNNlib.xi_dot_xj","page":"Message Passing","title":"GNNlib.xi_dot_xj","text":"xi_dot_xj(xi, xj, e) = sum(xi .* xj, dims=1)\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/messagepassing/#GNNlib.xi_sub_xj","page":"Message Passing","title":"GNNlib.xi_sub_xj","text":"xi_sub_xj(xi, xj, e) = xi .- xj\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/messagepassing/#GNNlib.xj_sub_xi","page":"Message Passing","title":"GNNlib.xj_sub_xi","text":"xj_sub_xi(xi, xj, e) = xj .- xi\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/messagepassing/#GNNlib.e_mul_xj","page":"Message Passing","title":"GNNlib.e_mul_xj","text":"e_mul_xj(xi, xj, e) = reshape(e, (...)) .* xj\n\nReshape e into a broadcast compatible shape with xj (by prepending singleton dimensions) then perform broadcasted multiplication.\n\n\n\n\n\n","category":"function"},{"location":"GNNlib/api/messagepassing/#GNNlib.w_mul_xj","page":"Message Passing","title":"GNNlib.w_mul_xj","text":"w_mul_xj(xi, xj, w) = reshape(w, (...)) .* xj\n\nSimilar to e_mul_xj but specialized on scalar edge features (weights).\n\n\n\n\n\n","category":"function"},{"location":"GNNGraphs/api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"CurrentModule = GNNGraphs\nCollapsedDocStrings = true","category":"page"},{"location":"GNNGraphs/api/gnngraph/#GNNGraph","page":"GNNGraph","title":"GNNGraph","text":"","category":"section"},{"location":"GNNGraphs/api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Documentation page for the graph type GNNGraph provided by GNNGraphs.jl and related methods. ","category":"page"},{"location":"GNNGraphs/api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Besides the methods documented here, one can rely on the large set of functionalities given by Graphs.jl thanks to the fact that GNNGraph inherits from Graphs.AbstractGraph.","category":"page"},{"location":"GNNGraphs/api/gnngraph/#GNNGraph-type","page":"GNNGraph","title":"GNNGraph type","text":"","category":"section"},{"location":"GNNGraphs/api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"GNNGraph\nBase.copy","category":"page"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.GNNGraph","page":"GNNGraph","title":"GNNGraphs.GNNGraph","text":"GNNGraph(data; [graph_type, ndata, edata, gdata, num_nodes, graph_indicator, dir])\nGNNGraph(g::GNNGraph; [ndata, edata, gdata])\n\nA type representing a graph structure that also stores feature arrays associated to nodes, edges, and the graph itself.\n\nThe feature arrays are stored in the fields ndata, edata, and gdata as DataStore objects offering a convenient dictionary-like  and namedtuple-like interface. The features can be passed at construction time or added later.\n\nA GNNGraph can be constructed out of different data objects expressing the connections inside the graph. The internal representation type is determined by graph_type.\n\nWhen constructed from another GNNGraph, the internal graph representation is preserved and shared. The node/edge/graph features are retained as well, unless explicitely set by the keyword arguments ndata, edata, and gdata.\n\nA GNNGraph can also represent multiple graphs batched togheter (see MLUtils.batch or SparseArrays.blockdiag). The field g.graph_indicator contains the graph membership of each node.\n\nGNNGraphs are always directed graphs, therefore each edge is defined by a source node and a target node (see edge_index). Self loops (edges connecting a node to itself) and multiple edges (more than one edge between the same pair of nodes) are supported.\n\nA GNNGraph is a Graphs.jl's AbstractGraph, therefore it supports most functionality from that library.\n\nArguments\n\ndata: Some data representing the graph topology. Possible type are\nAn adjacency matrix\nAn adjacency list.\nA tuple containing the source and target vectors (COO representation)\nA Graphs.jl' graph.\ngraph_type: A keyword argument that specifies               the underlying representation used by the GNNGraph.               Currently supported values are\n:coo. Graph represented as a tuple (source, target), such that the k-th edge         connects the node source[k] to node target[k].         Optionally, also edge weights can be given: (source, target, weights).\n:sparse. A sparse adjacency matrix representation.\n:dense. A dense adjacency matrix representation.\nDefaults to :coo, currently the most supported type.\ndir: The assumed edge direction when given adjacency matrix or adjacency list input data g.       Possible values are :out and :in. Default :out.\nnum_nodes: The number of nodes. If not specified, inferred from g. Default nothing.\ngraph_indicator: For batched graphs, a vector containing the graph assignment of each node. Default nothing.\nndata: Node features. An array or named tuple of arrays whose last dimension has size num_nodes.\nedata: Edge features. An array or named tuple of arrays whose last dimension has size num_edges.\ngdata: Graph features. An array or named tuple of arrays whose last dimension has size num_graphs.\n\nExamples\n\nusing GNNGraphs\n\n# Construct from adjacency list representation\ndata = [[2,3], [1,4,5], [1], [2,5], [2,4]]\ng = GNNGraph(data)\n\n# Number of nodes, edges, and batched graphs\ng.num_nodes  # 5\ng.num_edges  # 10\ng.num_graphs # 1\n\n# Same graph in COO representation\ns = [1,1,2,2,2,3,4,4,5,5]\nt = [2,3,1,4,5,3,2,5,2,4]\ng = GNNGraph(s, t)\n\n# From a Graphs' graph\ng = GNNGraph(erdos_renyi(100, 20))\n\n# Add 2 node feature arrays at creation time\ng = GNNGraph(g, ndata = (x=rand(100, g.num_nodes), y=rand(g.num_nodes)))\n\n# Add 1 edge feature array, after the graph creation\ng.edata.z = rand(16, g.num_edges)\n\n# Add node features and edge features with default names `x` and `e`\ng = GNNGraph(g, ndata = rand(100, g.num_nodes), edata = rand(16, g.num_edges))\n\ng.ndata.x # or just g.x\ng.edata.e # or just g.e\n\n# Collect edges' source and target nodes.\n# Both source and target are vectors of length num_edges\nsource, target = edge_index(g)\n\nA GNNGraph can be sent to the GPU, for example by using Flux.jl's gpu function or MLDataDevices.jl's utilities.  ```\n\n\n\n\n\n","category":"type"},{"location":"GNNGraphs/api/gnngraph/#Base.copy","page":"GNNGraph","title":"Base.copy","text":"copy(g::GNNGraph; deep=false)\n\nCreate a copy of g. If deep is true, then copy will be a deep copy (equivalent to deepcopy(g)), otherwise it will be a shallow copy with the same underlying graph data.\n\n\n\n\n\n","category":"function"},{"location":"GNNGraphs/api/gnngraph/#DataStore","page":"GNNGraph","title":"DataStore","text":"","category":"section"},{"location":"GNNGraphs/api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Modules = [GNNGraphs]\nPages   = [\"datastore.jl\"]\nPrivate = false","category":"page"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.DataStore","page":"GNNGraph","title":"GNNGraphs.DataStore","text":"DataStore([n, data])\nDataStore([n,] k1 = x1, k2 = x2, ...)\n\nA container for feature arrays. The optional argument n enforces that numobs(x) == n for each array contained in the datastore.\n\nAt construction time, the data can be provided as any iterables of pairs of symbols and arrays or as keyword arguments:\n\njulia> ds = DataStore(3, x = rand(Float32, 2, 3), y = rand(Float32, 3))\nDataStore(3) with 2 elements:\n  y = 3-element Vector{Float32}\n  x = 2×3 Matrix{Float32}\n\njulia> ds = DataStore(3, Dict(:x => rand(Float32, 2, 3), :y => rand(Float32, 3))); # equivalent to above\n\nThe DataStore has an interface similar to both dictionaries and named tuples. Arrays can be accessed and added using either the indexing or the property syntax:\n\njulia> ds = DataStore(x = ones(Float32, 2, 3), y = zeros(Float32, 3))\nDataStore() with 2 elements:\n  y = 3-element Vector{Float32}\n  x = 2×3 Matrix{Float32}\n\njulia> ds.x   # same as `ds[:x]`\n2×3 Matrix{Float32}:\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n\njulia> ds.z = zeros(Float32, 3)  # Add new feature array `z`. Same as `ds[:z] = rand(Float32, 3)`\n3-element Vector{Float32}:\n 0.0\n 0.0\n 0.0\n\nThe DataStore can be iterated over, and the keys and values can be accessed using keys(ds) and values(ds). map(f, ds) applies the function f to each feature array:\n\njulia> ds2 = map(x -> x .+ 1, ds)\nDataStore() with 3 elements:\n  y = 3-element Vector{Float32}\n  z = 3-element Vector{Float32}\n  x = 2×3 Matrix{Float32}\n\njulia> ds2.z\n3-element Vector{Float32}:\n 1.0\n 1.0\n 1.0\n\n\n\n\n\n","category":"type"},{"location":"GNNGraphs/api/gnngraph/#Query","page":"GNNGraph","title":"Query","text":"","category":"section"},{"location":"GNNGraphs/api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Modules = [GNNGraphs]\nPages   = [\"src/query.jl\"]\nPrivate = false","category":"page"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.adjacency_list-Tuple{GNNGraph, Any}","page":"GNNGraph","title":"GNNGraphs.adjacency_list","text":"adjacency_list(g; dir=:out)\nadjacency_list(g, nodes; dir=:out)\n\nReturn the adjacency list representation (a vector of vectors) of the graph g.\n\nCalling a the adjacency list, if dir=:out than a[i] will contain the neighbors of node i through outgoing edges. If dir=:in, it will contain neighbors from incoming edges instead.\n\nIf nodes is given, return the neighborhood of the nodes in nodes only.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.edge_index-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","page":"GNNGraph","title":"GNNGraphs.edge_index","text":"edge_index(g::GNNGraph)\n\nReturn a tuple containing two vectors, respectively storing  the source and target nodes for each edges in g.\n\ns, t = edge_index(g)\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.get_graph_type-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","page":"GNNGraph","title":"GNNGraphs.get_graph_type","text":"get_graph_type(g::GNNGraph)\n\nReturn the underlying representation for the graph g as a symbol.\n\nPossible values are:\n\n:coo: Coordinate list representation. The graph is stored as a tuple of vectors (s, t, w),         where s and t are the source and target nodes of the edges, and w is the edge weights.\n:sparse: Sparse matrix representation. The graph is stored as a sparse matrix representing the weighted adjacency matrix.\n:dense: Dense matrix representation. The graph is stored as a dense matrix representing the weighted adjacency matrix.\n\nThe default representation for graph constructors GNNGraphs.jl is :coo. The underlying representation can be accessed through the g.graph field.\n\nSee also GNNGraph.\n\nExamples\n\nThe default representation for graph constructors GNNGraphs.jl is :coo.\n\njulia> g = rand_graph(5, 10)\nGNNGraph:\n  num_nodes: 5\n  num_edges: 10\n\njulia> get_graph_type(g)\n:coo\n\nThe GNNGraph constructor can also be used to create graphs with different representations.\n\njulia> g = GNNGraph([2,3,5], [1,2,4], graph_type=:sparse)\nGNNGraph:\n  num_nodes: 5\n  num_edges: 3\n\njulia> g.graph\n5×5 SparseArrays.SparseMatrixCSC{Int64, Int64} with 3 stored entries:\n ⋅  ⋅  ⋅  ⋅  ⋅\n 1  ⋅  ⋅  ⋅  ⋅\n ⋅  1  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  1  ⋅\n\njulia> get_graph_type(g)\n:sparse\n\njulia> gcoo = GNNGraph(g, graph_type=:coo);\n\njulia> gcoo.graph\n([2, 3, 5], [1, 2, 4], [1, 1, 1])\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.graph_indicator-Tuple{GNNGraph}","page":"GNNGraph","title":"GNNGraphs.graph_indicator","text":"graph_indicator(g::GNNGraph; edges=false)\n\nReturn a vector containing the graph membership (an integer from 1 to g.num_graphs) of each node in the graph. If edges=true, return the graph membership of each edge instead.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.has_isolated_nodes-Tuple{GNNGraph}","page":"GNNGraph","title":"GNNGraphs.has_isolated_nodes","text":"has_isolated_nodes(g::GNNGraph; dir=:out)\n\nReturn true if the graph g contains nodes with out-degree (if dir=:out) or in-degree (if dir = :in) equal to zero.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.has_multi_edges-Tuple{GNNGraph}","page":"GNNGraph","title":"GNNGraphs.has_multi_edges","text":"has_multi_edges(g::GNNGraph)\n\nReturn true if g has any multiple edges.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.is_bidirected-Tuple{GNNGraph}","page":"GNNGraph","title":"GNNGraphs.is_bidirected","text":"is_bidirected(g::GNNGraph)\n\nCheck if the directed graph g essentially corresponds to an undirected graph, i.e. if for each edge it also contains the  reverse edge. \n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.khop_adj","page":"GNNGraph","title":"GNNGraphs.khop_adj","text":"khop_adj(g::GNNGraph,k::Int,T::DataType=eltype(g); dir=:out, weighted=true)\n\nReturn A^k where A is the adjacency matrix of the graph 'g'.\n\n\n\n\n\n","category":"function"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.laplacian_lambda_max","page":"GNNGraph","title":"GNNGraphs.laplacian_lambda_max","text":"laplacian_lambda_max(g::GNNGraph, T=Float32; add_self_loops=false, dir=:out)\n\nReturn the largest eigenvalue of the normalized symmetric Laplacian of the graph g.\n\nIf the graph is batched from multiple graphs, return the list of the largest eigenvalue for each graph.\n\n\n\n\n\n","category":"function"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.normalized_laplacian","page":"GNNGraph","title":"GNNGraphs.normalized_laplacian","text":"normalized_laplacian(g, T=Float32; add_self_loops=false, dir=:out)\n\nNormalized Laplacian matrix of graph g.\n\nArguments\n\ng: A GNNGraph.\nT: result element type.\nadd_self_loops: add self-loops while calculating the matrix.\ndir: the edge directionality considered (:out, :in, :both).\n\n\n\n\n\n","category":"function"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.scaled_laplacian","page":"GNNGraph","title":"GNNGraphs.scaled_laplacian","text":"scaled_laplacian(g, T=Float32; dir=:out)\n\nScaled Laplacian matrix of graph g, defined as hatL = frac2lambda_max L - I where L is the normalized Laplacian matrix.\n\nArguments\n\ng: A GNNGraph.\nT: result element type.\ndir: the edge directionality considered (:out, :in, :both).\n\n\n\n\n\n","category":"function"},{"location":"GNNGraphs/api/gnngraph/#Graphs.LinAlg.adjacency_matrix","page":"GNNGraph","title":"Graphs.LinAlg.adjacency_matrix","text":"adjacency_matrix(g::GNNGraph, T=eltype(g); dir=:out, weighted=true)\n\nReturn the adjacency matrix A for the graph g. \n\nIf dir=:out, A[i,j] > 0 denotes the presence of an edge from node i to node j. If dir=:in instead, A[i,j] > 0 denotes the presence of an edge from node j to node i.\n\nUser may specify the eltype T of the returned matrix. \n\nIf weighted=true, the A will contain the edge weights if any, otherwise the elements of A will be either 0 or 1.\n\n\n\n\n\n","category":"function"},{"location":"GNNGraphs/api/gnngraph/#Graphs.degree-Union{Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}, Tuple{TT}, Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, TT}} where TT<:Union{Nothing, Type{<:Number}}","page":"GNNGraph","title":"Graphs.degree","text":"degree(g::GNNGraph, T=nothing; dir=:out, edge_weight=true)\n\nReturn a vector containing the degrees of the nodes in g.\n\nThe gradient is propagated through this function only if edge_weight is true or a vector.\n\nArguments\n\ng: A graph.\nT: Element type of the returned vector. If nothing, is      chosen based on the graph type and will be an integer      if edge_weight = false. Default nothing.\ndir: For dir = :out the degree of a node is counted based on the outgoing edges.        For dir = :in, the ingoing edges are used. If dir = :both we have the sum of the two.\nedge_weight: If true and the graph contains weighted edges, the degree will                be weighted. Set to false instead to just count the number of               outgoing/ingoing edges.                Finally, you can also pass a vector of weights to be used               instead of the graph's own weights.               Default true.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#Graphs.has_self_loops-Tuple{GNNGraph}","page":"GNNGraph","title":"Graphs.has_self_loops","text":"has_self_loops(g::GNNGraph)\n\nReturn true if g has any self loops.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#Graphs.inneighbors-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Integer}","page":"GNNGraph","title":"Graphs.inneighbors","text":"inneighbors(g::GNNGraph, i::Integer)\n\nReturn the neighbors of node i in the graph g through incoming edges.\n\nSee also neighbors and outneighbors.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#Graphs.outneighbors-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Integer}","page":"GNNGraph","title":"Graphs.outneighbors","text":"outneighbors(g::GNNGraph, i::Integer)\n\nReturn the neighbors of node i in the graph g through outgoing edges.\n\nSee also neighbors and inneighbors.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Graphs.neighbors(::GNNGraph, ::Integer)","category":"page"},{"location":"GNNGraphs/api/gnngraph/#Graphs.neighbors-Tuple{GNNGraph, Integer}","page":"GNNGraph","title":"Graphs.neighbors","text":"neighbors(g::GNNGraph, i::Integer; dir=:out)\n\nReturn the neighbors of node i in the graph g. If dir=:out, return the neighbors through outgoing edges. If dir=:in, return the neighbors through incoming edges.\n\nSee also outneighbors, inneighbors.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#Transform","page":"GNNGraph","title":"Transform","text":"","category":"section"},{"location":"GNNGraphs/api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Modules = [GNNGraphs]\nPages   = [\"src/transform.jl\"]\nPrivate = false","category":"page"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.add_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractVector, AbstractVector}","page":"GNNGraph","title":"GNNGraphs.add_edges","text":"add_edges(g::GNNGraph, s::AbstractVector, t::AbstractVector; [edata])\nadd_edges(g::GNNGraph, (s, t); [edata])\nadd_edges(g::GNNGraph, (s, t, w); [edata])\n\nAdd to graph g the edges with source nodes s and target nodes t. Optionally, pass the edge weight w and the features  edata for the new edges. Returns a new graph sharing part of the underlying data with g.\n\nIf the s or t contain nodes that are not already present in the graph, they are added to the graph as well.\n\nExamples\n\njulia> s, t = [1, 2, 3, 3, 4], [2, 3, 4, 4, 4];\n\njulia> w = Float32[1.0, 2.0, 3.0, 4.0, 5.0];\n\njulia> g = GNNGraph((s, t, w))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 5\n\njulia> add_edges(g, ([2, 3], [4, 1], [10.0, 20.0]))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 7\n\njulia> g = GNNGraph()\nGNNGraph:\n  num_nodes: 0\n  num_edges: 0\n\njulia> add_edges(g, [1,2], [2,3])\nGNNGraph:\n  num_nodes: 3\n  num_edges: 2\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.add_nodes-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Integer}","page":"GNNGraph","title":"GNNGraphs.add_nodes","text":"add_nodes(g::GNNGraph, n; [ndata])\n\nAdd n new nodes to graph g. In the  new graph, these nodes will have indexes from g.num_nodes + 1 to g.num_nodes + n.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.add_self_loops-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","page":"GNNGraph","title":"GNNGraphs.add_self_loops","text":"add_self_loops(g::GNNGraph)\n\nReturn a graph with the same features as g but also adding edges connecting the nodes to themselves.\n\nNodes with already existing self-loops will obtain a second self-loop.\n\nIf the graphs has edge weights, the new edges will have weight 1.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.getgraph-Tuple{GNNGraph, Int64}","page":"GNNGraph","title":"GNNGraphs.getgraph","text":"getgraph(g::GNNGraph, i; nmap=false)\n\nReturn the subgraph of g induced by those nodes j for which g.graph_indicator[j] == i or, if i is a collection, g.graph_indicator[j] ∈ i.  In other words, it extract the component graphs from a batched graph. \n\nIf nmap=true, return also a vector v mapping the new nodes to the old ones.  The node i in the subgraph will correspond to the node v[i] in g.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.negative_sample-Tuple{GNNGraph}","page":"GNNGraph","title":"GNNGraphs.negative_sample","text":"negative_sample(g::GNNGraph; \n                num_neg_edges = g.num_edges, \n                bidirected = is_bidirected(g))\n\nReturn a graph containing random negative edges (i.e. non-edges) from graph g as edges.\n\nIf bidirected=true, the output graph will be bidirected and there will be no leakage from the origin graph. \n\nSee also is_bidirected.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.perturb_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractFloat}","page":"GNNGraph","title":"GNNGraphs.perturb_edges","text":"perturb_edges([rng], g::GNNGraph, perturb_ratio)\n\nReturn a new graph obtained from g by adding random edges, based on a specified perturb_ratio.  The perturb_ratio determines the fraction of new edges to add relative to the current number of edges in the graph.  These new edges are added without creating self-loops. \n\nThe function returns a new GNNGraph instance that shares some of the underlying data with g but includes the additional edges.  The nodes for the new edges are selected randomly, and no edge data (edata) or weights (w) are assigned to these new edges.\n\nArguments\n\ng::GNNGraph: The graph to be perturbed.\nperturb_ratio: The ratio of the number of new edges to add relative to the current number of edges in the graph. For example, a perturb_ratio of 0.1 means that 10% of the current number of edges will be added as new random edges.\nrng: An optionalrandom number generator to ensure reproducible results.\n\nExamples\n\njulia> g = GNNGraph((s, t, w))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 5\n\njulia> perturbed_g = perturb_edges(g, 0.2)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 6\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.ppr_diffusion-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","page":"GNNGraph","title":"GNNGraphs.ppr_diffusion","text":"ppr_diffusion(g::GNNGraph{<:COO_T}, alpha =0.85f0) -> GNNGraph\n\nCalculates the Personalized PageRank (PPR) diffusion based on the edge weight matrix of a GNNGraph and updates the graph with new edge weights derived from the PPR matrix. References paper: The pagerank citation ranking: Bringing order to the web\n\nThe function performs the following steps:\n\nConstructs a modified adjacency matrix A using the graph's edge weights, where A is adjusted by (α - 1) * A + I, with α being the damping factor (alpha_f32) and I the identity matrix.\nNormalizes A to ensure each column sums to 1, representing transition probabilities.\nApplies the PPR formula α * (I + (α - 1) * A)^-1 to compute the diffusion matrix.\nUpdates the original edge weights of the graph based on the PPR diffusion matrix, assigning new weights for each edge from the PPR matrix.\n\nArguments\n\ng::GNNGraph: The input graph for which PPR diffusion is to be calculated. It should have edge weights available.\nalpha_f32::Float32: The damping factor used in PPR calculation, controlling the teleport probability in the random walk. Defaults to 0.85f0.\n\nReturns\n\nA new GNNGraph instance with the same structure as g but with updated edge weights according to the PPR diffusion calculation.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.rand_edge_split-Tuple{GNNGraph, Any}","page":"GNNGraph","title":"GNNGraphs.rand_edge_split","text":"rand_edge_split(g::GNNGraph, frac; bidirected=is_bidirected(g)) -> g1, g2\n\nRandomly partition the edges in g to form two graphs, g1 and g2. Both will have the same number of nodes as g. g1 will contain a fraction frac of the original edges,  while g2 wil contain the rest.\n\nIf bidirected = true makes sure that an edge and its reverse go into the same split. This option is supported only for bidirected graphs with no self-loops and multi-edges.\n\nrand_edge_split is tipically used to create train/test splits in link prediction tasks.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.random_walk_pe-Tuple{GNNGraph, Int64}","page":"GNNGraph","title":"GNNGraphs.random_walk_pe","text":"random_walk_pe(g, walk_length)\n\nReturn the random walk positional encoding from the paper Graph Neural Networks with Learnable Structural and Positional Representations of the given graph g and the length of the walk walk_length as a matrix of size (walk_length, g.num_nodes). \n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.remove_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractVector{<:Integer}}","page":"GNNGraph","title":"GNNGraphs.remove_edges","text":"remove_edges(g::GNNGraph, edges_to_remove::AbstractVector{<:Integer})\nremove_edges(g::GNNGraph, p=0.5)\n\nRemove specified edges from a GNNGraph, either by specifying edge indices or by randomly removing edges with a given probability.\n\nArguments\n\ng: The input graph from which edges will be removed.\nedges_to_remove: Vector of edge indices to be removed. This argument is only required for the first method.\np: Probability of removing each edge. This argument is only required for the second method and defaults to 0.5.\n\nReturns\n\nA new GNNGraph with the specified edges removed.\n\nExample\n\njulia> using GNNGraphs\n\n# Construct a GNNGraph\njulia> g = GNNGraph([1, 1, 2, 2, 3], [2, 3, 1, 3, 1])\nGNNGraph:\n  num_nodes: 3\n  num_edges: 5\n  \n# Remove the second edge\njulia> g_new = remove_edges(g, [2]);\n\njulia> g_new\nGNNGraph:\n  num_nodes: 3\n  num_edges: 4\n\n# Remove edges with a probability of 0.5\njulia> g_new = remove_edges(g, 0.5);\n\njulia> g_new\nGNNGraph:\n  num_nodes: 3\n  num_edges: 2\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.remove_multi_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","page":"GNNGraph","title":"GNNGraphs.remove_multi_edges","text":"remove_multi_edges(g::GNNGraph; aggr=+)\n\nRemove multiple edges (also called parallel edges or repeated edges) from graph g. Possible edge features are aggregated according to aggr, that can take value  +,min, max or mean.\n\nSee also remove_self_loops, has_multi_edges, and to_bidirected.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.remove_nodes-Tuple{GNNGraph, AbstractFloat}","page":"GNNGraph","title":"GNNGraphs.remove_nodes","text":"remove_nodes(g::GNNGraph, p)\n\nReturns a new graph obtained by dropping nodes from g with independent probabilities p. \n\nExamples\n\njulia> g = GNNGraph([1, 1, 2, 2, 3, 4], [1, 2, 3, 1, 3, 1])\nGNNGraph:\n  num_nodes: 4\n  num_edges: 6\n\njulia> g_new = remove_nodes(g, 0.5)\nGNNGraph:\n  num_nodes: 2\n  num_edges: 2\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.remove_nodes-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractVector}","page":"GNNGraph","title":"GNNGraphs.remove_nodes","text":"remove_nodes(g::GNNGraph, nodes_to_remove::AbstractVector)\n\nRemove specified nodes, and their associated edges, from a GNNGraph. This operation reindexes the remaining nodes to maintain a continuous sequence of node indices, starting from 1. Similarly, edges are reindexed to account for the removal of edges connected to the removed nodes.\n\nArguments\n\ng: The input graph from which nodes (and their edges) will be removed.\nnodes_to_remove: Vector of node indices to be removed.\n\nReturns\n\nA new GNNGraph with the specified nodes and all edges associated with these nodes removed. \n\nExample\n\nusing GNNGraphs\n\ng = GNNGraph([1, 1, 2, 2, 3], [2, 3, 1, 3, 1])\n\n# Remove nodes with indices 2 and 3, for example\ng_new = remove_nodes(g, [2, 3])\n\n# g_new now does not contain nodes 2 and 3, and any edges that were connected to these nodes.\nprintln(g_new)\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.remove_self_loops-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","page":"GNNGraph","title":"GNNGraphs.remove_self_loops","text":"remove_self_loops(g::GNNGraph)\n\nReturn a graph constructed from g where self-loops (edges from a node to itself) are removed. \n\nSee also add_self_loops and remove_multi_edges.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.set_edge_weight-Tuple{GNNGraph, AbstractVector}","page":"GNNGraph","title":"GNNGraphs.set_edge_weight","text":"set_edge_weight(g::GNNGraph, w::AbstractVector)\n\nSet w as edge weights in the returned graph. \n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.to_bidirected-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","page":"GNNGraph","title":"GNNGraphs.to_bidirected","text":"to_bidirected(g)\n\nAdds a reverse edge for each edge in the graph, then calls  remove_multi_edges with mean aggregation to simplify the graph. \n\nSee also is_bidirected. \n\nExamples\n\njulia> s, t = [1, 2, 3, 3, 4], [2, 3, 4, 4, 4];\n\njulia> w = [1.0, 2.0, 3.0, 4.0, 5.0];\n\njulia> e = [10.0, 20.0, 30.0, 40.0, 50.0];\n\njulia> g = GNNGraph(s, t, w, edata = e)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 5\n  edata:\n        e = 5-element Vector{Float64}\n\njulia> g2 = to_bidirected(g)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 7\n  edata:\n        e = 7-element Vector{Float64}\n\njulia> edge_index(g2)\n([1, 2, 2, 3, 3, 4, 4], [2, 1, 3, 2, 4, 3, 4])\n\njulia> get_edge_weight(g2)\n7-element Vector{Float64}:\n 1.0\n 1.0\n 2.0\n 2.0\n 3.5\n 3.5\n 5.0\n\njulia> g2.edata.e\n7-element Vector{Float64}:\n 10.0\n 10.0\n 20.0\n 20.0\n 35.0\n 35.0\n 50.0\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.to_unidirected-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","page":"GNNGraph","title":"GNNGraphs.to_unidirected","text":"to_unidirected(g::GNNGraph)\n\nReturn a graph that for each multiple edge between two nodes in g keeps only an edge in one direction.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#MLUtils.batch-Tuple{AbstractVector{<:GNNGraph}}","page":"GNNGraph","title":"MLUtils.batch","text":"batch(gs::Vector{<:GNNGraph})\n\nBatch together multiple GNNGraphs into a single one  containing the total number of original nodes and edges.\n\nEquivalent to SparseArrays.blockdiag. See also MLUtils.unbatch.\n\nExamples\n\njulia> g1 = rand_graph(4, 4, ndata=ones(Float32, 3, 4))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 4\n  ndata:\n        x = 3×4 Matrix{Float32}\n\njulia> g2 = rand_graph(5, 4, ndata=zeros(Float32, 3, 5))\nGNNGraph:\n  num_nodes: 5\n  num_edges: 4\n  ndata:\n        x = 3×5 Matrix{Float32}\n\njulia> g12 = MLUtils.batch([g1, g2])\nGNNGraph:\n  num_nodes: 9\n  num_edges: 8\n  num_graphs: 2\n  ndata:\n        x = 3×9 Matrix{Float32}\n\njulia> g12.ndata.x\n3×9 Matrix{Float32}:\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#MLUtils.unbatch-Union{Tuple{GNNGraph{T}}, Tuple{T}} where T<:(Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}})","page":"GNNGraph","title":"MLUtils.unbatch","text":"unbatch(g::GNNGraph)\n\nOpposite of the MLUtils.batch operation, returns  an array of the individual graphs batched together in g.\n\nSee also MLUtils.batch and getgraph.\n\nExamples\n\njulia> using MLUtils\n\njulia> gbatched = MLUtils.batch([rand_graph(5, 6), rand_graph(10, 8), rand_graph(4,2)])\nGNNGraph:\n  num_nodes: 19\n  num_edges: 16\n  num_graphs: 3\n\njulia> MLUtils.unbatch(gbatched)\n3-element Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}:\n GNNGraph(5, 6) with no data\n GNNGraph(10, 8) with no data\n GNNGraph(4, 2) with no data\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#SparseArrays.blockdiag-Tuple{GNNGraph, Vararg{GNNGraph}}","page":"GNNGraph","title":"SparseArrays.blockdiag","text":"blockdiag(xs::GNNGraph...)\n\nEquivalent to MLUtils.batch.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#Utils","page":"GNNGraph","title":"Utils","text":"","category":"section"},{"location":"GNNGraphs/api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"GNNGraphs.sort_edge_index\nGNNGraphs.color_refinement","category":"page"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.sort_edge_index","page":"GNNGraph","title":"GNNGraphs.sort_edge_index","text":"sort_edge_index(ei::Tuple) -> u', v'\nsort_edge_index(u, v) -> u', v'\n\nReturn a sorted version of the tuple of vectors ei = (u, v), applying a common permutation to u and v. The sorting is lexycographic, that is the pairs (ui, vi)  are sorted first according to the ui and then according to vi. \n\n\n\n\n\n","category":"function"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.color_refinement","page":"GNNGraph","title":"GNNGraphs.color_refinement","text":"color_refinement(g::GNNGraph, [x0]) -> x, num_colors, niters\n\nThe color refinement algorithm for graph coloring.  Given a graph g and an initial coloring x0, the algorithm  iteratively refines the coloring until a fixed point is reached.\n\nAt each iteration the algorithm computes a hash of the coloring and the sorted list of colors of the neighbors of each node. This hash is used to determine if the coloring has changed.\n\nmath x_i' = hashmap((x_i, sort([x_j for j \\in N(i)]))).`\n\nThis algorithm is related to the 1-Weisfeiler-Lehman algorithm for graph isomorphism testing.\n\nArguments\n\ng::GNNGraph: The graph to color.\nx0::AbstractVector{<:Integer}: The initial coloring. If not provided, all nodes are colored with 1.\n\nReturns\n\nx::AbstractVector{<:Integer}: The final coloring.\nnum_colors::Int: The number of colors used.\nniters::Int: The number of iterations until convergence.\n\n\n\n\n\n","category":"function"},{"location":"GNNGraphs/api/gnngraph/#Generate","page":"GNNGraph","title":"Generate","text":"","category":"section"},{"location":"GNNGraphs/api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Modules = [GNNGraphs]\nPages   = [\"src/generate.jl\"]\nPrivate = false\nFilter = t -> typeof(t) <: Function && t!=rand_temporal_radius_graph && t!=rand_temporal_hyperbolic_graph","category":"page"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.knn_graph-Tuple{AbstractMatrix, Int64}","page":"GNNGraph","title":"GNNGraphs.knn_graph","text":"knn_graph(points::AbstractMatrix, \n          k::Int; \n          graph_indicator = nothing,\n          self_loops = false, \n          dir = :in, \n          kws...)\n\nCreate a k-nearest neighbor graph where each node is linked  to its k closest points.  \n\nArguments\n\npoints: A numfeatures × numnodes matrix storing the Euclidean positions of the nodes.\nk: The number of neighbors considered in the kNN algorithm.\ngraph_indicator: Either nothing or a vector containing the graph assignment of each node,                     in which case the returned graph will be a batch of graphs. \nself_loops: If true, consider the node itself among its k nearest neighbors, in which               case the graph will contain self-loops. \ndir: The direction of the edges. If dir=:in edges go from the k         neighbors to the central node. If dir=:out we have the opposite        direction.\nkws: Further keyword arguments will be passed to the GNNGraph constructor.\n\nExamples\n\njulia> n, k = 10, 3;\n\njulia> x = rand(Float32, 3, n);\n\njulia> g = knn_graph(x, k)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 30\n\njulia> graph_indicator = [1,1,1,1,1,2,2,2,2,2];\n\njulia> g = knn_graph(x, k; graph_indicator)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 30\n    num_graphs = 2\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.radius_graph-Tuple{AbstractMatrix, AbstractFloat}","page":"GNNGraph","title":"GNNGraphs.radius_graph","text":"radius_graph(points::AbstractMatrix, \n             r::AbstractFloat; \n             graph_indicator = nothing,\n             self_loops = false, \n             dir = :in, \n             kws...)\n\nCreate a graph where each node is linked  to its neighbors within a given distance r.  \n\nArguments\n\npoints: A numfeatures × numnodes matrix storing the Euclidean positions of the nodes.\nr: The radius.\ngraph_indicator: Either nothing or a vector containing the graph assignment of each node,                     in which case the returned graph will be a batch of graphs. \nself_loops: If true, consider the node itself among its neighbors, in which               case the graph will contain self-loops. \ndir: The direction of the edges. If dir=:in edges go from the        neighbors to the central node. If dir=:out we have the opposite        direction.\nkws: Further keyword arguments will be passed to the GNNGraph constructor.\n\nExamples\n\njulia> n, r = 10, 0.75;\n\njulia> x = rand(Float32, 3, n);\n\njulia> g = radius_graph(x, r)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 46\n\njulia> graph_indicator = [1,1,1,1,1,2,2,2,2,2];\n\njulia> g = radius_graph(x, r; graph_indicator)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 20\n    num_graphs = 2\n\nReferences\n\nSection B paragraphs 1 and 2 of the paper Dynamic Hidden-Variable Network Models\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.rand_graph-Tuple{Integer, Integer}","page":"GNNGraph","title":"GNNGraphs.rand_graph","text":"rand_graph([rng,] n, m; bidirected=true, edge_weight = nothing, kws...)\n\nGenerate a random (Erdós-Renyi) GNNGraph with n nodes and m edges.\n\nIf bidirected=true the reverse edge of each edge will be present. If bidirected=false instead, m unrelated edges are generated. In any case, the output graph will contain no self-loops or multi-edges.\n\nA vector can be passed  as edge_weight. Its length has to be equal to m in the directed case, and m÷2 in the bidirected one.\n\nPass a random number generator as the first argument to make the generation reproducible.\n\nAdditional keyword arguments will be passed to the GNNGraph constructor.\n\nExamples\n\njulia> g = rand_graph(5, 4, bidirected=false)\nGNNGraph:\n  num_nodes: 5\n  num_edges: 4\n\njulia> edge_index(g)\n([4, 3, 2, 1], [5, 4, 3, 2])\n\n# In the bidirected case, edge data will be duplicated on the reverse edges if needed.\njulia> g = rand_graph(5, 4, edata=rand(Float32, 16, 2))\nGNNGraph:\n  num_nodes: 5\n  num_edges: 4\n  edata:\n        e = 16×4 Matrix{Float32}\n\n# Each edge has a reverse\njulia> edge_index(g)\n([1, 1, 5, 3], [5, 3, 1, 1])\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/gnngraph/#Operators","page":"GNNGraph","title":"Operators","text":"","category":"section"},{"location":"GNNGraphs/api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Modules = [GNNGraphs]\nPages   = [\"src/operators.jl\"]\nPrivate = false","category":"page"},{"location":"GNNGraphs/api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Base.intersect","category":"page"},{"location":"GNNGraphs/api/gnngraph/#Base.intersect","page":"GNNGraph","title":"Base.intersect","text":"intersect(g1::GNNGraph, g2::GNNGraph)\n\nIntersect two graphs by keeping only the common edges.\n\n\n\n\n\n","category":"function"},{"location":"GNNGraphs/api/gnngraph/#Sampling","page":"GNNGraph","title":"Sampling","text":"","category":"section"},{"location":"GNNGraphs/api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Modules = [GNNGraphs]\nPages   = [\"src/sampling.jl\"]\nPrivate = false","category":"page"},{"location":"GNNGraphs/api/gnngraph/#GNNGraphs.sample_neighbors","page":"GNNGraph","title":"GNNGraphs.sample_neighbors","text":"sample_neighbors(g, nodes, K=-1; dir=:in, replace=false, dropnodes=false)\n\nSample neighboring edges of the given nodes and return the induced subgraph. For each node, a number of inbound (or outbound when dir = :out) edges will be randomly chosen.  Ifdropnodes=false`, the graph returned will then contain all the nodes in the original graph,  but only the sampled edges.\n\nThe returned graph will contain an edge feature EID corresponding to the id of the edge in the original graph. If dropnodes=true, it will also contain a node feature NID with the node ids in the original graph.\n\nArguments\n\ng. The graph.\nnodes. A list of node IDs to sample neighbors from.\nK. The maximum number of edges to be sampled for each node.      If -1, all the neighboring edges will be selected.\ndir. Determines whether to sample inbound (:in) or outbound (`:out) edges (Default :in).\nreplace. If true, sample with replacement.\ndropnodes. If true, the resulting subgraph will contain only the nodes involved in the sampled edges.\n\nExamples\n\njulia> g = rand_graph(20, 100)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 100\n\njulia> sample_neighbors(g, 2:3)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 9\n    edata:\n        EID => (9,)\n\njulia> sg = sample_neighbors(g, 2:3, dropnodes=true)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 9\n    ndata:\n        NID => (10,)\n    edata:\n        EID => (9,)\n\njulia> sg.ndata.NID\n10-element Vector{Int64}:\n  2\n  3\n 17\n 14\n 18\n 15\n 16\n 20\n  7\n 10\n\njulia> sample_neighbors(g, 2:3, 5, replace=true)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 10\n    edata:\n        EID => (10,)\n\n\n\n\n\n","category":"function"},{"location":"GNNGraphs/api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Graphs.induced_subgraph(::GNNGraph, ::Vector{Int})","category":"page"},{"location":"GNNGraphs/api/gnngraph/#Graphs.induced_subgraph-Tuple{GNNGraph, Vector{Int64}}","page":"GNNGraph","title":"Graphs.induced_subgraph","text":"induced_subgraph(graph, nodes)\n\nGenerates a subgraph from the original graph using the provided nodes.  The function includes the nodes' neighbors and creates edges between nodes that are connected in the original graph.  If a node has no neighbors, an isolated node will be added to the subgraph.  Returns A new GNNGraph containing the subgraph with the specified nodes and their features.\n\nArguments\n\ngraph. The original GNNGraph containing nodes, edges, and node features.\nnodes`. A vector of node indices to include in the subgraph.\n\nExamples\n\njulia> s = [1, 2]\n2-element Vector{Int64}:\n 1\n 2\n\njulia> t = [2, 3]\n2-element Vector{Int64}:\n 2\n 3\n\njulia> graph = GNNGraph((s, t), ndata = (; x=rand(Float32, 32, 3), y=rand(Float32, 3)), edata = rand(Float32, 2))\nGNNGraph:\n  num_nodes: 3\n  num_edges: 2\n  ndata:\n        y = 3-element Vector{Float32}\n        x = 32×3 Matrix{Float32}\n  edata:\n        e = 2-element Vector{Float32}\n\njulia> nodes = [1, 2]\n2-element Vector{Int64}:\n 1\n 2\n\njulia> subgraph = Graphs.induced_subgraph(graph, nodes)\nGNNGraph:\n  num_nodes: 2\n  num_edges: 1\n  ndata:\n        y = 2-element Vector{Float32}\n        x = 32×2 Matrix{Float32}\n  edata:\n        e = 1-element Vector{Float32}\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/guides/gnngraph/#Static-Graphs","page":"Graphs","title":"Static Graphs","text":"","category":"section"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"The fundamental graph type in GNNGraphs.jl is the GNNGraph. A GNNGraph g is a directed graph with nodes labeled from 1 to g.num_nodes. The underlying implementation allows for efficient application of graph neural network operators, gpu movement, and storage of node/edge/graph related feature arrays.","category":"page"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"GNNGraph inherits from Graphs.jl's AbstractGraph, therefore it supports most functionality from that library. ","category":"page"},{"location":"GNNGraphs/guides/gnngraph/#Graph-Creation","page":"Graphs","title":"Graph Creation","text":"","category":"section"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"A GNNGraph can be created from several different data sources encoding the graph topology:","category":"page"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"using GNNGraphs, Graphs, SparseArrays\n\n\n# Construct a GNNGraph from from a Graphs.jl's graph\nlg = erdos_renyi(10, 30)\ng = GNNGraph(lg)\n\n# Same as above using convenience method rand_graph\ng = rand_graph(10, 60)\n\n# From an adjacency matrix\nA = sprand(10, 10, 0.3)\ng = GNNGraph(A)\n\n# From an adjacency list\nadjlist = [[2,3], [1,3], [1,2,4], [3]]\ng = GNNGraph(adjlist)\n\n# From COO representation\nsource = [1,1,2,2,3,3,3,4]\ntarget = [2,3,1,3,1,2,4,3]\ng = GNNGraph(source, target)","category":"page"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"See also the related methods Graphs.adjacency_matrix, edge_index, and adjacency_list.","category":"page"},{"location":"GNNGraphs/guides/gnngraph/#Basic-Queries","page":"Graphs","title":"Basic Queries","text":"","category":"section"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"julia> source = [1,1,2,2,3,3,3,4];\n\njulia> target = [2,3,1,3,1,2,4,3];\n\njulia> g = GNNGraph(source, target)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 8\n\n\njulia> @assert g.num_nodes == 4   # number of nodes\n\njulia> @assert g.num_edges == 8   # number of edges\n\njulia> @assert g.num_graphs == 1  # number of subgraphs (a GNNGraph can batch many graphs together)\n\njulia> is_directed(g)      # a GNNGraph is always directed\ntrue\n\njulia> is_bidirected(g)      # for each edge, also the reverse edge is present\ntrue\n\njulia> has_self_loops(g)\nfalse\n\njulia> has_multi_edges(g)      \nfalse","category":"page"},{"location":"GNNGraphs/guides/gnngraph/#Data-Features","page":"Graphs","title":"Data Features","text":"","category":"section"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"One or more arrays can be associated to nodes, edges, and (sub)graphs of a GNNGraph. They will be stored in the fields g.ndata, g.edata, and g.gdata respectively.","category":"page"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"The data fields are DataStore objects. DataStores conveniently offer an interface similar to both dictionaries and named tuples. Similarly to dictionaries, DataStores support addition of new features after creation time.","category":"page"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"The array contained in the datastores have last dimension equal to num_nodes (in ndata), num_edges (in edata), or num_graphs (in gdata) respectively.","category":"page"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"# Create a graph with a single feature array `x` associated to nodes\ng = rand_graph(10,  60, ndata = (; x = rand(Float32, 32, 10)))\n\ng.ndata.x  # access the features\n\n# Equivalent definition passing directly the array\ng = rand_graph(10,  60, ndata = rand(Float32, 32, 10))\n\ng.ndata.x  # `:x` is the default name for node features\n\ng.ndata.z = rand(Float32, 3, 10)  # add new feature array `z`\n\n# For convenience, we can access the features through the shortcut\ng.x \n\n# You can have multiple feature arrays\ng = rand_graph(10,  60, ndata = (; x=rand(Float32, 32, 10), y=rand(Float32, 10)))\n\ng.ndata.y, g.ndata.x   # or g.x, g.y\n\n# Attach an array with edge features.\n# Since `GNNGraph`s are directed, the number of edges\n# will be double that of the original Graphs' undirected graph.\ng = GNNGraph(erdos_renyi(10,  30), edata = rand(Float32, 60))\n@assert g.num_edges == 60\n\ng.edata.e  # or g.e\n\n# If we pass only half of the edge features, they will be copied\n# on the reversed edges.\ng = GNNGraph(erdos_renyi(10,  30), edata = rand(Float32, 30))\n\n\n# Create a new graph from previous one, inheriting edge data\n# but replacing node data\ng′ = GNNGraph(g, ndata =(; z = ones(Float32, 16, 10)))\n\ng′.z\ng′.e","category":"page"},{"location":"GNNGraphs/guides/gnngraph/#Edge-weights","page":"Graphs","title":"Edge weights","text":"","category":"section"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"It is common to denote scalar edge features as edge weights. The GNNGraph has specific support for edge weights: they can be stored as part of internal representations of the graph (COO or adjacency matrix). Some graph convolutional layers, most notably the GCNConv, can use the edge weights to perform weighted sums over the nodes' neighborhoods.","category":"page"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"julia> source = [1, 1, 2, 2, 3, 3];\n\njulia> target = [2, 3, 1, 3, 1, 2];\n\njulia> weight = [1.0, 0.5, 2.1, 2.3, 4, 4.1];\n\njulia> g = GNNGraph(source, target, weight)\nGNNGraph:\n  num_nodes: 3\n  num_edges: 6\n\njulia> get_edge_weight(g)\n6-element Vector{Float64}:\n 1.0\n 0.5\n 2.1\n 2.3\n 4.0\n 4.1","category":"page"},{"location":"GNNGraphs/guides/gnngraph/#Batches-and-Subgraphs","page":"Graphs","title":"Batches and Subgraphs","text":"","category":"section"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"Multiple GNNGraphs can be batched together into a single graph that contains the total number of the original nodes  and where the original graphs are disjoint subgraphs.","category":"page"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"using MLUtils\n\ndata = [rand_graph(10, 30, ndata=rand(Float32, 3, 10)) for _ in 1:160]\ngall = MLUtils.batch(data)\n\n# gall is a GNNGraph containing many graphs\n@assert gall.num_graphs == 160 \n@assert gall.num_nodes == 1600   # 10 nodes x 160 graphs\n@assert gall.num_edges == 4800  # 30 undirected edges x 160 graphs\n\n# Let's create a mini-batch from gall\ng23 = getgraph(gall, 2:3)\n@assert g23.num_graphs == 2\n@assert g23.num_nodes == 20   # 10 nodes x 2 graphs\n@assert g23.num_edges == 60  # 30 undirected edges X 2 graphs\n\n# We can pass a GNNGraph to MLUtils' DataLoader\ntrain_loader = DataLoader(gall, batchsize=16, shuffle=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes == 160\n    @assert size(g.ndata.x) = (3, 160)    \n    # .....\nend\n\n# Access the nodes' graph memberships \ngraph_indicator(gall)","category":"page"},{"location":"GNNGraphs/guides/gnngraph/#DataLoader-and-mini-batch-iteration","page":"Graphs","title":"DataLoader and mini-batch iteration","text":"","category":"section"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"While constructing a batched graph and passing it to the DataLoader is always  an option for mini-batch iteration, the recommended way for better performance is to pass an array of graphs directly and set the collate option to true:","category":"page"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"using MLUtils: DataLoader\n\ndata = [rand_graph(10, 30, ndata=rand(Float32, 3, 10)) for _ in 1:320]\n\ntrain_loader = DataLoader(data, batchsize=16, shuffle=true, collate=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes == 160\n    @assert size(g.ndata.x) = (3, 160)    \n    # .....\nend","category":"page"},{"location":"GNNGraphs/guides/gnngraph/#Graph-Manipulation","page":"Graphs","title":"Graph Manipulation","text":"","category":"section"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"g′ = add_self_loops(g)\ng′ = remove_self_loops(g)\ng′ = add_edges(g, [1, 2], [2, 3]) # add edges 1->2 and 2->3","category":"page"},{"location":"GNNGraphs/guides/gnngraph/#GPU-movement","page":"Graphs","title":"GPU movement","text":"","category":"section"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"Move a GNNGraph to a CUDA device using Flux.gpu method. ","category":"page"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"using Flux, CUDA # or using Metal or using AMDGPU \n\ng_gpu = g |> Flux.gpu","category":"page"},{"location":"GNNGraphs/guides/gnngraph/#Integration-with-Graphs.jl","page":"Graphs","title":"Integration with Graphs.jl","text":"","category":"section"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"Since GNNGraph <: Graphs.AbstractGraph, we can use any functionality from Graphs.jl for querying and analyzing the graph structure.  Moreover, a GNNGraph can be easily constructed from a Graphs.Graph or a Graphs.DiGraph:","category":"page"},{"location":"GNNGraphs/guides/gnngraph/","page":"Graphs","title":"Graphs","text":"julia> import Graphs\n\njulia> using GNNGraphs\n\n# A Graphs.jl undirected graph\njulia> gu = Graphs.erdos_renyi(10, 20)    \n{10, 20} undirected simple Int64 graph\n\n# Since GNNGraphs are undirected, the edges are doubled when converting \n# to GNNGraph\njulia> GNNGraph(gu)\nGNNGraph:\n  num_nodes: 10\n  num_edges: 40\n\n# A Graphs.jl directed graph\njulia> gd = Graphs.erdos_renyi(10, 20, is_directed=true)\n{10, 20} directed simple Int64 graph\n\njulia> GNNGraph(gd)\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20","category":"page"},{"location":"api/basic/","page":"Basic layers","title":"Basic layers","text":"CurrentModule = GNNLux\nCollapsedDocStrings = true","category":"page"},{"location":"api/basic/#Basic-Layers","page":"Basic layers","title":"Basic Layers","text":"","category":"section"},{"location":"api/basic/","page":"Basic layers","title":"Basic layers","text":"GNNLayer\nGNNChain","category":"page"},{"location":"api/basic/#GNNLux.GNNLayer","page":"Basic layers","title":"GNNLux.GNNLayer","text":"abstract type GNNLayer <: AbstractLuxLayer end\n\nAn abstract type from which graph neural network layers are derived. It is derived from Lux's AbstractLuxLayer type.\n\nSee also GNNLux.GNNChain.\n\n\n\n\n\n","category":"type"},{"location":"api/basic/#GNNLux.GNNChain","page":"Basic layers","title":"GNNLux.GNNChain","text":"GNNChain(layers...)\nGNNChain(name = layer, ...)\n\nCollects multiple layers / functions to be called in sequence on given input graph and input node features. \n\nIt allows to compose layers in a sequential fashion as Lux.Chain does, propagating the output of each layer to the next one. In addition, GNNChain handles the input graph as well, providing it  as a first argument only to layers subtyping the GNNLayer abstract type. \n\nGNNChain supports indexing and slicing, m[2] or m[1:end-1], and if names are given, m[:name] == m[1] etc.\n\nExamples\n\njulia> using Lux, GNNLux, Random\n\njulia> rng = Random.default_rng();\n\njulia> m = GNNChain(GCNConv(2=>5), \n                    x -> relu.(x), \n                    Dense(5=>4))\n\njulia> x = randn(rng, Float32, 2, 3);\n\njulia> g = rand_graph(rng, 3, 6)\nGNNGraph:\n  num_nodes: 3\n  num_edges: 6\n\njulia> ps, st = LuxCore.setup(rng, m);\n\njulia> m(g, x, ps, st)     # First entry is the output, second entry is the state of the model\n(Float32[-0.15594329 -0.15594329 -0.15594329; 0.93431795 0.93431795 0.93431795; 0.27568763 0.27568763 0.27568763; 0.12568939 0.12568939 0.12568939], (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple()))\n\n\n\n\n\n","category":"type"},{"location":"GNNGraphs/api/heterograph/","page":"GNNHeteroGraph","title":"GNNHeteroGraph","text":"CurrentModule = GNNGraphs\nCollapsedDocStrings = true","category":"page"},{"location":"GNNGraphs/api/heterograph/#Heterogeneous-Graphs","page":"GNNHeteroGraph","title":"Heterogeneous Graphs","text":"","category":"section"},{"location":"GNNGraphs/api/heterograph/#GNNHeteroGraph","page":"GNNHeteroGraph","title":"GNNHeteroGraph","text":"","category":"section"},{"location":"GNNGraphs/api/heterograph/","page":"GNNHeteroGraph","title":"GNNHeteroGraph","text":"Documentation page for the type GNNHeteroGraph representing heterogeneous graphs, where  nodes and edges can have different types.","category":"page"},{"location":"GNNGraphs/api/heterograph/","page":"GNNHeteroGraph","title":"GNNHeteroGraph","text":"Modules = [GNNGraphs]\nPages   = [\"gnnheterograph.jl\"]\nPrivate = false","category":"page"},{"location":"GNNGraphs/api/heterograph/#GNNGraphs.GNNHeteroGraph","page":"GNNHeteroGraph","title":"GNNGraphs.GNNHeteroGraph","text":"GNNHeteroGraph(data; [ndata, edata, gdata, num_nodes])\nGNNHeteroGraph(pairs...; [ndata, edata, gdata, num_nodes])\n\nA type representing a heterogeneous graph structure. It is similar to GNNGraph but nodes and edges are of different types.\n\nConstructor Arguments\n\ndata: A dictionary or an iterable object that maps (source_type, edge_type, target_type)         triples to (source, target) index vectors (or to (source, target, weight) if also edge weights are present).\npairs: Passing multiple relations as pairs is equivalent to passing data=Dict(pairs...).\nndata: Node features. A dictionary of arrays or named tuple of arrays.          The size of the last dimension of each array must be given by g.num_nodes.\nedata: Edge features. A dictionary of arrays or named tuple of arrays. Default nothing.          The size of the last dimension of each array must be given by g.num_edges. Default nothing.\ngdata: Graph features. An array or named tuple of arrays whose last dimension has size num_graphs. Default nothing.\nnum_nodes: The number of nodes for each type. If not specified, inferred from data. Default nothing.\n\nFields\n\ngraph: A dictionary that maps (sourcetype, edgetype, target_type) triples to (source, target) index vectors.\nnum_nodes: The number of nodes for each type.\nnum_edges: The number of edges for each type.\nndata: Node features.\nedata: Edge features.\ngdata: Graph features.\nntypes: The node types.\netypes: The edge types.\n\nExamples\n\njulia> using GNNGraphs\n\njulia> nA, nB = 10, 20;\n\njulia> num_nodes = Dict(:A => nA, :B => nB);\n\njulia> edges1 = (rand(1:nA, 20), rand(1:nB, 20))\n([4, 8, 6, 3, 4, 7, 2, 7, 3, 2, 3, 4, 9, 4, 2, 9, 10, 1, 3, 9], [6, 4, 20, 8, 16, 7, 12, 16, 5, 4, 6, 20, 11, 19, 17, 9, 12, 2, 18, 12])\n\njulia> edges2 = (rand(1:nB, 30), rand(1:nA, 30))\n([17, 5, 2, 4, 5, 3, 8, 7, 9, 7  …  19, 8, 20, 7, 16, 2, 9, 15, 8, 13], [1, 1, 3, 1, 1, 3, 2, 7, 4, 4  …  7, 10, 6, 3, 4, 9, 1, 5, 8, 5])\n\njulia> data = ((:A, :rel1, :B) => edges1, (:B, :rel2, :A) => edges2);\n\njulia> hg = GNNHeteroGraph(data; num_nodes)\nGNNHeteroGraph:\n  num_nodes: (:A => 10, :B => 20)\n  num_edges: ((:A, :rel1, :B) => 20, (:B, :rel2, :A) => 30)\n\njulia> hg.num_edges\nDict{Tuple{Symbol, Symbol, Symbol}, Int64} with 2 entries:\n(:A, :rel1, :B) => 20\n(:B, :rel2, :A) => 30\n\n# Let's add some node features\njulia> ndata = Dict(:A => (x = rand(2, nA), y = rand(3, num_nodes[:A])),\n                    :B => rand(10, nB));\n\njulia> hg = GNNHeteroGraph(data; num_nodes, ndata)\nGNNHeteroGraph:\n    num_nodes: (:A => 10, :B => 20)\n    num_edges: ((:A, :rel1, :B) => 20, (:B, :rel2, :A) => 30)\n    ndata:\n    :A  =>  (x = 2×10 Matrix{Float64}, y = 3×10 Matrix{Float64})\n    :B  =>  x = 10×20 Matrix{Float64}\n\n# Access features of nodes of type :A\njulia> hg.ndata[:A].x\n2×10 Matrix{Float64}:\n    0.825882  0.0797502  0.245813  0.142281  0.231253  0.685025  0.821457  0.888838  0.571347   0.53165\n    0.631286  0.316292   0.705325  0.239211  0.533007  0.249233  0.473736  0.595475  0.0623298  0.159307\n\nSee also GNNGraph for a homogeneous graph type and rand_heterograph for a function to generate random heterographs.\n\n\n\n\n\n","category":"type"},{"location":"GNNGraphs/api/heterograph/#GNNGraphs.edge_type_subgraph-Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}}","page":"GNNHeteroGraph","title":"GNNGraphs.edge_type_subgraph","text":"edge_type_subgraph(g::GNNHeteroGraph, edge_ts)\n\nReturn a subgraph of g that contains only the edges of type edge_ts. Edge types can be specified as a single edge type (i.e. a tuple containing 3 symbols) or a vector of edge types.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/heterograph/#GNNGraphs.num_edge_types-Tuple{GNNGraph}","page":"GNNHeteroGraph","title":"GNNGraphs.num_edge_types","text":"num_edge_types(g)\n\nReturn the number of edge types in the graph. For GNNGraphs, this is always 1. For GNNHeteroGraphs, this is the number of unique edge types.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/heterograph/#GNNGraphs.num_node_types-Tuple{GNNGraph}","page":"GNNHeteroGraph","title":"GNNGraphs.num_node_types","text":"num_node_types(g)\n\nReturn the number of node types in the graph. For GNNGraphs, this is always 1. For GNNHeteroGraphs, this is the number of unique node types.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/heterograph/#Query","page":"GNNHeteroGraph","title":"Query","text":"","category":"section"},{"location":"GNNGraphs/api/heterograph/","page":"GNNHeteroGraph","title":"GNNHeteroGraph","text":"Modules = [GNNGraphs]\nPages   = [\"gnnheterograph/query.jl\"]\nPrivate = false","category":"page"},{"location":"GNNGraphs/api/heterograph/#GNNGraphs.edge_index-Tuple{GNNHeteroGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Tuple{Symbol, Symbol, Symbol}}","page":"GNNHeteroGraph","title":"GNNGraphs.edge_index","text":"edge_index(g::GNNHeteroGraph, [edge_t])\n\nReturn a tuple containing two vectors, respectively storing the source and target nodes for each edges in g of type edge_t = (src_t, rel_t, trg_t).\n\nIf edge_t is not provided, it will error if g has more than one edge type.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/heterograph/#GNNGraphs.graph_indicator-Tuple{GNNHeteroGraph}","page":"GNNHeteroGraph","title":"GNNGraphs.graph_indicator","text":"graph_indicator(g::GNNHeteroGraph, [node_t])\n\nReturn a Dict of vectors containing the graph membership (an integer from 1 to g.num_graphs) of each node in the graph for each node type. If node_t is provided, return the graph membership of each node of type node_t instead.\n\nSee also batch.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/heterograph/#Graphs.degree-Union{Tuple{TT}, Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}}, Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}, TT}} where TT<:Union{Nothing, Type{<:Number}}","page":"GNNHeteroGraph","title":"Graphs.degree","text":"degree(g::GNNHeteroGraph, edge_type::EType; dir = :in)\n\nReturn a vector containing the degrees of the nodes in g GNNHeteroGraph given edge_type.\n\nArguments\n\ng: A graph.\nedge_type: A tuple of symbols (source_t, edge_t, target_t) representing the edge type.\nT: Element type of the returned vector. If nothing, is      chosen based on the graph type. Default nothing.\ndir: For dir = :out the degree of a node is counted based on the outgoing edges.        For dir = :in, the ingoing edges are used. If dir = :both we have the sum of the two.        Default dir = :out.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/heterograph/#Graphs.has_edge-Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}, Integer, Integer}","page":"GNNHeteroGraph","title":"Graphs.has_edge","text":"has_edge(g::GNNHeteroGraph, edge_t, i, j)\n\nReturn true if there is an edge of type edge_t from node i to node j in g.\n\nExamples\n\njulia> g = rand_bipartite_heterograph((2, 2), (4, 0), bidirected=false)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 2, :B => 2)\n  num_edges: Dict((:A, :to, :B) => 4, (:B, :to, :A) => 0)\n\njulia> has_edge(g, (:A,:to,:B), 1, 1)\ntrue\n\njulia> has_edge(g, (:B,:to,:A), 1, 1)\nfalse\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/heterograph/#Transform","page":"GNNHeteroGraph","title":"Transform","text":"","category":"section"},{"location":"GNNGraphs/api/heterograph/","page":"GNNHeteroGraph","title":"GNNHeteroGraph","text":"Modules = [GNNGraphs]\nPages   = [\"gnnheterograph/transform.jl\"]\nPrivate = false","category":"page"},{"location":"GNNGraphs/api/heterograph/#GNNGraphs.add_edges-Tuple{GNNHeteroGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Tuple{Symbol, Symbol, Symbol}, AbstractVector, AbstractVector}","page":"GNNHeteroGraph","title":"GNNGraphs.add_edges","text":"add_edges(g::GNNHeteroGraph, edge_t, s, t; [edata, num_nodes])\nadd_edges(g::GNNHeteroGraph, edge_t => (s, t); [edata, num_nodes])\nadd_edges(g::GNNHeteroGraph, edge_t => (s, t, w); [edata, num_nodes])\n\nAdd to heterograph g edges of type edge_t with source node vector s and target node vector t. Optionally, pass the  edge weights w or the features  edata for the new edges. edge_t is a triplet of symbols (src_t, rel_t, dst_t). \n\nIf the edge type is not already present in the graph, it is added.  If it involves new node types, they are added to the graph as well. In this case, a dictionary or named tuple of num_nodes can be passed to specify the number of nodes of the new types, otherwise the number of nodes is inferred from the maximum node id in s and t.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/heterograph/#GNNGraphs.add_self_loops-Tuple{GNNHeteroGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Tuple{Symbol, Symbol, Symbol}}","page":"GNNHeteroGraph","title":"GNNGraphs.add_self_loops","text":"add_self_loops(g::GNNHeteroGraph, edge_t::EType)\nadd_self_loops(g::GNNHeteroGraph)\n\nIf the source node type is the same as the destination node type in edge_t, return a graph with the same features as g but also add self-loops  of the specified type, edge_t. Otherwise, it returns g unchanged.\n\nNodes with already existing self-loops of type edge_t will obtain  a second set of self-loops of the same type.\n\nIf the graph has edge weights for edges of type edge_t, the new edges will have weight 1.\n\nIf no edges of type edge_t exist, or all existing edges have no weight,  then all new self loops will have no weight.\n\nIf edge_t is not passed as argument, for the entire graph self-loop is added to each node for every edge type in the graph where the source and destination node types are the same.  This iterates over all edge types present in the graph, applying the self-loop addition logic to each applicable edge type.\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/heterograph/#Generate","page":"GNNHeteroGraph","title":"Generate","text":"","category":"section"},{"location":"GNNGraphs/api/heterograph/","page":"GNNHeteroGraph","title":"GNNHeteroGraph","text":"Modules = [GNNGraphs]\nPages   = [\"gnnheterograph/generate.jl\"]\nPrivate = false","category":"page"},{"location":"GNNGraphs/api/heterograph/#GNNGraphs.rand_bipartite_heterograph-Tuple{Any, Any}","page":"GNNHeteroGraph","title":"GNNGraphs.rand_bipartite_heterograph","text":"rand_bipartite_heterograph([rng,] \n                           (n1, n2), (m12, m21); \n                           bidirected = true, \n                           node_t = (:A, :B), \n                           edge_t = :to, \n                           kws...)\n\nConstruct an GNNHeteroGraph with random edges representing a bipartite graph. The graph will have two types of nodes, and edges will only connect nodes of different types.\n\nThe first argument is a tuple (n1, n2) specifying the number of nodes of each type. The second argument is a tuple (m12, m21) specifying the number of edges connecting nodes of type 1 to nodes of type 2  and vice versa.\n\nThe type of nodes and edges can be specified with the node_t and edge_t keyword arguments, which default to (:A, :B) and :to respectively.\n\nIf bidirected=true (default), the reverse edge of each edge will be present. In this case m12 == m21 is required.\n\nA random number generator can be passed as the first argument to make the generation reproducible.\n\nAdditional keyword arguments will be passed to the GNNHeteroGraph constructor.\n\nSee rand_heterograph for a more general version.\n\nExamples\n\njulia> g = rand_bipartite_heterograph((10, 15), 20)\nGNNHeteroGraph:\n  num_nodes: (:A => 10, :B => 15)\n  num_edges: ((:A, :to, :B) => 20, (:B, :to, :A) => 20)\n\njulia> g = rand_bipartite_heterograph((10, 15), (20, 0), node_t=(:user, :item), edge_t=:-, bidirected=false)\nGNNHeteroGraph:\n  num_nodes: Dict(:item => 15, :user => 10)\n  num_edges: Dict((:item, :-, :user) => 0, (:user, :-, :item) => 20)\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/api/heterograph/#GNNGraphs.rand_heterograph","page":"GNNHeteroGraph","title":"GNNGraphs.rand_heterograph","text":"rand_heterograph([rng,] n, m; bidirected=false, kws...)\n\nConstruct an GNNHeteroGraph with random edges and with number of nodes and edges  specified by n and m respectively. n and m can be any iterable of pairs specifing node/edge types and their numbers.\n\nPass a random number generator as a first argument to make the generation reproducible.\n\nSetting bidirected=true will generate a bidirected graph, i.e. each edge will have a reverse edge. Therefore, for each edge type (:A, :rel, :B) a corresponding reverse edge type (:B, :rel, :A) will be generated.\n\nAdditional keyword arguments will be passed to the GNNHeteroGraph constructor.\n\nExamples\n\njulia> g = rand_heterograph((:user => 10, :movie => 20),\n                            (:user, :rate, :movie) => 30)\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 20, :user => 10)\n  num_edges: Dict((:user, :rate, :movie) => 30)\n\n\n\n\n\n","category":"function"},{"location":"GNNGraphs/guides/datasets/#Datasets","page":"Datasets","title":"Datasets","text":"","category":"section"},{"location":"GNNGraphs/guides/datasets/","page":"Datasets","title":"Datasets","text":"GNNGraphs.jl doesn't come with its own datasets, but leverages those available in the Julia (and non-Julia) ecosystem. In particular, the examples in the GraphNeuralNetworks.jl repository make use of the MLDatasets.jl package. There you will find common graph datasets such as Cora, PubMed, Citeseer, TUDataset and many others. For graphs with static structures and temporal features, datasets such as METRLA, PEMSBAY, ChickenPox, and WindMillEnergy are available. For graphs featuring both temporal structures and temporal features, the TemporalBrains dataset is suitable.","category":"page"},{"location":"GNNGraphs/guides/datasets/","page":"Datasets","title":"Datasets","text":"GraphNeuralNetworks.jl provides the mldataset2gnngraph method for interfacing with MLDatasets.jl.","category":"page"},{"location":"GNNGraphs/guides/datasets/","page":"Datasets","title":"Datasets","text":"mldataset2gnngraph","category":"page"},{"location":"GNNGraphs/guides/datasets/#GNNGraphs.mldataset2gnngraph","page":"Datasets","title":"GNNGraphs.mldataset2gnngraph","text":"mldataset2gnngraph(dataset)\n\nConvert a graph dataset from the package MLDatasets.jl into one or many GNNGraphs.\n\nExamples\n\njulia> using MLDatasets, GNNGraphs\n\njulia> mldataset2gnngraph(Cora())\nGNNGraph:\n  num_nodes: 2708\n  num_edges: 10556\n  ndata:\n        val_mask = 2708-element BitVector\n        targets = 2708-element Vector{Int64}\n        test_mask = 2708-element BitVector\n        features = 1433×2708 Matrix{Float32}\n        train_mask = 2708-element BitVector\n\n\n\n\n\n","category":"function"},{"location":"api/temporalconv/","page":"Temporal Convolutional layers","title":"Temporal Convolutional layers","text":"CurrentModule = GNNLux\nCollapsedDocStrings = true","category":"page"},{"location":"api/temporalconv/#Temporal-Graph-Convolutional-Layers","page":"Temporal Convolutional layers","title":"Temporal Graph-Convolutional Layers","text":"","category":"section"},{"location":"api/temporalconv/","page":"Temporal Convolutional layers","title":"Temporal Convolutional layers","text":"Convolutions for time-varying graphs (temporal graphs) such as the TemporalSnapshotsGNNGraph.","category":"page"},{"location":"api/temporalconv/","page":"Temporal Convolutional layers","title":"Temporal Convolutional layers","text":"Modules = [GNNLux]\nPages   = [\"layers/temporalconv.jl\"]\nPrivate = false","category":"page"},{"location":"api/temporalconv/#GNNLux.A3TGCN","page":"Temporal Convolutional layers","title":"GNNLux.A3TGCN","text":"A3TGCN(in => out; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32, add_self_loops = false, use_edge_weight = true)\n\nAttention Temporal Graph Convolutional Network (A3T-GCN) model from the paper A3T-GCN: Attention Temporal Graph Convolutional Network for Traffic Forecasting.\n\nPerforms a TGCN layer, followed by a soft attention layer.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nuse_bias: Add learnable bias. Default true.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the GRU layer. Default zeros32.\ninit_bias: Bias initializer. Default zeros32.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default false.\nuse_edge_weight: If true, consider the edge weights in the input graph (if available).                    If add_self_loops=true the new weights will be set to 1.                     This option is ignored if the edge_weight is explicitly provided in the forward pass.                    Default false.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ng = rand_graph(rng, 5, 10)\nx = rand(rng, Float32, 2, 5)\n\n# create A3TGCN layer\nl = A3TGCN(2 => 6)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)      # result size (6, 5)\n\n\n\n\n\n","category":"type"},{"location":"api/temporalconv/#GNNLux.EvolveGCNO","page":"Temporal Convolutional layers","title":"GNNLux.EvolveGCNO","text":"EvolveGCNO(ch; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32)\n\nEvolving Graph Convolutional Network (EvolveGCNO) layer from the paper EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs.\n\nPerfoms a Graph Convolutional layer with parameters derived from a Long Short-Term Memory (LSTM) layer across the snapshots of the temporal graph.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nuse_bias: Add learnable bias. Default true.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the GRU layer. Default zeros32.\ninit_bias: Bias initializer. Default zeros32.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ntg = TemporalSnapshotsGNNGraph([rand_graph(rng, 10, 20; ndata = rand(rng, 4, 10)), rand_graph(rng, 10, 14; ndata = rand(rng, 4, 10)), rand_graph(rng, 10, 22; ndata = rand(rng, 4, 10))])\n\n# create layer\nl = EvolveGCNO(4 => 5)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(tg, tg.ndata.x , ps, st)      # result size 3, size y[1] (5, 10)\n\n\n\n\n\n","category":"type"},{"location":"api/temporalconv/#GNNLux.DCGRU-Tuple{Pair{Int64, Int64}, Int64}","page":"Temporal Convolutional layers","title":"GNNLux.DCGRU","text":"DCGRU(in => out, k; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32)\n\nDiffusion Convolutional Recurrent Neural Network (DCGRU) layer from the paper Diffusion Convolutional Recurrent Neural Network: Data-driven Traffic Forecasting.\n\nPerforms a Diffusion Convolutional layer to model spatial dependencies, followed by a Gated Recurrent Unit (GRU) cell to model temporal dependencies.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nk: Diffusion step.\nuse_bias: Add learnable bias. Default true.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the GRU layer. Default zeros32.\ninit_bias: Bias initializer. Default zeros32.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ng = rand_graph(rng, 5, 10)\nx = rand(rng, Float32, 2, 5)\n\n# create layer\nl = DCGRU(2 => 5, 2)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)      # result size (5, 5)\n\n\n\n\n\n","category":"method"},{"location":"api/temporalconv/#GNNLux.GConvGRU-Tuple{Pair{Int64, Int64}, Int64}","page":"Temporal Convolutional layers","title":"GNNLux.GConvGRU","text":"GConvGRU(in => out, k; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32)\n\nGraph Convolutional Gated Recurrent Unit (GConvGRU) recurrent layer from the paper Structured Sequence Modeling with Graph Convolutional Recurrent Networks.\n\nPerforms a layer of ChebConv to model spatial dependencies, followed by a Gated Recurrent Unit (GRU) cell to model temporal dependencies.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nk: Chebyshev polynomial order.\nuse_bias: Add learnable bias. Default true.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the GRU layer. Default zeros32.\ninit_bias: Bias initializer. Default zeros32.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ng = rand_graph(rng, 5, 10)\nx = rand(rng, Float32, 2, 5)\n\n# create layer\nl = GConvGRU(2 => 5, 2)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)      # result size (5, 5)\n\n\n\n\n\n","category":"method"},{"location":"api/temporalconv/#GNNLux.GConvLSTM-Tuple{Pair{Int64, Int64}, Int64}","page":"Temporal Convolutional layers","title":"GNNLux.GConvLSTM","text":"GConvLSTM(in => out, k; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32)\n\nGraph Convolutional Long Short-Term Memory (GConvLSTM) recurrent layer from the paper Structured Sequence Modeling with Graph Convolutional Recurrent Networks. \n\nPerforms a layer of ChebConv to model spatial dependencies, followed by a Long Short-Term Memory (LSTM) cell to model temporal dependencies.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nk: Chebyshev polynomial order.\nuse_bias: Add learnable bias. Default true.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the GRU layer. Default zeros32.\ninit_bias: Bias initializer. Default zeros32.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ng = rand_graph(rng, 5, 10)\nx = rand(rng, Float32, 2, 5)\n\n# create GConvLSTM layer\nl = GConvLSTM(2 => 5, 2)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)      # result size (5, 5)\n\n\n\n\n\n","category":"method"},{"location":"api/temporalconv/#GNNLux.TGCN-Tuple{Pair{Int64, Int64}}","page":"Temporal Convolutional layers","title":"GNNLux.TGCN","text":"TGCN(in => out; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32, add_self_loops = false, use_edge_weight = true)\n\nTemporal Graph Convolutional Network (T-GCN) recurrent layer from the paper T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction.\n\nPerforms a layer of GCNConv to model spatial dependencies, followed by a Gated Recurrent Unit (GRU) cell to model temporal dependencies.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nuse_bias: Add learnable bias. Default true.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the GRU layer. Default zeros32.\ninit_bias: Bias initializer. Default zeros32.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default false.\nuse_edge_weight: If true, consider the edge weights in the input graph (if available).                    If add_self_loops=true the new weights will be set to 1.                     This option is ignored if the edge_weight is explicitly provided in the forward pass.                    Default false.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ng = rand_graph(rng, 5, 10)\nx = rand(rng, Float32, 2, 5)\n\n# create TGCN layer\ntgcn = TGCN(2 => 6)\n\n# setup layer\nps, st = LuxCore.setup(rng, tgcn)\n\n# forward pass\ny, st = tgcn(g, x, ps, st)      # result size (6, 5)\n\n\n\n\n\n","category":"method"},{"location":"GNNGraphs/#GNNGraphs.jl","page":"GNNGraphs.jl","title":"GNNGraphs.jl","text":"","category":"section"},{"location":"GNNGraphs/","page":"GNNGraphs.jl","title":"GNNGraphs.jl","text":"GNNGraphs.jl is a package that provides graph data structures and helper functions specifically designed for working with graph neural networks. This package allows to store not only the graph structure, but also features associated with nodes, edges, and the graph itself. It is the core foundation for the GNNlib.jl, GraphNeuralNetworks.jl, and GNNLux.jl packages.","category":"page"},{"location":"GNNGraphs/","page":"GNNGraphs.jl","title":"GNNGraphs.jl","text":"It supports three types of graphs: ","category":"page"},{"location":"GNNGraphs/","page":"GNNGraphs.jl","title":"GNNGraphs.jl","text":"Static graph is the basic graph type represented by GNNGraph, where each node and edge can have associated features. This type of graph is used in typical graph neural network applications, where neural networks operate on both the structure of the graph and the features stored in it. It can be used to represent a graph where the structure does not change over time, but the features of the nodes and edges can change over time.\nHeterogeneous graph is a graph that supports multiple types of nodes and edges, and is represented by GNNHeteroGraph. Each type can have its own properties and relationships. This is useful in scenarios with different entities and interactions, such as in citation graphs or multi-relational data.\nTemporal graph is a graph that changes over time, and is represented by TemporalSnapshotsGNNGraph. Edges and features can change dynamically. This type of graph is useful for applications that involve tracking time-dependent relationships, such as social networks.","category":"page"},{"location":"GNNGraphs/","page":"GNNGraphs.jl","title":"GNNGraphs.jl","text":"This package depends on the package Graphs.jl.","category":"page"},{"location":"GNNGraphs/#Installation","page":"GNNGraphs.jl","title":"Installation","text":"","category":"section"},{"location":"GNNGraphs/","page":"GNNGraphs.jl","title":"GNNGraphs.jl","text":"The package can be installed with the Julia package manager. From the Julia REPL, type ] to enter the Pkg REPL mode and run:","category":"page"},{"location":"GNNGraphs/","page":"GNNGraphs.jl","title":"GNNGraphs.jl","text":"pkg> add GNNGraphs","category":"page"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"CurrentModule = GNNGraphs","category":"page"},{"location":"GNNGraphs/guides/heterograph/#Heterogeneous-Graphs","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"","category":"section"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"Heterogeneous graphs (also called heterographs), are graphs where each node has a type, that we denote with symbols such as :user and :movie. Relations such as :rate or :like can connect nodes of different types. We call a triplet (source_node_type, relation_type, target_node_type) the type of a edge, e.g. (:user, :rate, :movie).","category":"page"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"Different node/edge types can store different groups of features and this makes heterographs a very flexible modeling tools  and data containers. In GNNGraphs.jl heterographs are implemented in  the type GNNHeteroGraph.","category":"page"},{"location":"GNNGraphs/guides/heterograph/#Creating-a-Heterograph","page":"Heterogeneous Graphs","title":"Creating a Heterograph","text":"","category":"section"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"A heterograph can be created empty or by passing pairs edge_type => data to the constructor.","category":"page"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"julia> using GNNGraphs\n\njulia> g = GNNHeteroGraph()\nGNNHeteroGraph:\n  num_nodes: Dict()\n  num_edges: Dict()\n  \njulia> g = GNNHeteroGraph((:user, :like, :actor) => ([1,2,2,3], [1,3,2,9]),\n                          (:user, :rate, :movie) => ([1,1,2,3], [7,13,5,7]))\nGNNHeteroGraph:\n  num_nodes: Dict(:actor => 9, :movie => 13, :user => 3)\n  num_edges: Dict((:user, :like, :actor) => 4, (:user, :rate, :movie) => 4)\n\njulia> g = GNNHeteroGraph((:user, :rate, :movie) => ([1,1,2,3], [7,13,5,7]))\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 13, :user => 3)\n  num_edges: Dict((:user, :rate, :movie) => 4)","category":"page"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"New relations, possibly with new node types, can be added with the function add_edges.","category":"page"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"julia> g = add_edges(g, (:user, :like, :actor) => ([1,2,3,3,3], [3,5,1,9,4]))\nGNNHeteroGraph:\n  num_nodes: Dict(:actor => 9, :movie => 13, :user => 3)\n  num_edges: Dict((:user, :like, :actor) => 5, (:user, :rate, :movie) => 4)","category":"page"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"See rand_heterograph, rand_bipartite_heterograph for generating random heterographs. ","category":"page"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"julia> g = rand_bipartite_heterograph((10, 15), 20)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 10, :B => 15)\n  num_edges: Dict((:A, :to, :B) => 20, (:B, :to, :A) => 20)","category":"page"},{"location":"GNNGraphs/guides/heterograph/#Basic-Queries","page":"Heterogeneous Graphs","title":"Basic Queries","text":"","category":"section"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"Basic queries are similar to those for homogeneous graphs:","category":"page"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"julia> g = GNNHeteroGraph((:user, :rate, :movie) => ([1,1,2,3], [7,13,5,7]))\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 13, :user => 3)\n  num_edges: Dict((:user, :rate, :movie) => 4)\n\njulia> g.num_nodes\nDict{Symbol, Int64} with 2 entries:\n  :user  => 3\n  :movie => 13\n\njulia> g.num_edges\nDict{Tuple{Symbol, Symbol, Symbol}, Int64} with 1 entry:\n  (:user, :rate, :movie) => 4\n\njulia> edge_index(g, (:user, :rate, :movie)) # source and target node for a given relation\n([1, 1, 2, 3], [7, 13, 5, 7])\n\njulia> g.ntypes  # node types\n2-element Vector{Symbol}:\n :user\n :movie\n\njulia> g.etypes  # edge types\n1-element Vector{Tuple{Symbol, Symbol, Symbol}}:\n (:user, :rate, :movie)","category":"page"},{"location":"GNNGraphs/guides/heterograph/#Data-Features","page":"Heterogeneous Graphs","title":"Data Features","text":"","category":"section"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"Node, edge, and graph features can be added at construction time or later using:","category":"page"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"# equivalent to g.ndata[:user][:x] = ...\njulia> g[:user].x = rand(Float32, 64, 3);\n\njulia> g[:movie].z = rand(Float32, 64, 13);\n\n# equivalent to g.edata[(:user, :rate, :movie)][:e] = ...\njulia> g[:user, :rate, :movie].e = rand(Float32, 64, 4);\n\njulia> g\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 13, :user => 3)\n  num_edges: Dict((:user, :rate, :movie) => 4)\n  ndata:\n        :movie  =>  DataStore(z = [64×13 Matrix{Float32}])\n        :user  =>  DataStore(x = [64×3 Matrix{Float32}])\n  edata:\n        (:user, :rate, :movie)  =>  DataStore(e = [64×4 Matrix{Float32}])","category":"page"},{"location":"GNNGraphs/guides/heterograph/#Batching","page":"Heterogeneous Graphs","title":"Batching","text":"","category":"section"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"Similarly to graphs, also heterographs can be batched together.","category":"page"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"julia> gs = [rand_bipartite_heterograph((5, 10), 20) for _ in 1:32];\n\njulia> MLUtils.batch(gs)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 160, :B => 320)\n  num_edges: Dict((:A, :to, :B) => 640, (:B, :to, :A) => 640)\n  num_graphs: 32","category":"page"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"Batching is automatically performed by the DataLoader iterator when the collate option is set to true.","category":"page"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"using MLUtils: DataLoader\n\ndata = [rand_bipartite_heterograph((5, 10), 20, \n            ndata=Dict(:A=>rand(Float32, 3, 5))) \n        for _ in 1:320];\n\ntrain_loader = DataLoader(data, batchsize=16, shuffle=true, collate=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes[:A] == 80\n    @assert size(g.ndata[:A].x) == (3, 80)    \n    # ...\nend","category":"page"},{"location":"GNNGraphs/guides/heterograph/#Graph-convolutions-on-heterographs","page":"Heterogeneous Graphs","title":"Graph convolutions on heterographs","text":"","category":"section"},{"location":"GNNGraphs/guides/heterograph/","page":"Heterogeneous Graphs","title":"Heterogeneous Graphs","text":"See HeteroGraphConv for how to perform convolutions on heterogeneous graphs.","category":"page"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"CurrentModule = GNNLux\nCollapsedDocStrings = true","category":"page"},{"location":"api/conv/#Convolutional-Layers","page":"Convolutional layers","title":"Convolutional Layers","text":"","category":"section"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"Many different types of graphs convolutional layers have been proposed in the literature. Choosing the right layer for your application could involve a lot of exploration.  Multiple graph convolutional layers are typically stacked together to create a graph neural network model (see GNNChain).","category":"page"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"The table below lists all graph convolutional layers implemented in the GNNLux.jl. It also highlights the presence of some additional capabilities with respect to basic message passing:","category":"page"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"Sparse Ops: implements message passing as multiplication by sparse adjacency matrix instead of the gather/scatter mechanism. This can lead to better CPU performances but it is not supported on GPU yet. \nEdge Weight: supports scalar weights (or equivalently scalar features) on edges. \nEdge Features: supports feature vectors on edges.\nHeterograph: supports heterogeneous graphs (see GNNHeteroGraph).\nTemporalSnapshotsGNNGraphs: supports temporal graphs (see TemporalSnapshotsGNNGraph) by applying the convolution layers to each snapshot independently.","category":"page"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"Layer Sparse Ops Edge Weight Edge Features Heterograph TemporalSnapshotsGNNGraphs\nAGNNConv   ✓  \nCGConv   ✓ ✓ ✓\nChebConv     ✓\nEGNNConv   ✓  \nEdgeConv    ✓ \nGATConv   ✓ ✓ ✓\nGATv2Conv   ✓ ✓ ✓\nGatedGraphConv ✓    ✓\nGCNConv ✓ ✓  ✓ \nGINConv ✓   ✓ ✓\nGMMConv   ✓  \nGraphConv ✓   ✓ ✓\nMEGNetConv   ✓  \nNNConv   ✓  \nResGatedGraphConv    ✓ ✓\nSAGEConv ✓   ✓ ✓\nSGConv ✓    ✓","category":"page"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"Modules = [GNNLux]\nPages   = [\"layers/conv.jl\"]\nPrivate = false","category":"page"},{"location":"api/conv/#GNNLux.AGNNConv","page":"Convolutional layers","title":"GNNLux.AGNNConv","text":"AGNNConv(; init_beta=1.0f0, trainable=true, add_self_loops=true)\n\nAttention-based Graph Neural Network layer from paper Attention-based Graph Neural Network for Semi-Supervised Learning.\n\nThe forward pass is given by\n\nmathbfx_i = sum_j in N(i) alpha_ij mathbfx_j\n\nwhere the attention coefficients alpha_ij are given by\n\nalpha_ij =frace^beta cos(mathbfx_i mathbfx_j)\n                  sum_je^beta cos(mathbfx_i mathbfx_j)\n\nwith the cosine distance defined by\n\ncos(mathbfx_i mathbfx_j) = \n  fracmathbfx_i cdot mathbfx_jlVertmathbfx_irVert lVertmathbfx_jrVert\n\nand beta a trainable parameter if trainable=true.\n\nArguments\n\ninit_beta: The initial value of beta. Default 1.0f0.\ntrainable: If true, beta is trainable. Default true.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default true.\n\nExamples:\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\n\n# create layer\nl = AGNNConv(init_beta=2.0f0)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)   \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.CGConv","page":"Convolutional layers","title":"GNNLux.CGConv","text":"CGConv((in, ein) => out, act = identity; residual = false,\n            use_bias = true, init_weight = glorot_uniform, init_bias = zeros32)\nCGConv(in => out, ...)\n\nThe crystal graph convolutional layer from the paper Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties. Performs the operation\n\nmathbfx_i = mathbfx_i + sum_jin N(i)sigma(W_f mathbfz_ij + mathbfb_f) act(W_s mathbfz_ij + mathbfb_s)\n\nwhere mathbfz_ij  is the node and edge features concatenation  mathbfx_i mathbfx_j mathbfe_jto i  and sigma is the sigmoid function. The residual mathbfx_i is added only if residual=true and the output size is the same  as the input size.\n\nArguments\n\nin: The dimension of input node features.\nein: The dimension of input edge features. \n\nIf ein is not given, assumes that no edge features are passed as input in the forward pass.\n\nout: The dimension of output node features.\nact: Activation function.\nresidual: Add a residual connection.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create random graph\ng = rand_graph(rng, 5, 6)\nx = rand(rng, Float32, 2, g.num_nodes)\ne = rand(rng, Float32, 3, g.num_edges)\n\nl = CGConv((2, 3) => 4, tanh)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, e, ps, st)    # size: (4, num_nodes)\n\n# No edge features\nl = CGConv(2 => 4, tanh)\nps, st = LuxCore.setup(rng, l)\ny, st = l(g, x, ps, st)    # size: (4, num_nodes)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.ChebConv","page":"Convolutional layers","title":"GNNLux.ChebConv","text":"ChebConv(in => out, k; init_weight = glorot_uniform, init_bias = zeros32, use_bias = true)\n\nChebyshev spectral graph convolutional layer from paper Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering.\n\nImplements\n\nX = sum^K-1_k=0  W^(k) Z^(k)\n\nwhere Z^(k) is the k-th term of Chebyshev polynomials, and can be calculated by the following recursive form:\n\nbeginaligned\nZ^(0) = X \nZ^(1) = hatL X \nZ^(k) = 2 hatL Z^(k-1) - Z^(k-2)\nendaligned\n\nwith hatL the scaled_laplacian.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nk: The order of Chebyshev polynomial.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = ChebConv(3 => 5, 5)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size of the output y:  5 × num_nodes\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.DConv","page":"Convolutional layers","title":"GNNLux.DConv","text":"DConv(in => out, k; init_weight = glorot_uniform, init_bias = zeros32, use_bias = true)\n\nDiffusion convolution layer from the paper Diffusion Convolutional Recurrent Neural Networks: Data-Driven Traffic Forecasting.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nk: Number of diffusion steps.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create random graph\ng = GNNGraph(rand(rng, 10, 10), ndata = rand(rng, Float32, 2, 10))\n\ndconv = DConv(2 => 4, 4)\n\n# setup layer\nps, st = LuxCore.setup(rng, dconv)\n\n# forward pass\ny, st = dconv(g, g.ndata.x, ps, st)   # size: (4, num_nodes)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.EGNNConv","page":"Convolutional layers","title":"GNNLux.EGNNConv","text":"EGNNConv((in, ein) => out; hidden_size=2in, residual=false)\nEGNNConv(in => out; hidden_size=2in, residual=false)\n\nEquivariant Graph Convolutional Layer from E(n) Equivariant Graph Neural Networks.\n\nThe layer performs the following operation:\n\nbeginaligned\nmathbfm_jto i =phi_e(mathbfh_i mathbfh_j lVertmathbfx_i-mathbfx_jrVert^2 mathbfe_jto i)\nmathbfx_i = mathbfx_i + C_isum_jinmathcalN(i)(mathbfx_i-mathbfx_j)phi_x(mathbfm_jto i)\nmathbfm_i = C_isum_jinmathcalN(i) mathbfm_jto i\nmathbfh_i = mathbfh_i + phi_h(mathbfh_i mathbfm_i)\nendaligned\n\nwhere mathbfh_i, mathbfx_i, mathbfe_jto i are invariant node features, equivariant node features, and edge features respectively. phi_e, phi_h, and phi_x are two-layer MLPs. C is a constant for normalization, computed as 1mathcalN(i).\n\nConstructor Arguments\n\nin: Number of input features for h.\nout: Number of output features for h.\nein: Number of input edge features.\nhidden_size: Hidden representation size.\nresidual: If true, add a residual connection. Only possible if in == out. Default false.\n\nForward Pass\n\nl(g, x, h, e=nothing, ps, st)\n\nForward Pass Arguments:\n\ng : The graph.\nx : Matrix of equivariant node coordinates.\nh : Matrix of invariant node features.\ne : Matrix of invariant edge features. Default nothing.\nps : Parameters.\nst : State.\n\nReturns updated h and x.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create random graph\ng = rand_graph(rng, 10, 10)\nh = randn(rng, Float32, 5, g.num_nodes)\nx = randn(rng, Float32, 3, g.num_nodes)\n\negnn = EGNNConv(5 => 6, 10)\n\n# setup layer\nps, st = LuxCore.setup(rng, egnn)\n\n# forward pass\n(hnew, xnew), st = egnn(g, h, x, ps, st)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.EdgeConv","page":"Convolutional layers","title":"GNNLux.EdgeConv","text":"EdgeConv(nn; aggr=max)\n\nEdge convolutional layer from paper Dynamic Graph CNN for Learning on Point Clouds.\n\nPerforms the operation\n\nmathbfx_i = square_j in N(i) nn(mathbfx_i mathbfx_j - mathbfx_i)\n\nwhere nn generally denotes a learnable function, e.g. a linear layer or a multi-layer perceptron.\n\nArguments\n\nnn: A (possibly learnable) function. \naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\n\nExamples:\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = rand(rng, Float32, in_channel, g.num_nodes)\n\n# create layer\nl = EdgeConv(Dense(2 * in_channel, out_channel), aggr = +)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.GATConv","page":"Convolutional layers","title":"GNNLux.GATConv","text":"GATConv(in => out, σ = identity; heads = 1, concat = true, negative_slope = 0.2, init_weight = glorot_uniform, init_bias = zeros32, use_bias = true, add_self_loops = true, dropout=0.0)\nGATConv((in, ein) => out, ...)\n\nGraph attentional layer from the paper Graph Attention Networks.\n\nImplements the operation\n\nmathbfx_i = sum_j in N(i) cup i alpha_ij W mathbfx_j\n\nwhere the attention coefficients alpha_ij are given by\n\nalpha_ij = frac1z_i exp(LeakyReLU(mathbfa^T W mathbfx_i W mathbfx_j))\n\nwith z_i a normalization factor. \n\nIn case ein > 0 is given, edge features of dimension ein will be expected in the forward pass  and the attention coefficients will be calculated as  \n\nalpha_ij = frac1z_i exp(LeakyReLU(mathbfa^T W_e mathbfe_jto i W mathbfx_i W mathbfx_j))\n\nArguments\n\nin: The dimension of input node features.\nein: The dimension of input edge features. Default 0 (i.e. no edge features passed in the forward).\nout: The dimension of output node features.\nσ: Activation function. Default identity.\nheads: Number attention heads. Default 1.\nconcat: Concatenate layer output or not. If not, layer output is averaged over the heads. Default true.\nnegative_slope: The parameter of LeakyReLU.Default 0.2.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default true.\ndropout: Dropout probability on the normalized attention coefficient. Default 0.0.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = GATConv(in_channel => out_channel; add_self_loops = false, use_bias = false, heads=2, concat=true)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.GATv2Conv","page":"Convolutional layers","title":"GNNLux.GATv2Conv","text":"GATv2Conv(in => out, σ = identity; heads = 1, concat = true, negative_slope = 0.2, init_weight = glorot_uniform, init_bias = zeros32, use_bias = true, add_self_loops = true, dropout=0.0)\nGATv2Conv((in, ein) => out, ...)\n\nGATv2 attentional layer from the paper How Attentive are Graph Attention Networks?.\n\nImplements the operation\n\nmathbfx_i = sum_j in N(i) cup i alpha_ij W_1 mathbfx_j\n\nwhere the attention coefficients alpha_ij are given by\n\nalpha_ij = frac1z_i exp(mathbfa^T LeakyReLU(W_2 mathbfx_i + W_1 mathbfx_j))\n\nwith z_i a normalization factor.\n\nIn case ein > 0 is given, edge features of dimension ein will be expected in the forward pass  and the attention coefficients will be calculated as  \n\nalpha_ij = frac1z_i exp(mathbfa^T LeakyReLU(W_3 mathbfe_jto i + W_2 mathbfx_i + W_1 mathbfx_j))\n\nArguments\n\nin: The dimension of input node features.\nein: The dimension of input edge features. Default 0 (i.e. no edge features passed in the forward).\nout: The dimension of output node features.\nσ: Activation function. Default identity.\nheads: Number attention heads. Default 1.\nconcat: Concatenate layer output or not. If not, layer output is averaged over the heads. Default true.\nnegative_slope: The parameter of LeakyReLU.Default 0.2.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default true.\ndropout: Dropout probability on the normalized attention coefficient. Default 0.0.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\nein = 3\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = GATv2Conv((in_channel, ein) => out_channel, add_self_loops = false)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# edge features\ne = randn(rng, Float32, ein, length(s))\n\n# forward pass\ny, st = l(g, x, e, ps, st)    \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.GCNConv","page":"Convolutional layers","title":"GNNLux.GCNConv","text":"GCNConv(in => out, σ=identity; [init_weight, init_bias, use_bias, add_self_loops, use_edge_weight])\n\nGraph convolutional layer from paper Semi-supervised Classification with Graph Convolutional Networks.\n\nPerforms the operation\n\nmathbfx_i = sum_jin N(i) a_ij W mathbfx_j\n\nwhere a_ij = 1  sqrtN(i)N(j) is a normalization factor computed from the node degrees. \n\nIf the input graph has weighted edges and use_edge_weight=true, than a_ij will be computed as\n\na_ij = frace_jto isqrtsum_j in N(i)  e_jto i sqrtsum_i in N(j)  e_ito j\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nσ: Activation function. Default identity.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default false.\nuse_edge_weight: If true, consider the edge weights in the input graph (if available).                    If add_self_loops=true the new weights will be set to 1.                     This option is ignored if the edge_weight is explicitly provided in the forward pass.                    Default false.\n\nForward\n\n(::GCNConv)(g, x, [edge_weight], ps, st; norm_fn = d -> 1 ./ sqrt.(d), conv_weight=nothing)\n\nTakes as input a graph g, a node feature matrix x of size [in, num_nodes], optionally an edge weight vector and the parameter and state of the layer. Returns a node feature matrix of size  [out, num_nodes].\n\nThe norm_fn parameter allows for custom normalization of the graph convolution operation by passing a function as argument.  By default, it computes frac1sqrtd i.e the inverse square root of the degree (d) of each node in the graph.  If conv_weight is an AbstractMatrix of size [out, in], then the convolution is performed using that weight matrix.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = GCNConv(3 => 5) \n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny = l(g, x, ps, st)       # size of the output first entry:  5 × num_nodes\n\n# convolution with edge weights and custom normalization function\nw = [1.1, 0.1, 2.3, 0.5]\ncustom_norm_fn(d) = 1 ./ sqrt.(d + 1)  # Custom normalization function\ny = l(g, x, w, ps, st; norm_fn = custom_norm_fn)\n\n# Edge weights can also be embedded in the graph.\ng = GNNGraph(s, t, w)\nl = GCNConv(3 => 5, use_edge_weight=true)\nps, st = Lux.setup(rng, l)\ny = l(g, x, ps, st) # same as l(g, x, w) \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.GINConv","page":"Convolutional layers","title":"GNNLux.GINConv","text":"GINConv(f, ϵ; aggr=+)\n\nGraph Isomorphism convolutional layer from paper How Powerful are Graph Neural Networks?.\n\nImplements the graph convolution\n\nmathbfx_i = f_Thetaleft((1 + epsilon) mathbfx_i + sum_j in N(i) mathbfx_j right)\n\nwhere f_Theta typically denotes a learnable function, e.g. a linear layer or a multi-layer perceptron.\n\nArguments\n\nf: A (possibly learnable) function acting on node features. \nϵ: Weighting factor.\n\nExamples:\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(rng, Float32, in_channel, g.num_nodes)\n\n# create dense layer\nnn = Dense(in_channel, out_channel)\n\n# create layer\nl = GINConv(nn, 0.01f0, aggr = mean)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size:  out_channel × num_nodes\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.GMMConv","page":"Convolutional layers","title":"GNNLux.GMMConv","text":"GMMConv((in, ein) => out, σ=identity; K = 1, residual = false init_weight = glorot_uniform, init_bias = zeros32, use_bias = true)\n\nGraph mixture model convolution layer from the paper Geometric deep learning on graphs and manifolds using mixture model CNNs Performs the operation\n\nmathbfx_i = mathbfx_i + frac1N(i) sum_jin N(i)frac1Ksum_k=1^K mathbfw_k(mathbfe_jto i) odot Theta_k mathbfx_j\n\nwhere w^a_k(e^a) for feature a and kernel k is given by\n\nw^a_k(e^a) = exp(-frac12(e^a - mu^a_k)^T (Sigma^-1)^a_k(e^a - mu^a_k))\n\nTheta_k mu^a_k (Sigma^-1)^a_k are learnable parameters.\n\nThe input to the layer is a node feature array x of size (num_features, num_nodes) and edge pseudo-coordinate array e of size (num_features, num_edges) The residual mathbfx_i is added only if residual=true and the output size is the same  as the input size.\n\nArguments\n\nin: Number of input node features.\nein: Number of input edge features.\nout: Number of output features.\nσ: Activation function. Default identity.\nK: Number of kernels. Default 1.\nresidual: Residual conncetion. Default false.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s,t)\nnin, ein, out, K = 4, 10, 7, 8 \nx = randn(rng, Float32, nin, g.num_nodes)\ne = randn(rng, Float32, ein, g.num_edges)\n\n# create layer\nl = GMMConv((nin, ein) => out, K=K)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, e, ps, st)       # size:  out × num_nodes\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.GatedGraphConv","page":"Convolutional layers","title":"GNNLux.GatedGraphConv","text":"GatedGraphConv(out, num_layers; \n        aggr = +, init_weight = glorot_uniform)\n\nGated graph convolution layer from Gated Graph Sequence Neural Networks.\n\nImplements the recursion\n\nbeginaligned\nmathbfh^(0)_i = mathbfx_i mathbf0 \nmathbfh^(l)_i = GRU(mathbfh^(l-1)_i square_j in N(i) W mathbfh^(l-1)_j)\nendaligned\n\nwhere mathbfh^(l)_i denotes the l-th hidden variables passing through GRU. The dimension of input mathbfx_i needs to be less or equal to out.\n\nArguments\n\nout: The dimension of output features.\nnum_layers: The number of recursion steps.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\ninit_weight: Weights' initializer. Default glorot_uniform.\n\nExamples:\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nout_channel = 5\nnum_layers = 3\ng = GNNGraph(s, t)\n\n# create layer\nl = GatedGraphConv(out_channel, num_layers)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size:  out_channel × num_nodes  \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.GraphConv","page":"Convolutional layers","title":"GNNLux.GraphConv","text":"GraphConv(in => out, σ = identity; aggr = +, init_weight = glorot_uniform,init_bias = zeros32, use_bias = true)\n\nGraph convolution layer from Reference: Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks.\n\nPerforms:\n\nmathbfx_i = W_1 mathbfx_i + square_j in mathcalN(i) W_2 mathbfx_j\n\nwhere the aggregation type is selected by aggr.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nσ: Activation function.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = GraphConv(in_channel => out_channel, relu, use_bias = false, aggr = mean)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size of the output y:  5 × num_nodes\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.MEGNetConv","page":"Convolutional layers","title":"GNNLux.MEGNetConv","text":"MEGNetConv(ϕe, ϕv; aggr=mean)\nMEGNetConv(in => out; aggr=mean)\n\nConvolution from Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals paper. In the forward pass, takes as inputs node features x and edge features e and returns updated features x' and e' according to \n\nbeginaligned\nmathbfe_ito j  = phi_e(mathbfx_i  mathbfx_j  mathbfe_ito j)\nmathbfx_i  = phi_v(mathbfx_i square_jin mathcalN(i)mathbfe_jto i)\nendaligned\n\naggr defines the aggregation to be performed.\n\nIf the neural networks ϕe and  ϕv are not provided, they will be constructed from the in and out arguments instead as multi-layer perceptron with one hidden layer and relu  activations.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create a random graph\ng = rand_graph(rng, 10, 30)\nx = randn(rng, Float32, 3, 10)\ne = randn(rng, Float32, 3, 30)\n\n# create a MEGNetConv layer\nm = MEGNetConv(3 => 3)\n\n# setup layer\nps, st = LuxCore.setup(rng, m)\n\n# forward pass\n(x′, e′), st = m(g, x, e, ps, st)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.NNConv","page":"Convolutional layers","title":"GNNLux.NNConv","text":"NNConv(in => out, f, σ=identity; aggr=+, init_bias = zeros32, use_bias = true, init_weight = glorot_uniform)\n\nThe continuous kernel-based convolutional operator from the  Neural Message Passing for Quantum Chemistry paper.  This convolution is also known as the edge-conditioned convolution from the  Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs paper.\n\nPerforms the operation\n\nmathbfx_i = W mathbfx_i + square_j in N(i) f_Theta(mathbfe_jto i)mathbfx_j\n\nwhere f_Theta  denotes a learnable function (e.g. a linear layer or a multi-layer perceptron). Given an input of batched edge features e of size (num_edge_features, num_edges),  the function f will return an batched matrices array whose size is (out, in, num_edges). For convenience, also functions returning a single (out*in, num_edges) matrix are allowed.\n\nArguments\n\nin: The dimension of input node features.\nout: The dimension of output node features.\nf: A (possibly learnable) function acting on edge features.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\nσ: Activation function.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples:\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\nn_in = 3\nn_in_edge = 10\nn_out = 5\n\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(rng, Float32, n_in, g.num_nodes)\ne = randn(rng, Float32, n_in_edge, g.num_edges)\n\n# create dense layer\nnn = Dense(n_in_edge => n_out * n_in)\n\n# create layer\nl = NNConv(n_in => n_out, nn, tanh, use_bias = true, aggr = +)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, e, ps, st)       # size:  n_out × num_nodes \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.ResGatedGraphConv","page":"Convolutional layers","title":"GNNLux.ResGatedGraphConv","text":"ResGatedGraphConv(in => out, act=identity; init_weight = glorot_uniform, init_bias = zeros32, use_bias = true)\n\nThe residual gated graph convolutional operator from the Residual Gated Graph ConvNets paper.\n\nThe layer's forward pass is given by\n\nmathbfx_i = actbig(Umathbfx_i + sum_j in N(i) eta_ij V mathbfx_jbig)\n\nwhere the edge gates eta_ij are given by\n\neta_ij = sigmoid(Amathbfx_i + Bmathbfx_j)\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nact: Activation function.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples:\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(rng, Float32, in_channel, g.num_nodes)\n\n# create layer\nl = ResGatedGraphConv(in_channel => out_channel, tanh, use_bias = true)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size:  out_channel × num_nodes  \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.SAGEConv","page":"Convolutional layers","title":"GNNLux.SAGEConv","text":"SAGEConv(in => out, σ=identity; aggr=mean, init_weight = glorot_uniform, init_bias = zeros32, use_bias=true)\n\nGraphSAGE convolution layer from paper Inductive Representation Learning on Large Graphs.\n\nPerforms:\n\nmathbfx_i = W cdot mathbfx_i square_j in mathcalN(i) mathbfx_j\n\nwhere the aggregation type is selected by aggr.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nσ: Activation function.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples:\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = rand(rng, Float32, in_channel, g.num_nodes)\n\n# create layer\nl = SAGEConv(in_channel => out_channel, tanh, use_bias = false, aggr = +)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size:  out_channel × num_nodes   \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.SGConv","page":"Convolutional layers","title":"GNNLux.SGConv","text":"SGConv(int => out, k = 1; init_weight = glorot_uniform, init_bias = zeros32, use_bias = true, add_self_loops = true,use_edge_weight = false)\n\nSGC layer from Simplifying Graph Convolutional Networks Performs operation\n\nH^K = (tildeD^-12 tildeA tildeD^-12)^K X Theta\n\nwhere tildeA is A + I.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nk : Number of hops k. Default 1.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default false.\nuse_edge_weight: If true, consider the edge weights in the input graph (if available).                    If add_self_loops=true the new weights will be set to 1. Default false.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = SGConv(3 => 5; add_self_loops = true) \n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size:  5 × num_nodes\n\n# convolution with edge weights\nw = [1.1, 0.1, 2.3, 0.5]\ny = l(g, x, w, ps, st)\n\n# Edge weights can also be embedded in the graph.\ng = GNNGraph(s, t, w)\nl = SGConv(3 => 5, add_self_loops = true, use_edge_weight=true) \nps, st = LuxCore.setup(rng, l)\ny, st = l(g, x, ps, st) # same as l(g, x, w) \n\n\n\n\n\n","category":"type"},{"location":"guides/models/#Models","page":"Models","title":"Models","text":"","category":"section"},{"location":"guides/models/","page":"Models","title":"Models","text":"GNNLux.jl provides common graph convolutional layers by which you can assemble arbitrarily deep or complex models. GNN layers are compatible with  Lux.jl ones, therefore expert Lux users are promptly able to define and train  their models. ","category":"page"},{"location":"guides/models/","page":"Models","title":"Models","text":"In what follows, we discuss two different styles for model creation: the explicit modeling style, more verbose but more flexible,  and the implicit modeling style based on GNNLux.GNNChain, more concise but less flexible.","category":"page"},{"location":"guides/models/#Explicit-modeling","page":"Models","title":"Explicit modeling","text":"","category":"section"},{"location":"guides/models/","page":"Models","title":"Models","text":"In the explicit modeling style, the model is created according to the following steps:","category":"page"},{"location":"guides/models/","page":"Models","title":"Models","text":"Define a new type for your model (GNN in the example below). Refer to the","category":"page"},{"location":"guides/models/","page":"Models","title":"Models","text":"Lux Manual for the  definition of the type.","category":"page"},{"location":"guides/models/","page":"Models","title":"Models","text":"Define a convenience constructor for your model.\nDefine the forward pass by implementing the call method for your type.\nInstantiate the model. ","category":"page"},{"location":"guides/models/","page":"Models","title":"Models","text":"Here is an example of this construction:","category":"page"},{"location":"guides/models/","page":"Models","title":"Models","text":"using Lux, GNNLux\nusing Zygote\nusing Random, Statistics\n\nstruct GNN <: AbstractLuxContainerLayer{(:conv1, :bn, :conv2, :dropout, :dense)} # step 1\n    conv1\n    bn\n    conv2\n    dropout\n    dense\nend\n\nfunction GNN(din::Int, d::Int, dout::Int) # step 2\n    GNN(GraphConv(din => d),\n        BatchNorm(d),\n        GraphConv(d => d, relu),\n        Dropout(0.5),\n        Dense(d, dout))\nend\n\nfunction (model::GNN)(g::GNNGraph, x, ps, st) # step 3\n    x, st_conv1 = model.conv1(g, x, ps.conv1, st.conv1)\n    x, st_bn = model.bn(x, ps.bn, st.bn)\n    x = relu.(x)\n    x, st_conv2 = model.conv2(g, x, ps.conv2, st.conv2)\n    x, st_drop = model.dropout(x, ps.dropout, st.dropout)\n    x, st_dense = model.dense(x, ps.dense, st.dense)\n    return x, (conv1=st_conv1, bn=st_bn, conv2=st_conv2, dropout=st_drop, dense=st_dense)\nend\n\ndin, d, dout = 3, 4, 2 \nmodel = GNN(din, d, dout)                 # step 4\nrng = Random.default_rng()\nps, st = Lux.setup(rng, model)\ng = rand_graph(rng, 10, 30)\nX = randn(Float32, din, 10) \n\nst = Lux.testmode(st)\ny, st = model(g, X, ps, st) \nst = Lux.trainmode(st)\ngrad = Zygote.gradient(ps -> mean(model(g, X, ps, st)[1]), ps)[1]","category":"page"},{"location":"guides/models/#Implicit-modeling-with-GNNChains","page":"Models","title":"Implicit modeling with GNNChains","text":"","category":"section"},{"location":"guides/models/","page":"Models","title":"Models","text":"While very flexible, the way in which we defined GNN model definition in last section is a bit verbose. In order to simplify things, we provide the GNNLux.GNNChain type. It is very similar  to Lux's well known Chain. It allows to compose layers in a sequential fashion as Chain does, propagating the output of each layer to the next one. In addition, GNNChain   propagates the input graph as well, providing it as a first argument to layers subtyping the GNNLux.GNNLayer abstract type. ","category":"page"},{"location":"guides/models/","page":"Models","title":"Models","text":"Using GNNChain, the model definition becomes more concise:","category":"page"},{"location":"guides/models/","page":"Models","title":"Models","text":"model = GNNChain(GraphConv(din => d),\n                 BatchNorm(d),\n                 x -> relu.(x),\n                 GraphConv(d => d, relu),\n                 Dropout(0.5),\n                 Dense(d, dout))","category":"page"},{"location":"guides/models/","page":"Models","title":"Models","text":"The GNNChain only propagates the graph and the node features. More complex scenarios, e.g. when also edge features are updated, have to be handled using the explicit definition of the forward pass. ","category":"page"},{"location":"GNNlib/#GNNlib.jl","page":"GNNlib.jl","title":"GNNlib.jl","text":"","category":"section"},{"location":"GNNlib/","page":"GNNlib.jl","title":"GNNlib.jl","text":"GNNlib.jl is a package that provides the implementation of the basic message passing functions and  functional implementation of graph convolutional layers, which are used to build graph neural networks in both the Flux.jl and Lux.jl machine learning frameworks, created in the GraphNeuralNetworks.jl and GNNLux.jl packages, respectively.","category":"page"},{"location":"GNNlib/","page":"GNNlib.jl","title":"GNNlib.jl","text":"This package depends on GNNGraphs.jl and NNlib.jl, and is primarily intended for developers looking to create new GNN architectures. For most users, the higher-level GraphNeuralNetworks.jl and GNNLux.jl packages are recommended.","category":"page"},{"location":"GNNlib/#Installation","page":"GNNlib.jl","title":"Installation","text":"","category":"section"},{"location":"GNNlib/","page":"GNNlib.jl","title":"GNNlib.jl","text":"The package can be installed with the Julia package manager. From the Julia REPL, type ] to enter the Pkg REPL mode and run:","category":"page"},{"location":"GNNlib/","page":"GNNlib.jl","title":"GNNlib.jl","text":"pkg> add GNNlib","category":"page"},{"location":"#GNNLux.jl","page":"Home","title":"GNNLux.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GNNLux.jl is a package that implements graph convolutional layers fully compatible with the Lux.jl deep learning framework. It is built on top of the GNNGraphs.jl, GNNlib.jl, and Lux.jl packages.","category":"page"},{"location":"","page":"Home","title":"Home","text":"See GraphNeuralNetworks.jl instead for a  Flux.jl-based implementation of graph neural networks.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GNNLux.jl is a registered Julia package. You can easily install it through the package manager :","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add GNNLux","category":"page"},{"location":"#Package-overview","page":"Home","title":"Package overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Let's give a brief overview of the package by solving a graph regression problem with synthetic data. ","category":"page"},{"location":"#Data-preparation","page":"Home","title":"Data preparation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We generate a dataset of multiple random graphs with associated data features, then split it into training and testing sets.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GNNLux, Lux, Statistics, MLUtils, Random\nusing Zygote, Optimisers\n\nrng = Random.default_rng()\n\nall_graphs = GNNGraph[]\n\nfor _ in 1:1000\n    g = rand_graph(rng, 10, 40,  \n            ndata=(; x = randn(rng, Float32, 16,10)),  # Input node features\n            gdata=(; y = randn(rng, Float32)))         # Regression target   \n    push!(all_graphs, g)\nend\n\ntrain_graphs, test_graphs = MLUtils.splitobs(all_graphs, at=0.8)","category":"page"},{"location":"#Model-building","page":"Home","title":"Model building","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We concisely define our model as a GNNLux.GNNChain containing two graph convolutional layers and initialize the model's parameters and state.","category":"page"},{"location":"","page":"Home","title":"Home","text":"model = GNNChain(GCNConv(16 => 64),\n                x -> relu.(x),    \n                Dropout(0.6), \n                GCNConv(64 => 64, relu),\n                x -> mean(x, dims=2),\n                Dense(64, 1)) \n\nps, st = LuxCore.setup(rng, model)","category":"page"},{"location":"#Training","page":"Home","title":"Training","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Finally, we use a standard Lux training pipeline to fit our dataset.","category":"page"},{"location":"","page":"Home","title":"Home","text":"function custom_loss(model, ps, st, tuple)\n    g,x,y = tuple\n    y_pred,st = model(g, x, ps, st)  \n    return MSELoss()(y_pred, y), (layers = st,), 0\nend\n\nfunction train_model!(model, ps, st, train_graphs, test_graphs)\n    train_state = Lux.Training.TrainState(model, ps, st, Adam(0.0001f0))\n    train_loss=0\n    for iter in 1:100\n        for g in train_graphs\n            _, loss, _, train_state = Lux.Training.single_train_step!(AutoZygote(), custom_loss,(g, g.x, g.y), train_state)\n            train_loss += loss\n        end\n\n        train_loss = train_loss/length(train_graphs)\n\n        if iter % 10 == 0\n            st_ = Lux.testmode(train_state.states)\n            test_loss =0\n            for g in test_graphs\n                ŷ, st_ = model(g, g.x, train_state.parameters, st_)\n                st_ = (layers = st_,)\n                test_loss += MSELoss()(g.y,ŷ)\n            end\n            test_loss = test_loss/length(test_graphs)\n\n            @info (; iter, train_loss, test_loss)\n        end\n    end\n\n    return model, ps, st\nend\n\ntrain_model!(model, ps, st, train_graphs, test_graphs)","category":"page"}]
}
