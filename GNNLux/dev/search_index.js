var documenterSearchIndex = {"docs":
[{"location":"api/basic/","page":"Basic","title":"Basic","text":"CurrentModule = GNNLux","category":"page"},{"location":"api/basic/#GNNLayer","page":"Basic","title":"GNNLayer","text":"","category":"section"},{"location":"api/basic/","page":"Basic","title":"Basic","text":"GNNLayer\nGNNChain","category":"page"},{"location":"api/basic/#GNNLux.GNNLayer","page":"Basic","title":"GNNLux.GNNLayer","text":"abstract type GNNLayer <: AbstractLuxLayer end\n\nAn abstract type from which graph neural network layers are derived. It is derived from Lux's AbstractLuxLayer type.\n\nSee also GNNLux.GNNChain.\n\n\n\n\n\n","category":"type"},{"location":"api/basic/#GNNLux.GNNChain","page":"Basic","title":"GNNLux.GNNChain","text":"GNNChain(layers...)\nGNNChain(name = layer, ...)\n\nCollects multiple layers / functions to be called in sequence on given input graph and input node features. \n\nIt allows to compose layers in a sequential fashion as Lux.Chain does, propagating the output of each layer to the next one. In addition, GNNChain handles the input graph as well, providing it  as a first argument only to layers subtyping the GNNLayer abstract type. \n\nGNNChain supports indexing and slicing, m[2] or m[1:end-1], and if names are given, m[:name] == m[1] etc.\n\nExamples\n\njulia> using Lux, GNNLux, Random\n\njulia> rng = Random.default_rng();\n\njulia> m = GNNChain(GCNConv(2=>5), \n                    x -> relu.(x), \n                    Dense(5=>4))\n\njulia> x = randn(rng, Float32, 2, 3);\n\njulia> g = rand_graph(rng, 3, 6)\nGNNGraph:\n  num_nodes: 3\n  num_edges: 6\n\njulia> ps, st = LuxCore.setup(rng, m);\n\njulia> m(g, x, ps, st)     # First entry is the output, second entry is the state of the model\n(Float32[-0.15594329 -0.15594329 -0.15594329; 0.93431795 0.93431795 0.93431795; 0.27568763 0.27568763 0.27568763; 0.12568939 0.12568939 0.12568939], (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple()))\n\n\n\n\n\n","category":"type"},{"location":"api/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"CurrentModule = GNNLux","category":"page"},{"location":"api/conv/#Convolutional-Layers","page":"Convolutional Layers","title":"Convolutional Layers","text":"","category":"section"},{"location":"api/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Many different types of graphs convolutional layers have been proposed in the literature. Choosing the right layer for your application could involve a lot of exploration.  Multiple graph convolutional layers are typically stacked together to create a graph neural network model (see GNNChain).","category":"page"},{"location":"api/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"The table below lists all graph convolutional layers implemented in the GNNLux.jl. It also highlights the presence of some additional capabilities with respect to basic message passing:","category":"page"},{"location":"api/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Sparse Ops: implements message passing as multiplication by sparse adjacency matrix instead of the gather/scatter mechanism. This can lead to better CPU performances but it is not supported on GPU yet. \nEdge Weight: supports scalar weights (or equivalently scalar features) on edges. \nEdge Features: supports feature vectors on edges.\nHeterograph: supports heterogeneous graphs (see GNNHeteroGraph).\nTemporalSnapshotsGNNGraphs: supports temporal graphs (see TemporalSnapshotsGNNGraph) by applying the convolution layers to each snapshot independently.","category":"page"},{"location":"api/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"| Layer                       |Sparse Ops|Edge Weight|Edge Features| Heterograph  | TemporalSnapshotsGNNGraphs | | :––––                   |  :–-:   |:–-:      |:–-:        |  :–-:       | :–-:                      |         ✓               | | GCNConv           |     ✓    |     ✓     |             |       ✓      |                            |","category":"page"},{"location":"api/conv/#Docs","page":"Convolutional Layers","title":"Docs","text":"","category":"section"},{"location":"api/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Modules = [GNNLux]\nPages   = [\"layers/conv.jl\"]\nPrivate = false","category":"page"},{"location":"api/conv/#GNNLux.GCNConv","page":"Convolutional Layers","title":"GNNLux.GCNConv","text":"GCNConv(in => out, σ=identity; [init_weight, init_bias, use_bias, add_self_loops, use_edge_weight])\n\nGraph convolutional layer from paper Semi-supervised Classification with Graph Convolutional Networks.\n\nPerforms the operation\n\nmathbfx_i = sum_jin N(i) a_ij W mathbfx_j\n\nwhere a_ij = 1  sqrtN(i)N(j) is a normalization factor computed from the node degrees. \n\nIf the input graph has weighted edges and use_edge_weight=true, than a_ij will be computed as\n\na_ij = frace_jto isqrtsum_j in N(i)  e_jto i sqrtsum_i in N(j)  e_ito j\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nσ: Activation function. Default identity.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default false.\nuse_edge_weight: If true, consider the edge weights in the input graph (if available).                    If add_self_loops=true the new weights will be set to 1.                     This option is ignored if the edge_weight is explicitly provided in the forward pass.                    Default false.\n\nForward\n\n(::GCNConv)(g, x, [edge_weight], ps, st; norm_fn = d -> 1 ./ sqrt.(d), conv_weight=nothing)\n\nTakes as input a graph g, a node feature matrix x of size [in, num_nodes], optionally an edge weight vector and the parameter and state of the layer. Returns a node feature matrix of size  [out, num_nodes].\n\nThe norm_fn parameter allows for custom normalization of the graph convolution operation by passing a function as argument.  By default, it computes frac1sqrtd i.e the inverse square root of the degree (d) of each node in the graph.  If conv_weight is an AbstractMatrix of size [out, in], then the convolution is performed using that weight matrix.\n\nExamples\n\nusing GNNLux, Lux, Random\n# initialize random number generator\nrng = Random.default_rng()\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = GCNConv(3 => 5) \n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny = l(g, x, ps, st)       # size of the output first entry:  5 × num_nodes\n\n# convolution with edge weights and custom normalization function\nw = [1.1, 0.1, 2.3, 0.5]\ncustom_norm_fn(d) = 1 ./ sqrt.(d + 1)  # Custom normalization function\ny = l(g, x, w, ps, st; norm_fn = custom_norm_fn)\n\n# Edge weights can also be embedded in the graph.\ng = GNNGraph(s, t, w)\nl = GCNConv(3 => 5, use_edge_weight=true)\nps, st = Lux.setup(rng, l)\ny = l(g, x, ps, st) # same as l(g, x, w) \n\n\n\n\n\n","category":"type"},{"location":"#GNNLux.jl","page":"Home","title":"GNNLux.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GNNLux.jl is a work-in-progress package that implements stateless graph convolutional layers, fully compatible with the Lux.jl machine learning framework. It is built on top of the GNNGraphs.jl, GNNlib.jl, and Lux.jl packages.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The full documentation will be available soon.","category":"page"}]
}
