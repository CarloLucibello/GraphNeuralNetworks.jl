var documenterSearchIndex = {"docs":
[{"location":"api/basic/","page":"Basic","title":"Basic","text":"CurrentModule = GNNLux","category":"page"},{"location":"api/basic/#Basic-Layers","page":"Basic","title":"Basic Layers","text":"","category":"section"},{"location":"api/basic/","page":"Basic","title":"Basic","text":"GNNLayer\nGNNChain","category":"page"},{"location":"api/basic/#GNNLux.GNNLayer","page":"Basic","title":"GNNLux.GNNLayer","text":"abstract type GNNLayer <: AbstractLuxLayer end\n\nAn abstract type from which graph neural network layers are derived. It is derived from Lux's AbstractLuxLayer type.\n\nSee also GNNLux.GNNChain.\n\n\n\n\n\n","category":"type"},{"location":"api/basic/#GNNLux.GNNChain","page":"Basic","title":"GNNLux.GNNChain","text":"GNNChain(layers...)\nGNNChain(name = layer, ...)\n\nCollects multiple layers / functions to be called in sequence on given input graph and input node features. \n\nIt allows to compose layers in a sequential fashion as Lux.Chain does, propagating the output of each layer to the next one. In addition, GNNChain handles the input graph as well, providing it  as a first argument only to layers subtyping the GNNLayer abstract type. \n\nGNNChain supports indexing and slicing, m[2] or m[1:end-1], and if names are given, m[:name] == m[1] etc.\n\nExamples\n\njulia> using Lux, GNNLux, Random\n\njulia> rng = Random.default_rng();\n\njulia> m = GNNChain(GCNConv(2=>5), \n                    x -> relu.(x), \n                    Dense(5=>4))\n\njulia> x = randn(rng, Float32, 2, 3);\n\njulia> g = rand_graph(rng, 3, 6)\nGNNGraph:\n  num_nodes: 3\n  num_edges: 6\n\njulia> ps, st = LuxCore.setup(rng, m);\n\njulia> m(g, x, ps, st)     # First entry is the output, second entry is the state of the model\n(Float32[-0.15594329 -0.15594329 -0.15594329; 0.93431795 0.93431795 0.93431795; 0.27568763 0.27568763 0.27568763; 0.12568939 0.12568939 0.12568939], (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple()))\n\n\n\n\n\n","category":"type"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"CurrentModule = GNNLux","category":"page"},{"location":"api/conv/#Convolutional-Layers","page":"Convolutional layers","title":"Convolutional Layers","text":"","category":"section"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"Many different types of graphs convolutional layers have been proposed in the literature. Choosing the right layer for your application could involve a lot of exploration.  Multiple graph convolutional layers are typically stacked together to create a graph neural network model (see GNNChain).","category":"page"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"The table below lists all graph convolutional layers implemented in the GNNLux.jl. It also highlights the presence of some additional capabilities with respect to basic message passing:","category":"page"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"Sparse Ops: implements message passing as multiplication by sparse adjacency matrix instead of the gather/scatter mechanism. This can lead to better CPU performances but it is not supported on GPU yet. \nEdge Weight: supports scalar weights (or equivalently scalar features) on edges. \nEdge Features: supports feature vectors on edges.\nHeterograph: supports heterogeneous graphs (see GNNHeteroGraph).\nTemporalSnapshotsGNNGraphs: supports temporal graphs (see TemporalSnapshotsGNNGraph) by applying the convolution layers to each snapshot independently.","category":"page"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"Layer Sparse Ops Edge Weight Edge Features Heterograph TemporalSnapshotsGNNGraphs\nAGNNConv   ✓  \nCGConv   ✓ ✓ ✓\nChebConv     ✓\nEGNNConv   ✓  \nEdgeConv    ✓ \nGATConv   ✓ ✓ ✓\nGATv2Conv   ✓ ✓ ✓\nGatedGraphConv ✓    ✓\nGCNConv ✓ ✓  ✓ \nGINConv ✓   ✓ ✓\nGMMConv   ✓  \nGraphConv ✓   ✓ ✓\nMEGNetConv   ✓  \nNNConv   ✓  \nResGatedGraphConv    ✓ ✓\nSAGEConv ✓   ✓ ✓\nSGConv ✓    ✓","category":"page"},{"location":"api/conv/#Docs","page":"Convolutional layers","title":"Docs","text":"","category":"section"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"Modules = [GNNLux]\nPages   = [\"layers/conv.jl\"]\nPrivate = false","category":"page"},{"location":"api/conv/#GNNLux.AGNNConv","page":"Convolutional layers","title":"GNNLux.AGNNConv","text":"AGNNConv(; init_beta=1.0f0, trainable=true, add_self_loops=true)\n\nAttention-based Graph Neural Network layer from paper Attention-based Graph Neural Network for Semi-Supervised Learning.\n\nThe forward pass is given by\n\nmathbfx_i = sum_j in N(i) alpha_ij mathbfx_j\n\nwhere the attention coefficients alpha_ij are given by\n\nalpha_ij =frace^beta cos(mathbfx_i mathbfx_j)\n                  sum_je^beta cos(mathbfx_i mathbfx_j)\n\nwith the cosine distance defined by\n\ncos(mathbfx_i mathbfx_j) = \n  fracmathbfx_i cdot mathbfx_jlVertmathbfx_irVert lVertmathbfx_jrVert\n\nand beta a trainable parameter if trainable=true.\n\nArguments\n\ninit_beta: The initial value of beta. Default 1.0f0.\ntrainable: If true, beta is trainable. Default true.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default true.\n\nExamples:\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\n\n# create layer\nl = AGNNConv(init_beta=2.0f0)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)   \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.CGConv","page":"Convolutional layers","title":"GNNLux.CGConv","text":"CGConv((in, ein) => out, act = identity; residual = false,\n            use_bias = true, init_weight = glorot_uniform, init_bias = zeros32)\nCGConv(in => out, ...)\n\nThe crystal graph convolutional layer from the paper Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties. Performs the operation\n\nmathbfx_i = mathbfx_i + sum_jin N(i)sigma(W_f mathbfz_ij + mathbfb_f) act(W_s mathbfz_ij + mathbfb_s)\n\nwhere mathbfz_ij  is the node and edge features concatenation  mathbfx_i mathbfx_j mathbfe_jto i  and sigma is the sigmoid function. The residual mathbfx_i is added only if residual=true and the output size is the same  as the input size.\n\nArguments\n\nin: The dimension of input node features.\nein: The dimension of input edge features. \n\nIf ein is not given, assumes that no edge features are passed as input in the forward pass.\n\nout: The dimension of output node features.\nact: Activation function.\nresidual: Add a residual connection.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create random graph\ng = rand_graph(rng, 5, 6)\nx = rand(rng, Float32, 2, g.num_nodes)\ne = rand(rng, Float32, 3, g.num_edges)\n\nl = CGConv((2, 3) => 4, tanh)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, e, ps, st)    # size: (4, num_nodes)\n\n# No edge features\nl = CGConv(2 => 4, tanh)\nps, st = LuxCore.setup(rng, l)\ny, st = l(g, x, ps, st)    # size: (4, num_nodes)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.ChebConv","page":"Convolutional layers","title":"GNNLux.ChebConv","text":"ChebConv(in => out, k; init_weight = glorot_uniform, init_bias = zeros32, use_bias = true)\n\nChebyshev spectral graph convolutional layer from paper Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering.\n\nImplements\n\nX = sum^K-1_k=0  W^(k) Z^(k)\n\nwhere Z^(k) is the k-th term of Chebyshev polynomials, and can be calculated by the following recursive form:\n\nbeginaligned\nZ^(0) = X \nZ^(1) = hatL X \nZ^(k) = 2 hatL Z^(k-1) - Z^(k-2)\nendaligned\n\nwith hatL the scaled_laplacian.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nk: The order of Chebyshev polynomial.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = ChebConv(3 => 5, 5)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size of the output y:  5 × num_nodes\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.DConv","page":"Convolutional layers","title":"GNNLux.DConv","text":"DConv(in => out, k; init_weight = glorot_uniform, init_bias = zeros32, use_bias = true)\n\nDiffusion convolution layer from the paper Diffusion Convolutional Recurrent Neural Networks: Data-Driven Traffic Forecasting.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nk: Number of diffusion steps.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create random graph\ng = GNNGraph(rand(rng, 10, 10), ndata = rand(rng, Float32, 2, 10))\n\ndconv = DConv(2 => 4, 4)\n\n# setup layer\nps, st = LuxCore.setup(rng, dconv)\n\n# forward pass\ny, st = dconv(g, g.ndata.x, ps, st)   # size: (4, num_nodes)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.EGNNConv","page":"Convolutional layers","title":"GNNLux.EGNNConv","text":"EGNNConv((in, ein) => out; hidden_size=2in, residual=false)\nEGNNConv(in => out; hidden_size=2in, residual=false)\n\nEquivariant Graph Convolutional Layer from E(n) Equivariant Graph Neural Networks.\n\nThe layer performs the following operation:\n\nbeginaligned\nmathbfm_jto i =phi_e(mathbfh_i mathbfh_j lVertmathbfx_i-mathbfx_jrVert^2 mathbfe_jto i)\nmathbfx_i = mathbfx_i + C_isum_jinmathcalN(i)(mathbfx_i-mathbfx_j)phi_x(mathbfm_jto i)\nmathbfm_i = C_isum_jinmathcalN(i) mathbfm_jto i\nmathbfh_i = mathbfh_i + phi_h(mathbfh_i mathbfm_i)\nendaligned\n\nwhere mathbfh_i, mathbfx_i, mathbfe_jto i are invariant node features, equivariant node features, and edge features respectively. phi_e, phi_h, and phi_x are two-layer MLPs. C is a constant for normalization, computed as 1mathcalN(i).\n\nConstructor Arguments\n\nin: Number of input features for h.\nout: Number of output features for h.\nein: Number of input edge features.\nhidden_size: Hidden representation size.\nresidual: If true, add a residual connection. Only possible if in == out. Default false.\n\nForward Pass\n\nl(g, x, h, e=nothing, ps, st)\n\nForward Pass Arguments:\n\ng : The graph.\nx : Matrix of equivariant node coordinates.\nh : Matrix of invariant node features.\ne : Matrix of invariant edge features. Default nothing.\nps : Parameters.\nst : State.\n\nReturns updated h and x.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create random graph\ng = rand_graph(rng, 10, 10)\nh = randn(rng, Float32, 5, g.num_nodes)\nx = randn(rng, Float32, 3, g.num_nodes)\n\negnn = EGNNConv(5 => 6, 10)\n\n# setup layer\nps, st = LuxCore.setup(rng, egnn)\n\n# forward pass\n(hnew, xnew), st = egnn(g, h, x, ps, st)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.EdgeConv","page":"Convolutional layers","title":"GNNLux.EdgeConv","text":"EdgeConv(nn; aggr=max)\n\nEdge convolutional layer from paper Dynamic Graph CNN for Learning on Point Clouds.\n\nPerforms the operation\n\nmathbfx_i = square_j in N(i) nn(mathbfx_i mathbfx_j - mathbfx_i)\n\nwhere nn generally denotes a learnable function, e.g. a linear layer or a multi-layer perceptron.\n\nArguments\n\nnn: A (possibly learnable) function. \naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\n\nExamples:\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = rand(rng, Float32, in_channel, g.num_nodes)\n\n# create layer\nl = EdgeConv(Dense(2 * in_channel, out_channel), aggr = +)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.GATConv","page":"Convolutional layers","title":"GNNLux.GATConv","text":"GATConv(in => out, σ = identity; heads = 1, concat = true, negative_slope = 0.2, init_weight = glorot_uniform, init_bias = zeros32, use_bias = true, add_self_loops = true, dropout=0.0)\nGATConv((in, ein) => out, ...)\n\nGraph attentional layer from the paper Graph Attention Networks.\n\nImplements the operation\n\nmathbfx_i = sum_j in N(i) cup i alpha_ij W mathbfx_j\n\nwhere the attention coefficients alpha_ij are given by\n\nalpha_ij = frac1z_i exp(LeakyReLU(mathbfa^T W mathbfx_i W mathbfx_j))\n\nwith z_i a normalization factor. \n\nIn case ein > 0 is given, edge features of dimension ein will be expected in the forward pass  and the attention coefficients will be calculated as  \n\nalpha_ij = frac1z_i exp(LeakyReLU(mathbfa^T W_e mathbfe_jto i W mathbfx_i W mathbfx_j))\n\nArguments\n\nin: The dimension of input node features.\nein: The dimension of input edge features. Default 0 (i.e. no edge features passed in the forward).\nout: The dimension of output node features.\nσ: Activation function. Default identity.\nheads: Number attention heads. Default 1.\nconcat: Concatenate layer output or not. If not, layer output is averaged over the heads. Default true.\nnegative_slope: The parameter of LeakyReLU.Default 0.2.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default true.\ndropout: Dropout probability on the normalized attention coefficient. Default 0.0.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = GATConv(in_channel => out_channel; add_self_loops = false, use_bias = false, heads=2, concat=true)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.GATv2Conv","page":"Convolutional layers","title":"GNNLux.GATv2Conv","text":"GATv2Conv(in => out, σ = identity; heads = 1, concat = true, negative_slope = 0.2, init_weight = glorot_uniform, init_bias = zeros32, use_bias = true, add_self_loops = true, dropout=0.0)\nGATv2Conv((in, ein) => out, ...)\n\nGATv2 attentional layer from the paper How Attentive are Graph Attention Networks?.\n\nImplements the operation\n\nmathbfx_i = sum_j in N(i) cup i alpha_ij W_1 mathbfx_j\n\nwhere the attention coefficients alpha_ij are given by\n\nalpha_ij = frac1z_i exp(mathbfa^T LeakyReLU(W_2 mathbfx_i + W_1 mathbfx_j))\n\nwith z_i a normalization factor.\n\nIn case ein > 0 is given, edge features of dimension ein will be expected in the forward pass  and the attention coefficients will be calculated as  \n\nalpha_ij = frac1z_i exp(mathbfa^T LeakyReLU(W_3 mathbfe_jto i + W_2 mathbfx_i + W_1 mathbfx_j))\n\nArguments\n\nin: The dimension of input node features.\nein: The dimension of input edge features. Default 0 (i.e. no edge features passed in the forward).\nout: The dimension of output node features.\nσ: Activation function. Default identity.\nheads: Number attention heads. Default 1.\nconcat: Concatenate layer output or not. If not, layer output is averaged over the heads. Default true.\nnegative_slope: The parameter of LeakyReLU.Default 0.2.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default true.\ndropout: Dropout probability on the normalized attention coefficient. Default 0.0.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\nein = 3\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = GATv2Conv((in_channel, ein) => out_channel, add_self_loops = false)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# edge features\ne = randn(rng, Float32, ein, length(s))\n\n# forward pass\ny, st = l(g, x, e, ps, st)    \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.GCNConv","page":"Convolutional layers","title":"GNNLux.GCNConv","text":"GCNConv(in => out, σ=identity; [init_weight, init_bias, use_bias, add_self_loops, use_edge_weight])\n\nGraph convolutional layer from paper Semi-supervised Classification with Graph Convolutional Networks.\n\nPerforms the operation\n\nmathbfx_i = sum_jin N(i) a_ij W mathbfx_j\n\nwhere a_ij = 1  sqrtN(i)N(j) is a normalization factor computed from the node degrees. \n\nIf the input graph has weighted edges and use_edge_weight=true, than a_ij will be computed as\n\na_ij = frace_jto isqrtsum_j in N(i)  e_jto i sqrtsum_i in N(j)  e_ito j\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nσ: Activation function. Default identity.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default false.\nuse_edge_weight: If true, consider the edge weights in the input graph (if available).                    If add_self_loops=true the new weights will be set to 1.                     This option is ignored if the edge_weight is explicitly provided in the forward pass.                    Default false.\n\nForward\n\n(::GCNConv)(g, x, [edge_weight], ps, st; norm_fn = d -> 1 ./ sqrt.(d), conv_weight=nothing)\n\nTakes as input a graph g, a node feature matrix x of size [in, num_nodes], optionally an edge weight vector and the parameter and state of the layer. Returns a node feature matrix of size  [out, num_nodes].\n\nThe norm_fn parameter allows for custom normalization of the graph convolution operation by passing a function as argument.  By default, it computes frac1sqrtd i.e the inverse square root of the degree (d) of each node in the graph.  If conv_weight is an AbstractMatrix of size [out, in], then the convolution is performed using that weight matrix.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = GCNConv(3 => 5) \n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny = l(g, x, ps, st)       # size of the output first entry:  5 × num_nodes\n\n# convolution with edge weights and custom normalization function\nw = [1.1, 0.1, 2.3, 0.5]\ncustom_norm_fn(d) = 1 ./ sqrt.(d + 1)  # Custom normalization function\ny = l(g, x, w, ps, st; norm_fn = custom_norm_fn)\n\n# Edge weights can also be embedded in the graph.\ng = GNNGraph(s, t, w)\nl = GCNConv(3 => 5, use_edge_weight=true)\nps, st = Lux.setup(rng, l)\ny = l(g, x, ps, st) # same as l(g, x, w) \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.GINConv","page":"Convolutional layers","title":"GNNLux.GINConv","text":"GINConv(f, ϵ; aggr=+)\n\nGraph Isomorphism convolutional layer from paper How Powerful are Graph Neural Networks?.\n\nImplements the graph convolution\n\nmathbfx_i = f_Thetaleft((1 + epsilon) mathbfx_i + sum_j in N(i) mathbfx_j right)\n\nwhere f_Theta typically denotes a learnable function, e.g. a linear layer or a multi-layer perceptron.\n\nArguments\n\nf: A (possibly learnable) function acting on node features. \nϵ: Weighting factor.\n\nExamples:\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(rng, Float32, in_channel, g.num_nodes)\n\n# create dense layer\nnn = Dense(in_channel, out_channel)\n\n# create layer\nl = GINConv(nn, 0.01f0, aggr = mean)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size:  out_channel × num_nodes\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.GMMConv","page":"Convolutional layers","title":"GNNLux.GMMConv","text":"GMMConv((in, ein) => out, σ=identity; K = 1, residual = false init_weight = glorot_uniform, init_bias = zeros32, use_bias = true)\n\nGraph mixture model convolution layer from the paper Geometric deep learning on graphs and manifolds using mixture model CNNs Performs the operation\n\nmathbfx_i = mathbfx_i + frac1N(i) sum_jin N(i)frac1Ksum_k=1^K mathbfw_k(mathbfe_jto i) odot Theta_k mathbfx_j\n\nwhere w^a_k(e^a) for feature a and kernel k is given by\n\nw^a_k(e^a) = exp(-frac12(e^a - mu^a_k)^T (Sigma^-1)^a_k(e^a - mu^a_k))\n\nTheta_k mu^a_k (Sigma^-1)^a_k are learnable parameters.\n\nThe input to the layer is a node feature array x of size (num_features, num_nodes) and edge pseudo-coordinate array e of size (num_features, num_edges) The residual mathbfx_i is added only if residual=true and the output size is the same  as the input size.\n\nArguments\n\nin: Number of input node features.\nein: Number of input edge features.\nout: Number of output features.\nσ: Activation function. Default identity.\nK: Number of kernels. Default 1.\nresidual: Residual conncetion. Default false.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s,t)\nnin, ein, out, K = 4, 10, 7, 8 \nx = randn(rng, Float32, nin, g.num_nodes)\ne = randn(rng, Float32, ein, g.num_edges)\n\n# create layer\nl = GMMConv((nin, ein) => out, K=K)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, e, ps, st)       # size:  out × num_nodes\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.GatedGraphConv","page":"Convolutional layers","title":"GNNLux.GatedGraphConv","text":"GatedGraphConv(out, num_layers; \n        aggr = +, init_weight = glorot_uniform)\n\nGated graph convolution layer from Gated Graph Sequence Neural Networks.\n\nImplements the recursion\n\nbeginaligned\nmathbfh^(0)_i = mathbfx_i mathbf0 \nmathbfh^(l)_i = GRU(mathbfh^(l-1)_i square_j in N(i) W mathbfh^(l-1)_j)\nendaligned\n\nwhere mathbfh^(l)_i denotes the l-th hidden variables passing through GRU. The dimension of input mathbfx_i needs to be less or equal to out.\n\nArguments\n\nout: The dimension of output features.\nnum_layers: The number of recursion steps.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\ninit_weight: Weights' initializer. Default glorot_uniform.\n\nExamples:\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nout_channel = 5\nnum_layers = 3\ng = GNNGraph(s, t)\n\n# create layer\nl = GatedGraphConv(out_channel, num_layers)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size:  out_channel × num_nodes  \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.GraphConv","page":"Convolutional layers","title":"GNNLux.GraphConv","text":"GraphConv(in => out, σ = identity; aggr = +, init_weight = glorot_uniform,init_bias = zeros32, use_bias = true)\n\nGraph convolution layer from Reference: Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks.\n\nPerforms:\n\nmathbfx_i = W_1 mathbfx_i + square_j in mathcalN(i) W_2 mathbfx_j\n\nwhere the aggregation type is selected by aggr.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nσ: Activation function.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = GraphConv(in_channel => out_channel, relu, use_bias = false, aggr = mean)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size of the output y:  5 × num_nodes\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.MEGNetConv","page":"Convolutional layers","title":"GNNLux.MEGNetConv","text":"MEGNetConv(ϕe, ϕv; aggr=mean)\nMEGNetConv(in => out; aggr=mean)\n\nConvolution from Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals paper. In the forward pass, takes as inputs node features x and edge features e and returns updated features x' and e' according to \n\nbeginaligned\nmathbfe_ito j  = phi_e(mathbfx_i  mathbfx_j  mathbfe_ito j)\nmathbfx_i  = phi_v(mathbfx_i square_jin mathcalN(i)mathbfe_jto i)\nendaligned\n\naggr defines the aggregation to be performed.\n\nIf the neural networks ϕe and  ϕv are not provided, they will be constructed from the in and out arguments instead as multi-layer perceptron with one hidden layer and relu  activations.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create a random graph\ng = rand_graph(rng, 10, 30)\nx = randn(rng, Float32, 3, 10)\ne = randn(rng, Float32, 3, 30)\n\n# create a MEGNetConv layer\nm = MEGNetConv(3 => 3)\n\n# setup layer\nps, st = LuxCore.setup(rng, m)\n\n# forward pass\n(x′, e′), st = m(g, x, e, ps, st)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.NNConv","page":"Convolutional layers","title":"GNNLux.NNConv","text":"NNConv(in => out, f, σ=identity; aggr=+, init_bias = zeros32, use_bias = true, init_weight = glorot_uniform)\n\nThe continuous kernel-based convolutional operator from the  Neural Message Passing for Quantum Chemistry paper.  This convolution is also known as the edge-conditioned convolution from the  Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs paper.\n\nPerforms the operation\n\nmathbfx_i = W mathbfx_i + square_j in N(i) f_Theta(mathbfe_jto i)mathbfx_j\n\nwhere f_Theta  denotes a learnable function (e.g. a linear layer or a multi-layer perceptron). Given an input of batched edge features e of size (num_edge_features, num_edges),  the function f will return an batched matrices array whose size is (out, in, num_edges). For convenience, also functions returning a single (out*in, num_edges) matrix are allowed.\n\nArguments\n\nin: The dimension of input node features.\nout: The dimension of output node features.\nf: A (possibly learnable) function acting on edge features.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\nσ: Activation function.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples:\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\nn_in = 3\nn_in_edge = 10\nn_out = 5\n\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(rng, Float32, n_in, g.num_nodes)\ne = randn(rng, Float32, n_in_edge, g.num_edges)\n\n# create dense layer\nnn = Dense(n_in_edge => n_out * n_in)\n\n# create layer\nl = NNConv(n_in => n_out, nn, tanh, use_bias = true, aggr = +)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, e, ps, st)       # size:  n_out × num_nodes \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.ResGatedGraphConv","page":"Convolutional layers","title":"GNNLux.ResGatedGraphConv","text":"ResGatedGraphConv(in => out, act=identity; init_weight = glorot_uniform, init_bias = zeros32, use_bias = true)\n\nThe residual gated graph convolutional operator from the Residual Gated Graph ConvNets paper.\n\nThe layer's forward pass is given by\n\nmathbfx_i = actbig(Umathbfx_i + sum_j in N(i) eta_ij V mathbfx_jbig)\n\nwhere the edge gates eta_ij are given by\n\neta_ij = sigmoid(Amathbfx_i + Bmathbfx_j)\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nact: Activation function.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples:\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(rng, Float32, in_channel, g.num_nodes)\n\n# create layer\nl = ResGatedGraphConv(in_channel => out_channel, tanh, use_bias = true)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size:  out_channel × num_nodes  \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.SAGEConv","page":"Convolutional layers","title":"GNNLux.SAGEConv","text":"SAGEConv(in => out, σ=identity; aggr=mean, init_weight = glorot_uniform, init_bias = zeros32, use_bias=true)\n\nGraphSAGE convolution layer from paper Inductive Representation Learning on Large Graphs.\n\nPerforms:\n\nmathbfx_i = W cdot mathbfx_i square_j in mathcalN(i) mathbfx_j\n\nwhere the aggregation type is selected by aggr.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nσ: Activation function.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples:\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = rand(rng, Float32, in_channel, g.num_nodes)\n\n# create layer\nl = SAGEConv(in_channel => out_channel, tanh, use_bias = false, aggr = +)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size:  out_channel × num_nodes   \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GNNLux.SGConv","page":"Convolutional layers","title":"GNNLux.SGConv","text":"SGConv(int => out, k = 1; init_weight = glorot_uniform, init_bias = zeros32, use_bias = true, add_self_loops = true,use_edge_weight = false)\n\nSGC layer from Simplifying Graph Convolutional Networks Performs operation\n\nH^K = (tildeD^-12 tildeA tildeD^-12)^K X Theta\n\nwhere tildeA is A + I.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nk : Number of hops k. Default 1.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default false.\nuse_edge_weight: If true, consider the edge weights in the input graph (if available).                    If add_self_loops=true the new weights will be set to 1. Default false.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_bias: Bias initializer. Default zeros32.\nuse_bias: Add learnable bias. Default true.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = SGConv(3 => 5; add_self_loops = true) \n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size:  5 × num_nodes\n\n# convolution with edge weights\nw = [1.1, 0.1, 2.3, 0.5]\ny = l(g, x, w, ps, st)\n\n# Edge weights can also be embedded in the graph.\ng = GNNGraph(s, t, w)\nl = SGConv(3 => 5, add_self_loops = true, use_edge_weight=true) \nps, st = LuxCore.setup(rng, l)\ny, st = l(g, x, ps, st) # same as l(g, x, w) \n\n\n\n\n\n","category":"type"},{"location":"api/temporalconv/","page":"Temporal Convolutional layers","title":"Temporal Convolutional layers","text":"CurrentModule = GNNLux","category":"page"},{"location":"api/temporalconv/#Temporal-Graph-Convolutional-Layers","page":"Temporal Convolutional layers","title":"Temporal Graph-Convolutional Layers","text":"","category":"section"},{"location":"api/temporalconv/","page":"Temporal Convolutional layers","title":"Temporal Convolutional layers","text":"Convolutions for time-varying graphs (temporal graphs) such as the TemporalSnapshotsGNNGraph.","category":"page"},{"location":"api/temporalconv/#Docs","page":"Temporal Convolutional layers","title":"Docs","text":"","category":"section"},{"location":"api/temporalconv/","page":"Temporal Convolutional layers","title":"Temporal Convolutional layers","text":"Modules = [GNNLux]\nPages   = [\"layers/temporalconv.jl\"]\nPrivate = false","category":"page"},{"location":"api/temporalconv/#GNNLux.A3TGCN","page":"Temporal Convolutional layers","title":"GNNLux.A3TGCN","text":"A3TGCN(in => out; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32, add_self_loops = false, use_edge_weight = true)\n\nAttention Temporal Graph Convolutional Network (A3T-GCN) model from the paper A3T-GCN: Attention Temporal Graph Convolutional Network for Traffic Forecasting.\n\nPerforms a TGCN layer, followed by a soft attention layer.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nuse_bias: Add learnable bias. Default true.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the GRU layer. Default zeros32.\ninit_bias: Bias initializer. Default zeros32.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default false.\nuse_edge_weight: If true, consider the edge weights in the input graph (if available).                    If add_self_loops=true the new weights will be set to 1.                     This option is ignored if the edge_weight is explicitly provided in the forward pass.                    Default false.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ng = rand_graph(rng, 5, 10)\nx = rand(rng, Float32, 2, 5)\n\n# create A3TGCN layer\nl = A3TGCN(2 => 6)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)      # result size (6, 5)\n\n\n\n\n\n","category":"type"},{"location":"api/temporalconv/#GNNLux.EvolveGCNO","page":"Temporal Convolutional layers","title":"GNNLux.EvolveGCNO","text":"EvolveGCNO(ch; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32)\n\nEvolving Graph Convolutional Network (EvolveGCNO) layer from the paper EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs.\n\nPerfoms a Graph Convolutional layer with parameters derived from a Long Short-Term Memory (LSTM) layer across the snapshots of the temporal graph.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nuse_bias: Add learnable bias. Default true.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the GRU layer. Default zeros32.\ninit_bias: Bias initializer. Default zeros32.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ntg = TemporalSnapshotsGNNGraph([rand_graph(rng, 10, 20; ndata = rand(rng, 4, 10)), rand_graph(rng, 10, 14; ndata = rand(rng, 4, 10)), rand_graph(rng, 10, 22; ndata = rand(rng, 4, 10))])\n\n# create layer\nl = EvolveGCNO(4 => 5)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(tg, tg.ndata.x , ps, st)      # result size 3, size y[1] (5, 10)\n\n\n\n\n\n","category":"type"},{"location":"api/temporalconv/#GNNLux.DCGRU-Tuple{Pair{Int64, Int64}, Int64}","page":"Temporal Convolutional layers","title":"GNNLux.DCGRU","text":"DCGRU(in => out, k; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32)\n\nDiffusion Convolutional Recurrent Neural Network (DCGRU) layer from the paper Diffusion Convolutional Recurrent Neural Network: Data-driven Traffic Forecasting.\n\nPerforms a Diffusion Convolutional layer to model spatial dependencies, followed by a Gated Recurrent Unit (GRU) cell to model temporal dependencies.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nk: Diffusion step.\nuse_bias: Add learnable bias. Default true.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the GRU layer. Default zeros32.\ninit_bias: Bias initializer. Default zeros32.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ng = rand_graph(rng, 5, 10)\nx = rand(rng, Float32, 2, 5)\n\n# create layer\nl = DCGRU(2 => 5, 2)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)      # result size (5, 5)\n\n\n\n\n\n","category":"method"},{"location":"api/temporalconv/#GNNLux.GConvGRU-Tuple{Pair{Int64, Int64}, Int64}","page":"Temporal Convolutional layers","title":"GNNLux.GConvGRU","text":"GConvGRU(in => out, k; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32)\n\nGraph Convolutional Gated Recurrent Unit (GConvGRU) recurrent layer from the paper Structured Sequence Modeling with Graph Convolutional Recurrent Networks.\n\nPerforms a layer of ChebConv to model spatial dependencies, followed by a Gated Recurrent Unit (GRU) cell to model temporal dependencies.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nk: Chebyshev polynomial order.\nuse_bias: Add learnable bias. Default true.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the GRU layer. Default zeros32.\ninit_bias: Bias initializer. Default zeros32.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ng = rand_graph(rng, 5, 10)\nx = rand(rng, Float32, 2, 5)\n\n# create layer\nl = GConvGRU(2 => 5, 2)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)      # result size (5, 5)\n\n\n\n\n\n","category":"method"},{"location":"api/temporalconv/#GNNLux.GConvLSTM-Tuple{Pair{Int64, Int64}, Int64}","page":"Temporal Convolutional layers","title":"GNNLux.GConvLSTM","text":"GConvLSTM(in => out, k; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32)\n\nGraph Convolutional Long Short-Term Memory (GConvLSTM) recurrent layer from the paper Structured Sequence Modeling with Graph Convolutional Recurrent Networks. \n\nPerforms a layer of ChebConv to model spatial dependencies, followed by a Long Short-Term Memory (LSTM) cell to model temporal dependencies.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nk: Chebyshev polynomial order.\nuse_bias: Add learnable bias. Default true.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the GRU layer. Default zeros32.\ninit_bias: Bias initializer. Default zeros32.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ng = rand_graph(rng, 5, 10)\nx = rand(rng, Float32, 2, 5)\n\n# create GConvLSTM layer\nl = GConvLSTM(2 => 5, 2)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)      # result size (5, 5)\n\n\n\n\n\n","category":"method"},{"location":"api/temporalconv/#GNNLux.TGCN-Tuple{Pair{Int64, Int64}}","page":"Temporal Convolutional layers","title":"GNNLux.TGCN","text":"TGCN(in => out; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32, add_self_loops = false, use_edge_weight = true)\n\nTemporal Graph Convolutional Network (T-GCN) recurrent layer from the paper T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction.\n\nPerforms a layer of GCNConv to model spatial dependencies, followed by a Gated Recurrent Unit (GRU) cell to model temporal dependencies.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nuse_bias: Add learnable bias. Default true.\ninit_weight: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the GRU layer. Default zeros32.\ninit_bias: Bias initializer. Default zeros32.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default false.\nuse_edge_weight: If true, consider the edge weights in the input graph (if available).                    If add_self_loops=true the new weights will be set to 1.                     This option is ignored if the edge_weight is explicitly provided in the forward pass.                    Default false.\n\nExamples\n\nusing GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ng = rand_graph(rng, 5, 10)\nx = rand(rng, Float32, 2, 5)\n\n# create TGCN layer\ntgcn = TGCN(2 => 6)\n\n# setup layer\nps, st = LuxCore.setup(rng, tgcn)\n\n# forward pass\ny, st = tgcn(g, x, ps, st)      # result size (6, 5)\n\n\n\n\n\n","category":"method"},{"location":"#GNNLux.jl","page":"Home","title":"GNNLux.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GNNLux.jl is a work-in-progress package that implements stateless graph convolutional layers, fully compatible with the Lux.jl machine learning framework. It is built on top of the GNNGraphs.jl, GNNlib.jl, and Lux.jl packages.","category":"page"},{"location":"#Package-overview","page":"Home","title":"Package overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Let's give a brief overview of the package by solving a graph regression problem with synthetic data. ","category":"page"},{"location":"#Data-preparation","page":"Home","title":"Data preparation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We generate a dataset of multiple random graphs with associated data features, then split it into training and testing sets.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GNNLux, Lux, Statistics, MLUtils, Random\nusing Zygote, Optimisers\n\nrng = Random.default_rng()\n\nall_graphs = GNNGraph[]\n\nfor _ in 1:1000\n    g = rand_graph(rng, 10, 40,  \n            ndata=(; x = randn(rng, Float32, 16,10)),  # Input node features\n            gdata=(; y = randn(rng, Float32)))         # Regression target   \n    push!(all_graphs, g)\nend\n\ntrain_graphs, test_graphs = MLUtils.splitobs(all_graphs, at=0.8)","category":"page"},{"location":"#Model-building","page":"Home","title":"Model building","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We concisely define our model as a GNNLux.GNNChain containing two graph convolutional layers and initialize the model's parameters and state.","category":"page"},{"location":"","page":"Home","title":"Home","text":"model = GNNChain(GCNConv(16 => 64),\n                x -> relu.(x),    \n                Dropout(0.6), \n                GCNConv(64 => 64, relu),\n                x -> mean(x, dims=2),\n                Dense(64, 1)) \n\nps, st = LuxCore.setup(rng, model)","category":"page"},{"location":"#Training","page":"Home","title":"Training","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Finally, we use a standard Lux training pipeline to fit our dataset.","category":"page"},{"location":"","page":"Home","title":"Home","text":"function custom_loss(model, ps, st, tuple)\n    g,x,y = tuple\n    y_pred,st = model(g, x, ps, st)  \n    return MSELoss()(y_pred, y), (layers = st,), 0\nend\n\nfunction train_model!(model, ps, st, train_graphs, test_graphs)\n    train_state = Lux.Training.TrainState(model, ps, st, Adam(0.0001f0))\n    train_loss=0\n    for iter in 1:100\n        for g in train_graphs\n            _, loss, _, train_state = Lux.Training.single_train_step!(AutoZygote(), custom_loss,(g, g.x, g.y), train_state)\n            train_loss += loss\n        end\n\n        train_loss = train_loss/length(train_graphs)\n\n        if iter % 10 == 0\n            st_ = Lux.testmode(train_state.states)\n            test_loss =0\n            for g in test_graphs\n                ŷ, st_ = model(g, g.x, train_state.parameters, st_)\n                st_ = (layers = st_,)\n                test_loss += MSELoss()(g.y,ŷ)\n            end\n            test_loss = test_loss/length(test_graphs)\n\n            @info (; iter, train_loss, test_loss)\n        end\n    end\n\n    return model, ps, st\nend\n\ntrain_model!(model, ps, st, train_graphs, test_graphs)","category":"page"}]
}
