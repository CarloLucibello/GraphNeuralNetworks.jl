var documenterSearchIndex = {"docs":
[{"location":"api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"CurrentModule = GraphNeuralNetworks","category":"page"},{"location":"api/gnngraph/#GNNGraph","page":"GNNGraph","title":"GNNGraph","text":"","category":"section"},{"location":"api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Documentation page for the graph type GNNGraph provided by GraphNeuralNetworks.jl and related methods. ","category":"page"},{"location":"api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Besides the methods documented here, one can rely on the large set of functionalities given by Graphs.jl thanks to the fact that GNNGraph inherits from Graphs.AbstractGraph.","category":"page"},{"location":"api/gnngraph/#Index","page":"GNNGraph","title":"Index","text":"","category":"section"},{"location":"api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Order = [:type, :function]\nPages   = [\"gnngraph.md\"]","category":"page"},{"location":"api/gnngraph/#GNNGraph-type","page":"GNNGraph","title":"GNNGraph type","text":"","category":"section"},{"location":"api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Modules = [GraphNeuralNetworks.GNNGraphs]\nPages   = [\"gnngraph.jl\"]\nPrivate = false","category":"page"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.GNNGraph","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.GNNGraph","text":"GNNGraph(data; [graph_type, ndata, edata, gdata, num_nodes, graph_indicator, dir])\nGNNGraph(g::GNNGraph; [ndata, edata, gdata])\n\nA type representing a graph structure that also stores  feature arrays associated to nodes, edges, and the graph itself. \n\nA GNNGraph can be constructed out of different data objects  expressing the connections inside the graph. The internal representation type is determined by graph_type.\n\nWhen constructed from another GNNGraph, the internal graph representation is preserved and shared. The node/edge/graph features are retained as well, unless explicitely set by the keyword arguments ndata, edata, and gdata.\n\nA GNNGraph can also represent multiple graphs batched togheter  (see Flux.batch or SparseArrays.blockdiag). The field g.graph_indicator contains the graph membership of each node.\n\nGNNGraphs are always directed graphs, therefore each edge is defined by a source node and a target node (see edge_index). Self loops (edges connecting a node to itself) and multiple edges (more than one edge between the same pair of nodes) are supported.\n\nA GNNGraph is a Graphs.jl's AbstractGraph, therefore it supports most  functionality from that library.\n\nArguments\n\ndata: Some data representing the graph topology. Possible type are \nAn adjacency matrix\nAn adjacency list.\nA tuple containing the source and target vectors (COO representation)\nA Graphs.jl' graph.\ngraph_type: A keyword argument that specifies                the underlying representation used by the GNNGraph.                Currently supported values are \n:coo. Graph represented as a tuple (source, target), such that the k-th edge          connects the node source[k] to node target[k].         Optionally, also edge weights can be given: (source, target, weights).\n:sparse. A sparse adjacency matrix representation.\n:dense. A dense adjacency matrix representation.  \nDefaults to :coo, currently the most supported type.\ndir: The assumed edge direction when given adjacency matrix or adjacency list input data g.        Possible values are :out and :in. Default :out.\nnum_nodes: The number of nodes. If not specified, inferred from g. Default nothing.\ngraph_indicator: For batched graphs, a vector containing the graph assigment of each node. Default nothing.  \nndata: Node features. An array or named tuple of arrays whose last dimension has size num_nodes.\nedata: Edge features. An array or named tuple of arrays whose last dimension has size num_edges.\ngdata: Graph features. An array or named tuple of arrays whose last dimension has size num_graphs. \n\nExamples\n\nusing Flux, GraphNeuralNetworks\n\n# Construct from adjacency list representation\ndata = [[2,3], [1,4,5], [1], [2,5], [2,4]]\ng = GNNGraph(data)\n\n# Number of nodes, edges, and batched graphs\ng.num_nodes  # 5\ng.num_edges  # 10 \ng.num_graphs # 1 \n\n# Same graph in COO representation\ns = [1,1,2,2,2,3,4,4,5,5]\nt = [2,3,1,4,5,3,2,5,2,4]\ng = GNNGraph(s, t)\n\n# From a Graphs' graph\ng = GNNGraph(erdos_renyi(100, 20))\n\n# Add 2 node feature arrays\ng = GNNGraph(g, ndata = (x=rand(100, g.num_nodes), y=rand(g.num_nodes)))\n\n# Add node features and edge features with default names `x` and `e` \ng = GNNGraph(g, ndata = rand(100, g.num_nodes), edata = rand(16, g.num_edges))\n\ng.ndata.x\ng.ndata.e\n\n# Send to gpu\ng = g |> gpu\n\n# Collect edges' source and target nodes.\n# Both source and target are vectors of length num_edges\nsource, target = edge_index(g)\n\n\n\n\n\n","category":"type"},{"location":"api/gnngraph/#Query","page":"GNNGraph","title":"Query","text":"","category":"section"},{"location":"api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Modules = [GraphNeuralNetworks.GNNGraphs]\nPages   = [\"query.jl\"]\nPrivate = false","category":"page"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.adjacency_list-Tuple{GNNGraph, Any}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.adjacency_list","text":"adjacency_list(g; dir=:out)\nadjacency_list(g, nodes; dir=:out)\n\nReturn the adjacency list representation (a vector of vectors) of the graph g.\n\nCalling a the adjacency list, if dir=:out than a[i] will contain the neighbors of node i through outgoing edges. If dir=:in, it will contain neighbors from incoming edges instead.\n\nIf nodes is given, return the neighborhood of the nodes in nodes only.\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.edge_index-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V}}}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.edge_index","text":"edge_index(g::GNNGraph)\n\nReturn a tuple containing two vectors, respectively storing  the source and target nodes for each edges in g.\n\ns, t = edge_index(g)\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.graph_indicator-Tuple{Any}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.graph_indicator","text":"graph_indicator(g)\n\nReturn a vector containing the graph membership (an integer from 1 to g.num_graphs) of each node in the graph.\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.has_multi_edges-Tuple{GNNGraph}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.has_multi_edges","text":"has_multi_edges(g::GNNGraph)\n\nReturn true if g has any multiple edges.\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.is_bidirected-Tuple{GNNGraph}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.is_bidirected","text":"is_bidirected(g::GNNGraph)\n\nCheck if the directed graph g essentially corresponds to an undirected graph, i.e. if for each edge it also contains the  reverse edge. \n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.normalized_laplacian","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.normalized_laplacian","text":"normalized_laplacian(g, T=Float32; add_self_loops=false, dir=:out)\n\nNormalized Laplacian matrix of graph g.\n\nArguments\n\ng: A GNNGraph.\nT: result element type.\nadd_self_loops: add self-loops while calculating the matrix.\ndir: the edge directionality considered (:out, :in, :both).\n\n\n\n\n\n","category":"function"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.scaled_laplacian","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.scaled_laplacian","text":"scaled_laplacian(g, T=Float32; dir=:out)\n\nScaled Laplacian matrix of graph g, defined as hatL = frac2lambda_max L - I where L is the normalized Laplacian matrix.\n\nArguments\n\ng: A GNNGraph.\nT: result element type.\ndir: the edge directionality considered (:out, :in, :both).\n\n\n\n\n\n","category":"function"},{"location":"api/gnngraph/#Graphs.LinAlg.adjacency_matrix","page":"GNNGraph","title":"Graphs.LinAlg.adjacency_matrix","text":"adjacency_matrix(g::GNNGraph, T=eltype(g); dir=:out, weighted=true)\n\nReturn the adjacency matrix A for the graph g. \n\nIf dir=:out, A[i,j] > 0 denotes the presence of an edge from node i to node j. If dir=:in instead, A[i,j] > 0 denotes the presence of an edge from node j to node i.\n\nUser may specify the eltype T of the returned matrix. \n\nIf weighted=true, the A will contain the edge weigths if any, otherwise the elements of A will be either 0 or 1.\n\n\n\n\n\n","category":"function"},{"location":"api/gnngraph/#Graphs.degree-Union{Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V}}}, Tuple{TT}, Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V}}, TT}} where TT<:Union{Nothing, Type{<:Number}}","page":"GNNGraph","title":"Graphs.degree","text":"degree(g::GNNGraph, T=nothing; dir=:out, edge_weight=true)\n\nReturn a vector containing the degrees of the nodes in g.\n\nArguments\n\ng: A graph.\nT: Element type of the returned vector. If nothing, is      chosen based on the graph type and will be an integer      if edge_weight=false.\ndir: For dir=:out the degree of a node is counted based on the outgoing edges.        For dir=:in, the ingoing edges are used. If dir=:both we have the sum of the two.\nedge_weight: If true and the graph contains weighted edges, the degree will                be weighted. Set to false instead to just count the number of               outgoing/ingoing edges.               In alternative, you can also pass a vector of weights to be used               instead of the graph's own weights.\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#Graphs.has_self_loops-Tuple{GNNGraph}","page":"GNNGraph","title":"Graphs.has_self_loops","text":"has_self_loops(g::GNNGraph)\n\nReturn true if g has any self loops.\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Graphs.adjacency_matrix\nGraphs.degree\nGraphs.outneighbors\nGraphs.inneighbors","category":"page"},{"location":"api/gnngraph/#Graphs.LinAlg.adjacency_matrix-2","page":"GNNGraph","title":"Graphs.LinAlg.adjacency_matrix","text":"adjacency_matrix(g[, T=Int; dir=:out])\n\nReturn a sparse adjacency matrix for a graph, indexed by [u, v] vertices. Non-zero values indicate an edge from u to v. Users may override the default data type (Int) and specify an optional direction.\n\nOptional Arguments\n\ndir=:out: :in, :out, or :both are currently supported.\n\nImplementation Notes\n\nThis function is optimized for speed and directly manipulates CSC sparse matrix fields.\n\n\n\n\n\nadjacency_matrix(g::GNNGraph, T=eltype(g); dir=:out, weighted=true)\n\nReturn the adjacency matrix A for the graph g. \n\nIf dir=:out, A[i,j] > 0 denotes the presence of an edge from node i to node j. If dir=:in instead, A[i,j] > 0 denotes the presence of an edge from node j to node i.\n\nUser may specify the eltype T of the returned matrix. \n\nIf weighted=true, the A will contain the edge weigths if any, otherwise the elements of A will be either 0 or 1.\n\n\n\n\n\n","category":"function"},{"location":"api/gnngraph/#Graphs.degree","page":"GNNGraph","title":"Graphs.degree","text":"degree(g[, v])\n\nReturn a vector corresponding to the number of edges which start or end at each vertex in graph g. If v is specified, only return degrees for vertices in v. For directed graphs, this value equals the incoming plus outgoing edges. For undirected graphs, it equals the connected edges.\n\nExamples\n\njulia> using Graphs\n\njulia> g = DiGraph(3);\n\njulia> add_edge!(g, 2, 3);\n\njulia> add_edge!(g, 3, 1);\n\njulia> degree(g)\n3-element Array{Int64,1}:\n 1\n 1\n 2\n\n\n\n\n\n","category":"function"},{"location":"api/gnngraph/#Graphs.outneighbors","page":"GNNGraph","title":"Graphs.outneighbors","text":"outneighbors(g, v)\n\nReturn a list of all neighbors connected to vertex v by an outgoing edge.\n\nImplementation Notes\n\nReturns a reference to the current graph's internal structures, not a copy. Do not modify result. If the graph is modified, the behavior is undefined: the array behind this reference may be modified too, but this is not guaranteed.\n\nExamples\n\njulia> g = SimpleDiGraph([0 1 0 0 0; 0 0 1 0 0; 1 0 0 1 0; 0 0 0 0 1; 0 0 0 1 0]);\n\njulia> outneighbors(g, 4)\n1-element Array{Int64,1}:\n 5\n\n\n\n\n\n","category":"function"},{"location":"api/gnngraph/#Graphs.inneighbors","page":"GNNGraph","title":"Graphs.inneighbors","text":"inneighbors(g, v)\n\nReturn a list of all neighbors connected to vertex v by an incoming edge.\n\nImplementation Notes\n\nReturns a reference to the current graph's internal structures, not a copy. Do not modify result. If the graph is modified, the behavior is undefined: the array behind this reference may be modified too, but this is not guaranteed.\n\nExamples\n\njulia> g = SimpleDiGraph([0 1 0 0 0; 0 0 1 0 0; 1 0 0 1 0; 0 0 0 0 1; 0 0 0 1 0]);\n\njulia> inneighbors(g, 4)\n2-element Array{Int64,1}:\n 3\n 5\n\n\n\n\n\n","category":"function"},{"location":"api/gnngraph/#Transform","page":"GNNGraph","title":"Transform","text":"","category":"section"},{"location":"api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Modules = [GraphNeuralNetworks.GNNGraphs]\nPages   = [\"transform.jl\"]\nPrivate = false","category":"page"},{"location":"api/gnngraph/#Flux.batch-Tuple{AbstractVector{<:GNNGraph}}","page":"GNNGraph","title":"Flux.batch","text":"batch(gs::Vector{<:GNNGraph})\n\nBatch together multiple GNNGraphs into a single one  containing the total number of original nodes and edges.\n\nEquivalent to SparseArrays.blockdiag. See also Flux.unbatch.\n\nExamples\n\njulia> g1 = rand_graph(4, 6, ndata=ones(8, 4))\nGNNGraph:\n    num_nodes = 4\n    num_edges = 6\n    ndata:\n        x => (8, 4)\n\njulia> g2 = rand_graph(7, 4, ndata=zeros(8, 7))\nGNNGraph:\n    num_nodes = 7\n    num_edges = 4\n    ndata:\n        x => (8, 7)\n\njulia> g12 = Flux.batch([g1, g2])\nGNNGraph:\n    num_nodes = 11\n    num_edges = 10\n    num_graphs = 2\n    ndata:\n        x => (8, 11)\n\njulia> g12.ndata.x\n8×11 Matrix{Float64}:\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#Flux.unbatch-Tuple{GNNGraph}","page":"GNNGraph","title":"Flux.unbatch","text":"unbatch(g::GNNGraph)\n\nOpposite of the Flux.batch operation, returns  an array of the individual graphs batched together in g.\n\nSee also Flux.batch and getgraph.\n\nExamples\n\njulia> gbatched = Flux.batch([rand_graph(5, 6), rand_graph(10, 8), rand_graph(4,2)])\nGNNGraph:\n    num_nodes = 19\n    num_edges = 16\n    num_graphs = 3\n\njulia> Flux.unbatch(gbatched)\n3-element Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}:\n GNNGraph:\n    num_nodes = 5\n    num_edges = 6\n\n GNNGraph:\n    num_nodes = 10\n    num_edges = 8\n\n GNNGraph:\n    num_nodes = 4\n    num_edges = 2\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.add_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V}}, AbstractVector{<:Integer}, AbstractVector{<:Integer}}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.add_edges","text":"add_edges(g::GNNGraph, s::AbstractVector, t::AbstractVector; [edata])\n\nAdd to graph g the edges with source nodes s and target nodes t. Optionally, pass the features  edata for the new edges. \n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.add_nodes-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V}}, Integer}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.add_nodes","text":"add_nodes(g::GNNGraph, n; [ndata])\n\nAdd n new nodes to graph g. In the  new graph, these nodes will have indexes from g.num_nodes + 1 to g.num_nodes + n.\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.add_self_loops-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V}}}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.add_self_loops","text":"add_self_loops(g::GNNGraph)\n\nReturn a graph with the same features as g but also adding edges connecting the nodes to themselves.\n\nNodes with already existing self-loops will obtain a second self-loop.\n\nIf the graphs has edge weights, the new edges will have weight 1.\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.getgraph-Tuple{GNNGraph, Int64}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.getgraph","text":"getgraph(g::GNNGraph, i; nmap=false)\n\nReturn the subgraph of g induced by those nodes j for which g.graph_indicator[j] == i or, if i is a collection, g.graph_indicator[j] ∈ i.  In other words, it extract the component graphs from a batched graph. \n\nIf nmap=true, return also a vector v mapping the new nodes to the old ones.  The node i in the subgraph will correspond to the node v[i] in g.\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.negative_sample-Tuple{GNNGraph}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.negative_sample","text":"negative_sample(g::GNNGraph; \n                num_neg_edges = g.num_edges, \n                bidirected = is_bidirected(g))\n\nReturn a graph containing random negative edges (i.e. non-edges) from graph g as edges.\n\nIs bidirected=true, the output graph will be bidirected and there will be no leakage from the origin graph. \n\nSee also is_bidirected.\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.rand_edge_split-Tuple{GNNGraph, Any}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.rand_edge_split","text":"rand_edge_split(g::GNNGraph, frac; bidirected=is_bidirected(g)) -> g1, g2\n\nRandomly partition the edges in g to form two graphs, g1 and g2. Both will have the same number of nodes as g. g1 will contain a fraction frac of the original edges,  while g2 wil contain the rest.\n\nIf bidirected = true makes sure that an edge and its reverse go into the same split. This option is supported only for bidirected graphs with no self-loops and multi-edges.\n\nrand_edge_split is tipically used to create train/test splits in link prediction tasks.\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.remove_multi_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V}}}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.remove_multi_edges","text":"remove_multi_edges(g::GNNGraph; aggr=+)\n\nRemove multiple edges (also called parallel edges or repeated edges) from graph g. Possible edge features are aggregated according to aggr, that can take value  +,min, max or mean.\n\nSee also remove_self_loops, has_multi_edges, and to_bidirected.\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.remove_self_loops-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V}}}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.remove_self_loops","text":"remove_self_loops(g::GNNGraph)\n\nReturn a graph constructed from g where self-loops (edges from a node to itself) are removed. \n\nSee also add_self_loops and remove_multi_edges.\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.set_edge_weight-Tuple{GNNGraph, AbstractVector}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.set_edge_weight","text":"set_edge_weight(g::GNNGraph, w::AbstractVector)\n\nSet w as edge weights in the returned graph. \n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.to_bidirected-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V}}}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.to_bidirected","text":"to_bidirected(g)\n\nAdds a reverse edge for each edge in the graph, then calls  remove_multi_edges with mean aggregation to simplify the graph. \n\nSee also is_bidirected. \n\nExamples\n\njulia> s, t = [1, 2, 3, 3, 4], [2, 3, 4, 4, 4];\n\njulia> w = [1.0, 2.0, 3.0, 4.0, 5.0];\n\njulia> e = [10.0, 20.0, 30.0, 40.0, 50.0];\n\njulia> g = GNNGraph(s, t, w, edata = e)\nGNNGraph:\n    num_nodes = 4\n    num_edges = 5\n    edata:\n        e => (5,)\n\njulia> g2 = to_bidirected(g)\nGNNGraph:\n    num_nodes = 4\n    num_edges = 7\n    edata:\n        e => (7,)\n\njulia> edge_index(g2)\n([1, 2, 2, 3, 3, 4, 4], [2, 1, 3, 2, 4, 3, 4])\n\njulia> get_edge_weight(g2)\n7-element Vector{Float64}:\n 1.0\n 1.0\n 2.0\n 2.0\n 3.5\n 3.5\n 5.0\n\njulia> g2.edata.e\n7-element Vector{Float64}:\n 10.0\n 10.0\n 20.0\n 20.0\n 35.0\n 35.0\n 50.0\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#SparseArrays.blockdiag-Tuple{GNNGraph, Vararg{GNNGraph}}","page":"GNNGraph","title":"SparseArrays.blockdiag","text":"blockdiag(xs::GNNGraph...)\n\nEquivalent to Flux.batch.\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#Generate","page":"GNNGraph","title":"Generate","text":"","category":"section"},{"location":"api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Modules = [GraphNeuralNetworks.GNNGraphs]\nPages   = [\"generate.jl\"]\nPrivate = false","category":"page"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.knn_graph-Tuple{AbstractMatrix, Int64}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.knn_graph","text":"knn_graph(points::AbstractMatrix, \n          k::Int; \n          graph_indicator = nothing,\n          self_loops = false, \n          dir = :in, \n          kws...)\n\nCreate a k-nearest neighbor graph where each node is linked  to its k closest points.  \n\nArguments\n\npoints: A numfeatures × numnodes matrix storing the Euclidean positions of the nodes.\nk: The number of neighbors considered in the kNN algorithm.\ngraph_indicator: Either nothing or a vector containing the graph assigment of each node,                     in which case the returned graph will be a batch of graphs. \nself_loops: If true, consider the node itself among its k nearest neighbors, in which               case the graph will contain self-loops. \ndir: The direction of the edges. If dir=:in edges go from the k         neighbors to the central node. If dir=:out we have the opposite        direction.\nkws: Further keyword arguments will be passed to the `GNNGraph  constructor.\n\nExamples\n\njulia> n, k = 10, 3;\n\njulia> x = rand(3, n);\n\njulia> g = knn_graph(x, k)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 30\n\njulia> graph_indicator = [1,1,1,1,1,2,2,2,2,2];\n\njulia> g = knn_graph(x, k; graph_indicator)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 30\n    num_graphs = 2\n\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.rand_graph-Tuple{Integer, Integer}","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.rand_graph","text":"rand_graph(n, m; bidirected=true, seed=-1, kws...)\n\nGenerate a random (Erdós-Renyi) GNNGraph with n nodes and m edges.\n\nIf bidirected=true the reverse edge of each edge will be present. If bidirected=false instead, m unrelated edges are generated. In any case, the output graph will contain no self-loops or multi-edges.\n\nUse a seed > 0 for reproducibility.\n\nAdditional keyword arguments will be passed to the GNNGraph constructor.\n\nExamples\n\njulia> g = rand_graph(5, 4, bidirected=false)\nGNNGraph:\n    num_nodes = 5\n    num_edges = 4\n\njulia> edge_index(g)\n([1, 3, 3, 4], [5, 4, 5, 2])\n\n# In the bidirected case, edge data will be duplicated on the reverse edges if needed.\njulia> g = rand_graph(5, 4, edata=rand(16, 2))\nGNNGraph:\n    num_nodes = 5\n    num_edges = 4\n    edata:\n        e => (16, 4)\n\n# Each edge has a reverse\njulia> edge_index(g)\n([1, 3, 3, 4], [3, 4, 1, 3])\n\n\n\n\n\n\n","category":"method"},{"location":"api/gnngraph/#Operators","page":"GNNGraph","title":"Operators","text":"","category":"section"},{"location":"api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Modules = [GraphNeuralNetworks.GNNGraphs]\nPages   = [\"operators.jl\"]\nPrivate = false","category":"page"},{"location":"api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Graphs.intersect","category":"page"},{"location":"api/gnngraph/#Base.intersect","page":"GNNGraph","title":"Base.intersect","text":"intersect(g, h)\n\nReturn a graph with edges that are only in both graph g and graph h.\n\nImplementation Notes\n\nThis function may produce a graph with 0-degree vertices. Preserves the eltype of the input graph.\n\nExamples\n\njulia> g1 = SimpleDiGraph([0 1 0 0 0; 0 0 1 0 0; 1 0 0 1 0; 0 0 0 0 1; 0 0 0 1 0]);\n\njulia> g2 = SimpleDiGraph([0 1 0; 0 0 1; 1 0 0]);\n\njulia> foreach(println, edges(intersect(g1, g2)))\nEdge 1 => 2\nEdge 2 => 3\nEdge 3 => 1\n\n\n\n\n\n","category":"function"},{"location":"api/gnngraph/#Sampling","page":"GNNGraph","title":"Sampling","text":"","category":"section"},{"location":"api/gnngraph/","page":"GNNGraph","title":"GNNGraph","text":"Modules = [GraphNeuralNetworks.GNNGraphs]\nPages   = [\"sampling.jl\"]\nPrivate = false","category":"page"},{"location":"api/gnngraph/#GraphNeuralNetworks.GNNGraphs.sample_neighbors","page":"GNNGraph","title":"GraphNeuralNetworks.GNNGraphs.sample_neighbors","text":"sample_neighbors(g, nodes, K=-1; dir=:in, replace=false, dropnodes=false)\n\nSample neighboring edges of the given nodes and return the induced subgraph. For each node, a number of inbound (or outbound when dir = :out) edges will be randomly chosen.  Ifdropnodes=false`, the graph returned will then contain all the nodes in the original graph,  but only the sampled edges.\n\nThe returned graph will contain an edge feature EID corresponding to the id of the edge in the original graph. If dropnodes=true, it will also contain a node feature NID with the node ids in the original graph.\n\nArguments\n\ng. The graph.\nnodes. A list of node IDs to sample neighbors from.\nK. The maximum number of edges to be sampled for each node.      If -1, all the neighboring edges will be selected.\ndir. Determines whether to sample inbound (:in) or outbound (`:out) edges (Default :in).\nreplace. If true, sample with replacement.\ndropnodes. If true, the resulting subgraph will contain only the nodes involved in the sampled edges.\n\nExamples\n\njulia> g = rand_graph(20, 100)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 100\n\njulia> sample_neighbors(g, 2:3)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 9\n    edata:\n        EID => (9,)\n\njulia> sg = sample_neighbors(g, 2:3, dropnodes=true)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 9\n    ndata:\n        NID => (10,)\n    edata:\n        EID => (9,)\n\njulia> sg.ndata.NID\n10-element Vector{Int64}:\n  2\n  3\n 17\n 14\n 18\n 15\n 16\n 20\n  7\n 10\n\njulia> sample_neighbors(g, 2:3, 5, replace=true)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 10\n    edata:\n        EID => (10,)\n\n\n\n\n\n","category":"function"},{"location":"datasets/#Datasets","page":"Datasets","title":"Datasets","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"GraphNeuralNetworks.jl doesn't come with its own datasets, but leverages those available in the Julia (and non-Julia) ecosystem. In particular, the examples in the GraphNeuralNetworks.jl repository make use of the MLDatasets.jl package. There you will find common graph datasets such as Cora, PubMed, and Citeseer. Also MLDatasets gives access to the TUDataset repository and its numerous datasets.","category":"page"},{"location":"dev/#Developer-Notes","page":"Developer Notes","title":"Developer Notes","text":"","category":"section"},{"location":"dev/#Benchmarking","page":"Developer Notes","title":"Benchmarking","text":"","category":"section"},{"location":"dev/","page":"Developer Notes","title":"Developer Notes","text":"You can benchmark the effect on performance of your commits using the script perf/perf.jl.","category":"page"},{"location":"dev/","page":"Developer Notes","title":"Developer Notes","text":"First, checkout and benchmark the master branch:","category":"page"},{"location":"dev/","page":"Developer Notes","title":"Developer Notes","text":"julia> include(\"perf.jl\")\n\njulia> df = run_benchmarks()\n\n# observe results\njulia> for g in groupby(df, :layer); println(g, \"\\n\"); end\n\njulia> @save \"perf_master_20210803_mymachine.jld2\" dfmaster=df","category":"page"},{"location":"dev/","page":"Developer Notes","title":"Developer Notes","text":"Now checkout your branch and do the same:","category":"page"},{"location":"dev/","page":"Developer Notes","title":"Developer Notes","text":"julia> df = run_benchmarks()\n\njulia> @save \"perf_pr_20210803_mymachine.jld2\" dfpr=df","category":"page"},{"location":"dev/","page":"Developer Notes","title":"Developer Notes","text":"Finally, compare the results:","category":"page"},{"location":"dev/","page":"Developer Notes","title":"Developer Notes","text":"julia> @load \"perf_master_20210803_mymachine.jld2\"\n\njulia> @load \"perf_pr_20210803_mymachine.jld2\"\n\njulia> compare(dfpr, dfmaster)","category":"page"},{"location":"api/utils/","page":"Utils","title":"Utils","text":"CurrentModule = GraphNeuralNetworks","category":"page"},{"location":"api/utils/#Utility-Functions","page":"Utils","title":"Utility Functions","text":"","category":"section"},{"location":"api/utils/#Index","page":"Utils","title":"Index","text":"","category":"section"},{"location":"api/utils/","page":"Utils","title":"Utils","text":"Order = [:type, :function]\nPages   = [\"utils.md\"]","category":"page"},{"location":"api/utils/#Docs","page":"Utils","title":"Docs","text":"","category":"section"},{"location":"api/utils/#Graph-wise-operations","page":"Utils","title":"Graph-wise operations","text":"","category":"section"},{"location":"api/utils/","page":"Utils","title":"Utils","text":"GraphNeuralNetworks.reduce_nodes\nGraphNeuralNetworks.reduce_edges\nGraphNeuralNetworks.softmax_nodes\nGraphNeuralNetworks.softmax_edges\nGraphNeuralNetworks.broadcast_nodes\nGraphNeuralNetworks.broadcast_edges","category":"page"},{"location":"api/utils/#GraphNeuralNetworks.reduce_nodes","page":"Utils","title":"GraphNeuralNetworks.reduce_nodes","text":"reduce_nodes(aggr, g, x)\n\nFor a batched graph g, return the graph-wise aggregation of the node features x. The aggregation operator aggr can be +, mean, max, or min. The returned array will have last dimension g.num_graphs.\n\n\n\n\n\n","category":"function"},{"location":"api/utils/#GraphNeuralNetworks.reduce_edges","page":"Utils","title":"GraphNeuralNetworks.reduce_edges","text":"reduce_edges(aggr, g, e)\n\nFor a batched graph g, return the graph-wise aggregation of the edge features e. The aggregation operator aggr can be +, mean, max, or min. The returned array will have last dimension g.num_graphs.\n\n\n\n\n\n","category":"function"},{"location":"api/utils/#GraphNeuralNetworks.softmax_nodes","page":"Utils","title":"GraphNeuralNetworks.softmax_nodes","text":"softmax_nodes(g, x)\n\nGraph-wise softmax of the node features x.\n\n\n\n\n\n","category":"function"},{"location":"api/utils/#GraphNeuralNetworks.softmax_edges","page":"Utils","title":"GraphNeuralNetworks.softmax_edges","text":"softmax_edges(g, e)\n\nGraph-wise softmax of the edge features e.\n\n\n\n\n\n","category":"function"},{"location":"api/utils/#GraphNeuralNetworks.broadcast_nodes","page":"Utils","title":"GraphNeuralNetworks.broadcast_nodes","text":"broadcast_nodes(g, x)\n\nGraph-wise broadcast array x of size (*, g.num_graphs)  to size (*, g.num_nodes).\n\n\n\n\n\n","category":"function"},{"location":"api/utils/#GraphNeuralNetworks.broadcast_edges","page":"Utils","title":"GraphNeuralNetworks.broadcast_edges","text":"broadcast_edges(g, x)\n\nGraph-wise broadcast array x of size (*, g.num_graphs)  to size (*, g.num_edges).\n\n\n\n\n\n","category":"function"},{"location":"api/utils/#Neighborhood-operations","page":"Utils","title":"Neighborhood operations","text":"","category":"section"},{"location":"api/utils/","page":"Utils","title":"Utils","text":"GraphNeuralNetworks.softmax_edge_neighbors","category":"page"},{"location":"api/utils/#GraphNeuralNetworks.softmax_edge_neighbors","page":"Utils","title":"GraphNeuralNetworks.softmax_edge_neighbors","text":"softmax_edge_neighbors(g, e)\n\nSoftmax over each node's neighborhood of the edge features e.\n\nmathbfe_jto i = frace^mathbfe_jto i\n                    sum_jin N(i) e^mathbfe_jto i\n\n\n\n\n\n","category":"function"},{"location":"api/utils/#NNlib","page":"Utils","title":"NNlib","text":"","category":"section"},{"location":"api/utils/","page":"Utils","title":"Utils","text":"Primitive functions implemented in NNlib.jl.","category":"page"},{"location":"api/utils/","page":"Utils","title":"Utils","text":"NNlib.gather!\nNNlib.gather\nNNlib.scatter!\nNNlib.scatter","category":"page"},{"location":"api/utils/#NNlib.gather!","page":"Utils","title":"NNlib.gather!","text":"NNlib.gather!(dst, src, idx)\n\nReverse operation of scatter!. Gathers data from source src  and writes it in destination dst according to the index array idx. For each k in CartesianIndices(idx), assign values to dst according to\n\ndst[:, ... , k] .= src[:, ... , idx[k]...]\n\nNotice that if idx is a vector containing integers, and both dst and src are matrices, previous expression simplifies to\n\ndst[:, k] .= src[:, idx[k]]\n\nand k will run over 1:length(idx). \n\nThe elements of idx can be integers or integer tuples and may be repeated.  A single src column can end up being copied into zero, one,  or multiple dst columns.\n\nSee gather for an allocating version.\n\n\n\n\n\n","category":"function"},{"location":"api/utils/#NNlib.gather","page":"Utils","title":"NNlib.gather","text":"NNlib.gather(src, idx) -> dst\n\nReverse operation of scatter. Gathers data from source src  and writes it in a destination dst according to the index array idx. For each k in CartesianIndices(idx), assign values to dst  according to\n\ndst[:, ... , k] .= src[:, ... , idx[k]...]\n\nNotice that if idx is a vector containing integers and src is a matrix, previous expression simplifies to\n\ndst[:, k] .= src[:, idx[k]]\n\nand k will run over 1:length(idx). \n\nThe elements of idx can be integers or integer tuples and may be repeated.  A single src column can end up being copied into zero, one,  or multiple dst columns.\n\nSee gather! for an in-place version.\n\nExamples\n\njulia> NNlib.gather([1,20,300,4000], [2,4,2])\n3-element Vector{Int64}:\n   20\n 4000\n   20\n\njulia> NNlib.gather([1 2 3; 4 5 6], [1,3,1,3,1])\n2×5 Matrix{Int64}:\n 1  3  1  3  1\n 4  6  4  6  4\n\n\n\n\n\n","category":"function"},{"location":"api/utils/#NNlib.scatter!","page":"Utils","title":"NNlib.scatter!","text":"NNlib.scatter!(op, dst, src, idx)\n\nScatter operation, which writes data in src into dst at locations idx. A binary reduction operator op is applied during the scatter.  For each index k in idx, accumulates values in dst according to\n\ndst[:, ..., idx[k]...] = (op).(dst[:, ..., idx[k]...], src[:, ..., k...])\n\nSee also scatter, gather.\n\nArguments\n\nop: Operations to be applied on dst and src, e.g. +, -, *, /, max, min and mean.\ndst: The destination for src to aggregate to. This argument will be mutated.\nsrc: The source data for aggregating.\nidx: The mapping for aggregation from source (index) to destination (value).         The idx array can contain either integers or tuples.\n\nExamples\n\njulia> NNlib.scatter!(+, ones(3), [10,100], [1,3])\n3-element Vector{Float64}:\n  11.0\n   1.0\n 101.0\n\njulia> NNlib.scatter!(*, fill(0.5, 2, 4), [1 10; 100 1000], [3,2])\n2×4 Matrix{Float64}:\n 0.5    5.0   0.5  0.5\n 0.5  500.0  50.0  0.5\n\n\n\n\n\n","category":"function"},{"location":"api/utils/#NNlib.scatter","page":"Utils","title":"NNlib.scatter","text":"NNlib.scatter(op, src, idx; [init, dstsize])\n\nScatter operation allocating a destination array dst and  calling scatter!(op, dst, src, idx) on it.\n\nIf keyword init is provided, it is used to initialize the content of dst. Otherwise, the init values is inferred from the reduction operator op for some common operators (e.g. init = 0 for op = +). \nIf dstsize is provided, it will be used to define the size of destination array, otherwise it will be inferred by src and idx.\n\nSee scatter! for full details on how idx works.\n\nExamples\n\njulia> NNlib.scatter(+, [10,100,1000], [3,1,2])\n3-element Vector{Int64}:\n  100\n 1000\n   10\n\njulia> NNlib.scatter(+, [1 2 3 4; 5 6 7 8], [2,1,1,5])\n2×5 Matrix{Int64}:\n  5  1  0  0  4\n 13  5  0  0  8\n\njulia> NNlib.scatter(*, [10,200,3000], [1,4,2]; init = 10, dstsize = 6)\n6-element Vector{Int64}:\n   100\n 30000\n    10\n  2000\n    10\n    10\n\n\n\n\n\n","category":"function"},{"location":"gnngraph/#Graphs","page":"Graphs","title":"Graphs","text":"","category":"section"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"The fundamental graph type in GraphNeuralNetworks.jl is the GNNGraph. A GNNGraph g is a directed graph with nodes labeled from 1 to g.num_nodes. The underlying implementation allows for efficient application of graph neural network operators, gpu movement, and storage of node/edge/graph related feature arrays.","category":"page"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"GNNGraph inherits from Graphs.jl's AbstractGraph, therefore it supports most functionality from that library. ","category":"page"},{"location":"gnngraph/#Graph-Creation","page":"Graphs","title":"Graph Creation","text":"","category":"section"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"A GNNGraph can be created from several different data sources encoding the graph topology:","category":"page"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"using GraphNeuralNetworks, Graphs, SparseArrays\n\n\n# Construct a GNNGraph from from a Graphs.jl's graph\nlg = erdos_renyi(10, 30)\ng = GNNGraph(lg)\n\n# Same as above using convenience method rand_graph\ng = rand_graph(10, 60)\n\n# From an adjacency matrix\nA = sprand(10, 10, 0.3)\ng = GNNGraph(A)\n\n# From an adjacency list\nadjlist = [[2,3], [1,3], [1,2,4], [3]]\ng = GNNGraph(adjlist)\n\n# From COO representation\nsource = [1,1,2,2,3,3,3,4]\ntarget = [2,3,1,3,1,2,4,3]\ng = GNNGraph(source, target)","category":"page"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"See also the related methods Graphs.adjacency_matrix, edge_index, and adjacency_list.","category":"page"},{"location":"gnngraph/#Basic-Queries","page":"Graphs","title":"Basic Queries","text":"","category":"section"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"julia> source = [1,1,2,2,3,3,3,4];\n\njulia> target = [2,3,1,3,1,2,4,3];\n\njulia> g = GNNGraph(source, target)\nGNNGraph:\n    num_nodes = 4\n    num_edges = 8\n\n\njulia> @assert g.num_nodes == 4   # number of nodes\n\njulia> @assert g.num_edges == 8   # number of edges\n\njulia> @assert g.num_graphs == 1  # number of subgraphs (a GNNGraph can batch many graphs together)\n\njulia> is_directed(g)      # a GNNGraph is always directed\ntrue\n\njulia> is_bidirected(g)      # for each edge, also the reverse edge is present\ntrue\n\njulia> has_self_loops(g)\nfalse\n\njulia> has_multi_edges(g)      \nfalse","category":"page"},{"location":"gnngraph/#Data-Features","page":"Graphs","title":"Data Features","text":"","category":"section"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"One or more arrays can be associated to nodes, edges, and (sub)graphs of a GNNGraph. They will be stored in the fields g.ndata, g.edata, and g.gdata respectivaly. The data fields are NamedTuples. The array they contain must have last dimension equal to num_nodes (in ndata), num_edges (in edata), or num_graphs (in gdata).","category":"page"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"# Create a graph with a single feature array `x` associated to nodes\ng = GNNGraph(erdos_renyi(10,  30), ndata = (; x = rand(Float32, 32, 10)))\n\ng.ndata.x  # access the features\n\n# Equivalent definition passing directly the array\ng = GNNGraph(erdos_renyi(10,  30), ndata = rand(Float32, 32, 10))\n\ng.ndata.x  # `:x` is the default name for node features\n\n# You can have multiple feature arrays\ng = GNNGraph(erdos_renyi(10,  30), ndata = (; x=rand(Float32, 32, 10), y=rand(Float32, 10)))\n\ng.ndata.y, g.ndata.x\n\n# Attach an array with edge features.\n# Since `GNNGraph`s are directed, the number of edges\n# will be double that of the original Graphs' undirected graph.\ng = GNNGraph(erdos_renyi(10,  30), edata = rand(Float32, 60))\n@assert g.num_edges == 60\n\ng.edata.e\n\n# If we pass only half of the edge features, they will be copied\n# on the reversed edges.\ng = GNNGraph(erdos_renyi(10,  30), edata = rand(Float32, 30))\n\n\n# Create a new graph from previous one, inheriting edge data\n# but replacing node data\ng′ = GNNGraph(g, ndata =(; z = ones(Float32, 16, 10)))\n\ng.ndata.z\ng.edata.e","category":"page"},{"location":"gnngraph/#Edge-weights","page":"Graphs","title":"Edge weights","text":"","category":"section"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"It is common to denote scalar edge features as edge weights. The GNNGraph has specific support for edge weights: they can be stored as part of internal representions of the graph (COO or adjacency matrix). Some graph convolutional layers, most notably the GCNConv, can use the edge weights to perform weighted sums over the nodes' neighborhoods.","category":"page"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"julia> source = [1, 1, 2, 2, 3, 3];\n\njulia> target = [2, 3, 1, 3, 1, 2];\n\njulia> weight = [1.0, 0.5, 2.1, 2.3, 4, 4.1];\n\njulia> g = GNNGraph(source, target, weight)\nGNNGraph:\n    num_nodes = 3\n    num_edges = 6\n\njulia> get_edge_weight(g)\n6-element Vector{Float64}:\n 1.0\n 0.5\n 2.1\n 2.3\n 4.0\n 4.1","category":"page"},{"location":"gnngraph/#Batches-and-Subgraphs","page":"Graphs","title":"Batches and Subgraphs","text":"","category":"section"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"Multiple GNNGraphs can be batched togheter into a single graph containing the total number of the original nodes  and where the original graphs are disjoint subgraphs.","category":"page"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"using Flux\n\ngall = Flux.batch([GNNGraph(erdos_renyi(10, 30), ndata=rand(Float32,3,10)) for _ in 1:160])\n\n@assert gall.num_graphs == 160 \n@assert gall.num_nodes == 1600   # 10 nodes x 160 graphs\n@assert gall.num_edges == 9600  # 30 undirected edges x 2 directions x 160 graphs\n\ng23, _ = getgraph(gall, 2:3)\n@assert g23.num_graphs == 2\n@assert g23.num_nodes == 20   # 10 nodes x 160 graphs\n@assert g23.num_edges == 120  # 30 undirected edges x 2 directions x 2 graphs x\n\n\n# DataLoader compatibility\ntrain_loader = Flux.Data.DataLoader(gall, batchsize=16, shuffle=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes == 160\n    @assert size(g.ndata.x) = (3, 160)    \n    .....\nend\n\n# Access the nodes' graph memberships \ngraph_indicator(gall)","category":"page"},{"location":"gnngraph/#Graph-Manipulation","page":"Graphs","title":"Graph Manipulation","text":"","category":"section"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"g′ = add_self_loops(g)\ng′ = remove_self_loops(g)\ng′ = add_edges(g, [1, 2], [2, 3]) # add edges 1->2 and 2->3","category":"page"},{"location":"gnngraph/#GPU-movement","page":"Graphs","title":"GPU movement","text":"","category":"section"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"Move a GNNGraph to a CUDA device using Flux.gpu method. ","category":"page"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"using Flux: gpu\n\ng_gpu = g |> gpu","category":"page"},{"location":"gnngraph/#JuliaGraphs/Graphs.jl-integration","page":"Graphs","title":"JuliaGraphs/Graphs.jl integration","text":"","category":"section"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"Since GNNGraph <: Graphs.AbstractGraph, we can use any functionality from Graphs.jl.  Moreover, GNNGraphs can be constructed from Graphs.Graph and Graphs.DiGraph.","category":"page"},{"location":"gnngraph/","page":"Graphs","title":"Graphs","text":"julia> import Graphs\n\njulia> using GraphNeuralNetworks\n\n# A Graphs.jl undirected graph\njulia> gu = Graphs.erdos_renyi(10, 20)    \n{10, 20} undirected simple Int64 graph\n\n# Since GNNGraphs are undirected, the edges are doubled when converting \n# to GNNGraph\njulia> GNNGraph(gu)  # Since GNNGraphs are \nGNNGraph:\n    num_nodes = 10\n    num_edges = 40\n\n# A Graphs.jl directed graph\njulia> gd = Graphs.erdos_renyi(10, 20, is_directed=true)\n{10, 20} directed simple Int64 graph\n\njulia> GNNGraph(gd)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 20","category":"page"},{"location":"gsoc/#Graph-Neural-Networks-Summer-of-Code","page":"Summer Of Code","title":"Graph Neural Networks - Summer of Code","text":"","category":"section"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"Graph neural networks (GNN) are deep learning models well adapated to data  rapresented by graphs along with feature vectors associated to nodes and edges.  GNNs are a growing are of research and find many applications in complex networks analysis, relational reasoning, combinatorial optimization, molecule generation, and many other fields.  GraphNeuralNetworks.jl is a pure Julia package for GNNs already equipped with plenty of features such as many common graph convolutional layers, CUDA support and graph batching for fast parallel operations. There are a number of ways by which the package could be improved:","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"Adding graph convolutional layers (duration: 175h, expected difficulty: easy to medium):","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"While we implement a good variety of graph convolutional layers, there is still a vast zoology to be implemented yet.","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"Adding models and examples (duration: 175h, expected difficulty: medium):","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"As part of the documentation and for bootstrapping new projects,  we want to add fully worked out examples and applications of graph neural networks.","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"Adding graph datasets (duration: 175h, expected difficulty: easy):","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":". Provide julia friendly wrappers for common graph datasets in MLDatasets.jl.","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"Supporting heterogeneous graphs (duration: 175h, expected difficulty: hard):","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"In some complex networks, the relations expressed by edges can be of different types. We need to implement an heteroeneous graph type and implement convolutional layers supporting them. ","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"Training on very large graphs (duration: 175h, expected difficulty: medium to hard):","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"Graph containing several milions of nodes are too large for gpu memory. Mini-batch training si performed on subgraphs, as in the GraphSAGE algorithm.","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"Supporting temporal graph neural networks (duration: 175h, expected difficulty: hard):","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"We aim at implementing temporal graph convolutions for time-varying graph and/or node features.","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"Improving perfomance using sparse linear algebra (duration: 175h, expected difficulty: medium to hard): ","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"Many graph convolutional layers can be expressed as non-materializing algebraic operations involving the adjacency matrix instead of the slower and more memory consuming gather/scatter mechanism. We aim at extending as far as possible and in a gpu-friendly way these fused implementation.","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"Recommended skills: Familiarity with graph neural networks and Flux.jl.","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"Expected results: New features added to the package along with tests and relevant documentation.","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"Mentors: Carlo Lucibello (author of GraphNeuralNetworks.jl.For linear algebra, co-mentoring by Will Kimmerer (lead developer of SuiteSparseGraphBLAS.jl).","category":"page"},{"location":"gsoc/","page":"Summer Of Code","title":"Summer Of Code","text":"Contact: Feel free to contact us on the Julia Slack Workspace or by opening an issue on GraphNeuralNetworks.jl.","category":"page"},{"location":"models/#Models","page":"Model Building","title":"Models","text":"","category":"section"},{"location":"models/","page":"Model Building","title":"Model Building","text":"GraphNeuralNetworks.jl provides common graph convolutional layers by which you can assemble arbitrarily deep or complex models. GNN layers are compatible with  Flux.jl ones, therefore expert Flux users are promptly able to define and train  their models. ","category":"page"},{"location":"models/","page":"Model Building","title":"Model Building","text":"In what follows, we discuss two different styles for model creation: the explicit modeling style, more verbose but more flexible,  and the implicit modeling style based on GNNChain, more concise but less flexible.","category":"page"},{"location":"models/#Explicit-modeling","page":"Model Building","title":"Explicit modeling","text":"","category":"section"},{"location":"models/","page":"Model Building","title":"Model Building","text":"In the explicit modeling style, the model is created according to the following steps:","category":"page"},{"location":"models/","page":"Model Building","title":"Model Building","text":"Define a new type for your model (GNN in the example below). Layers and submodels are fields.\nApply Flux.@functor to the new type to make it Flux's compatible (parameters' collection, gpu movement, etc...)\nOptionally define a convenience constructor for your model.\nDefine the forward pass by implementing the call method for your type.\nInstantiate the model. ","category":"page"},{"location":"models/","page":"Model Building","title":"Model Building","text":"Here is an example of this construction:","category":"page"},{"location":"models/","page":"Model Building","title":"Model Building","text":"using Flux, Graphs, GraphNeuralNetworks\n\nstruct GNN                                # step 1\n    conv1\n    bn\n    conv2\n    dropout\n    dense\nend\n\nFlux.@functor GNN                              # step 2\n\nfunction GNN(din::Int, d::Int, dout::Int) # step 3    \n    GNN(GCNConv(din => d),\n        BatchNorm(d),\n        GraphConv(d => d, relu),\n        Dropout(0.5),\n        Dense(d, dout))\nend\n\nfunction (model::GNN)(g::GNNGraph, x)     # step 4\n    x = model.conv1(g, x)\n    x = relu.(model.bn(x))\n    x = model.conv2(g, x)\n    x = model.dropout(x)\n    x = model.dense(x)\n    return x \nend\n\ndin, d, dout = 3, 4, 2 \nmodel = GNN(din, d, dout)                 # step 5\n\ng = rand_graph(10, 30)\nX = randn(Float32, din, 10) \n\ny = model(g, X)  # output size: (dout, g.num_nodes)\ngs = gradient(() -> sum(model(g, X)), Flux.params(model))","category":"page"},{"location":"models/#Implicit-modeling-with-GNNChains","page":"Model Building","title":"Implicit modeling with GNNChains","text":"","category":"section"},{"location":"models/","page":"Model Building","title":"Model Building","text":"While very flexible, the way in which we defined GNN model definition in last section is a bit verbose. In order to simplify things, we provide the GNNChain type. It is very similar  to Flux's well known Chain. It allows to compose layers in a sequential fashion as Chain does, propagating the output of each layer to the next one. In addition, GNNChain  handles propagates the input graph as well, providing it as a first argument to layers subtyping the GNNLayer abstract type. ","category":"page"},{"location":"models/","page":"Model Building","title":"Model Building","text":"Using GNNChain, the previous example becomes","category":"page"},{"location":"models/","page":"Model Building","title":"Model Building","text":"using Flux, Graphs, GraphNeuralNetworks\n\ndin, d, dout = 3, 4, 2 \ng = rand_graph(10, 30)\nX = randn(Float32, din, 10)\n\nmodel = GNNChain(GCNConv(din => d),\n                 BatchNorm(d),\n                 x -> relu.(x),\n                 GCNConv(d => d, relu),\n                 Dropout(0.5),\n                 Dense(d, dout))","category":"page"},{"location":"models/","page":"Model Building","title":"Model Building","text":"The GNNChain only propagates the graph and the node features. More complex scenarios, e.g. when also edge features are updated, have to be handled using the explicit definition of the forward pass. ","category":"page"},{"location":"models/","page":"Model Building","title":"Model Building","text":"A GNNChain opportunely propagates the graph into the branches created by the Flux.Parallel layer:","category":"page"},{"location":"models/","page":"Model Building","title":"Model Building","text":"AddResidual(l) = Parallel(+, identity, l)  # implementing a skip/residual connection\n\nmodel = GNNChain( ResGatedGraphConv(din => d, relu),\n                  AddResidual(ResGatedGraphConv(d => d, relu)),\n                  AddResidual(ResGatedGraphConv(d => d, relu)),\n                  AddResidual(ResGatedGraphConv(d => d, relu)),\n                  GlobalPooling(mean),\n                  Dense(d, dout))\n\ny = model(g, X) # output size: (dout, g.num_graphs)","category":"page"},{"location":"models/#Embedding-a-graph-in-the-model","page":"Model Building","title":"Embedding a graph in the model","text":"","category":"section"},{"location":"models/","page":"Model Building","title":"Model Building","text":"Sometimes it is useful to consider a specific graph as a part of a model instead of  its input. GraphNeuralNetworks.jl provides the WithGraph type to deal with this scenario.","category":"page"},{"location":"models/","page":"Model Building","title":"Model Building","text":"chain = GNNChain(GCNConv(din => d, relu),\n                 GCNConv(d => d))\n\n\ng = rand_graph(10, 30)\n\nmodel = WithGraph(chain, g)\n\nX = randn(Float32, din, 10)\n\n# Pass only X as input, the model already contains the graph.\ny = model(X) ","category":"page"},{"location":"models/","page":"Model Building","title":"Model Building","text":"An example of WithGraph usage is given in the graph neural ODE script in the examples folder.","category":"page"},{"location":"api/pool/","page":"Pooling Layers","title":"Pooling Layers","text":"CurrentModule = GraphNeuralNetworks","category":"page"},{"location":"api/pool/#Pooling-Layers","page":"Pooling Layers","title":"Pooling Layers","text":"","category":"section"},{"location":"api/pool/#Index","page":"Pooling Layers","title":"Index","text":"","category":"section"},{"location":"api/pool/","page":"Pooling Layers","title":"Pooling Layers","text":"Order = [:type, :function]\nPages   = [\"pool.md\"]","category":"page"},{"location":"api/pool/#Docs","page":"Pooling Layers","title":"Docs","text":"","category":"section"},{"location":"api/pool/","page":"Pooling Layers","title":"Pooling Layers","text":"Modules = [GraphNeuralNetworks]\nPages   = [\"layers/pool.jl\"]\nPrivate = false","category":"page"},{"location":"api/pool/#GraphNeuralNetworks.GlobalAttentionPool","page":"Pooling Layers","title":"GraphNeuralNetworks.GlobalAttentionPool","text":"GlobalAttentionPool(fgate, ffeat=identity)\n\nGlobal soft attention layer from the Gated Graph Sequence Neural Networks paper\n\nmathbfu_V = sum_iin V alpha_i f_feat(mathbfx_i)\n\nwhere the coefficients alpha_i are given by a softmax_nodes operation:\n\nalpha_i = frace^f_gate(mathbfx_i)\n                sum_iin V e^f_gate(mathbfx_i)\n\nArguments\n\nfgate: The function f_gate mathbbR^D_in to mathbbR.           It is tipically expressed by a neural network.\nffeat: The function f_feat mathbbR^D_in to mathbbR^D_out.           It is tipically expressed by a neural network.\n\nExamples\n\nchin = 6\nchout = 5    \n\nfgate = Dense(chin, 1)\nffeat = Dense(chin, chout)\npool = GlobalAttentionPool(fgate, ffeat)\n\ng = Flux.batch([GNNGraph(random_regular_graph(10, 4), \n                         ndata=rand(Float32, chin, 10)) \n                for i=1:3])\n\nu = pool(g, g.ndata.x)\n\n@assert size(u) == (chout, g.num_graphs)\n\n\n\n\n\n","category":"type"},{"location":"api/pool/#GraphNeuralNetworks.GlobalPool","page":"Pooling Layers","title":"GraphNeuralNetworks.GlobalPool","text":"GlobalPool(aggr)\n\nGlobal pooling layer for graph neural networks. Takes a graph and feature nodes as inputs and performs the operation\n\nmathbfu_V = square_i in V mathbfx_i\n\nwhere V is the set of nodes of the input graph and  the type of aggregation represented by square is selected by the aggr argument.  Commonly used aggregations are mean, max, and +.\n\nSee also reduce_nodes.\n\nExamples\n\nusing Flux, GraphNeuralNetworks, Graphs\n\npool = GlobalPool(mean)\n\ng = GNNGraph(erdos_renyi(10, 4))\nX = rand(32, 10)\npool(g, X) # => 32x1 matrix\n\n\ng = Flux.batch([GNNGraph(erdos_renyi(10, 4)) for _ in 1:5])\nX = rand(32, 50)\npool(g, X) # => 32x5 matrix\n\n\n\n\n\n","category":"type"},{"location":"api/pool/#GraphNeuralNetworks.TopKPool","page":"Pooling Layers","title":"GraphNeuralNetworks.TopKPool","text":"TopKPool(adj, k, in_channel)\n\nTop-k pooling layer.\n\nArguments\n\nadj: Adjacency matrix  of a graph.\nk: Top-k nodes are selected to pool together.\nin_channel: The dimension of input channel.\n\n\n\n\n\n","category":"type"},{"location":"messagepassing/#Message-Passing","page":"Message Passing","title":"Message Passing","text":"","category":"section"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"A generic message passing on graph takes the form","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"beginaligned\nmathbfm_jto i = phi(mathbfx_i mathbfx_j mathbfe_jto i) \nbarmathbfm_i = square_jin N(i)  mathbfm_jto i \nmathbfx_i = gamma_x(mathbfx_i barmathbfm_i)\nmathbfe_jto i^prime =  gamma_e(mathbfe_j to imathbfm_j to i)\nendaligned","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"where we refer to phi as to the message function,  and to gamma_x and gamma_e as to the node update and edge update function respectively. The aggregation square is over the neighborhood N(i) of node i,  and it is usually equal either to sum, to max or to a mean operation. ","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"In GraphNeuralNetworks.jl, the function propagate takes care of materializing the node features on each edge, applying the message function, performing the aggregation, and returning barmathbfm.  It is then left to the user to perform further node and edge updates, manipulating arrays of size D_node times num_nodes and    D_edge times num_edges.","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"propagate is composed of two steps corresponding to two exported methods:","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"apply_edges materializes node features on edges and ","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"performs edge-related computation without. ","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"aggregate_neighbors applies a reduction operator on the messages coming","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"from the neighborhood of each node.","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"The whole propagation mechanism internally relies on the NNlib.gather  and NNlib.scatter methods.","category":"page"},{"location":"messagepassing/#Examples","page":"Message Passing","title":"Examples","text":"","category":"section"},{"location":"messagepassing/#Basic-use-of-apply_edges-and-propagate","page":"Message Passing","title":"Basic use of apply_edges and propagate","text":"","category":"section"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"The function apply_edges can be used to broadcast node data on each edge and produce new edge data.","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"julia> using GraphNeuralNetworks, Graphs, Statistics\n\njulia> g = rand_graph(10, 20)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 20\n\njulia> x = ones(2,10);\n\njulia> z = 2ones(2,10);\n\n# Return an edge features arrays (D × num_edges)\njulia> apply_edges((xi, xj, e) -> xi .+ xj, g, xi=x, xj=z)\n2×20 Matrix{Float64}:\n 3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0\n 3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0\n\n# now returning a named tuple\njulia> apply_edges((xi, xj, e) -> (a=xi .+ xj, b=xi .- xj), g, xi=x, xj=z)\n(a = [3.0 3.0 … 3.0 3.0; 3.0 3.0 … 3.0 3.0], b = [-1.0 -1.0 … -1.0 -1.0; -1.0 -1.0 … -1.0 -1.0])\n\n# Here we provide a named tuple input\njulia> apply_edges((xi, xj, e) -> xi.a + xi.b .* xj, g, xi=(a=x,b=z), xj=z)\n2×20 Matrix{Float64}:\n 5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0\n 5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"The function propagate instead performs the apply_edges operation but then also applies a reduction over each node's neighborhood (see aggregate_neighbors).","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"julia> propagate((xi, xj, e) -> xi .+ xj, g, +, xi=x, xj=z)\n2×10 Matrix{Float64}:\n 3.0  6.0  9.0  9.0  0.0  6.0  6.0  3.0  15.0  3.0\n 3.0  6.0  9.0  9.0  0.0  6.0  6.0  3.0  15.0  3.0\n\n# Previous output can be understood by looking at the degree\njulia> degree(g)\n10-element Vector{Int64}:\n 1\n 2\n 3\n 3\n 0\n 2\n 2\n 1\n 5\n 1","category":"page"},{"location":"messagepassing/#Implementing-a-custom-Graph-Convolutional-Layer","page":"Message Passing","title":"Implementing a custom Graph Convolutional Layer","text":"","category":"section"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"Let's implement a simple graph convolutional layer using the message passing framework. The convolution reads ","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"mathbfx_i = W cdot sum_j in N(i)  mathbfx_j","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"We will also add a bias and an activation function.","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"using Flux, Graphs, GraphNeuralNetworks\n\nstruct GCN{A<:AbstractMatrix, B, F} <: GNNLayer\n    weight::A\n    bias::B\n    σ::F\nend\n\nFlux.@functor GCN # allow collecting params, gpu movement, etc...\n\nfunction GCN(ch::Pair{Int,Int}, σ=identity)\n    in, out = ch\n    W = Flux.glorot_uniform(out, in)\n    b = zeros(Float32, out)\n    GCN(W, b, σ)\nend\n\nfunction (l::GCN)(g::GNNGraph, x::AbstractMatrix{T}) where T\n    @assert size(x, 2) == g.num_nodes\n\n    # Computes messages from source/neighbour nodes (j) to target/root nodes (i).\n    # The message function will have to handle matrices of size (*, num_edges).\n    # In this simple case we just let the neighbor features go through.\n    message(xi, xj, e) = xj \n\n    # The + operator gives the sum aggregation.\n    # `mean`, `max`, `min`, and `*` are other possibilities.\n    x = propagate(message, g, +, xj=x) \n\n    return l.σ.(l.weight * x .+ l.bias)\nend","category":"page"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"See the GATConv implementation here for a more complex example.","category":"page"},{"location":"messagepassing/#Built-in-message-functions","page":"Message Passing","title":"Built-in message functions","text":"","category":"section"},{"location":"messagepassing/","page":"Message Passing","title":"Message Passing","text":"In order to exploit optimized specializations of the propagate, it is recommended  to use built-in message functions such as copy_xj whenever possible. ","category":"page"},{"location":"api/basic/","page":"Basic Layers","title":"Basic Layers","text":"CurrentModule = GraphNeuralNetworks","category":"page"},{"location":"api/basic/#Basic-Layers","page":"Basic Layers","title":"Basic Layers","text":"","category":"section"},{"location":"api/basic/#Index","page":"Basic Layers","title":"Index","text":"","category":"section"},{"location":"api/basic/","page":"Basic Layers","title":"Basic Layers","text":"Order = [:type, :function]\nModules = [GraphNeuralNetworks]\nPages = [\"basic.md\"]","category":"page"},{"location":"api/basic/#Docs","page":"Basic Layers","title":"Docs","text":"","category":"section"},{"location":"api/basic/","page":"Basic Layers","title":"Basic Layers","text":"Modules = [GraphNeuralNetworks]\nPages   = [\"layers/basic.jl\"]\nPrivate = false","category":"page"},{"location":"api/basic/#GraphNeuralNetworks.DotDecoder","page":"Basic Layers","title":"GraphNeuralNetworks.DotDecoder","text":"DotDecoder()\n\nA graph neural network layer that  for given input graph g and node features x, returns the dot product x_i ⋅ xj on each edge. \n\nExamples\n\njulia> g = rand_graph(5, 6)\nGNNGraph:\n    num_nodes = 5\n    num_edges = 6\n\njulia> dotdec = DotDecoder()\nDotDecoder()\n\njulia> dotdec(g, rand(2, 5))\n1×6 Matrix{Float64}:\n 0.345098  0.458305  0.106353  0.345098  0.458305  0.106353\n\n\n\n\n\n","category":"type"},{"location":"api/basic/#GraphNeuralNetworks.GNNChain","page":"Basic Layers","title":"GraphNeuralNetworks.GNNChain","text":"GNNChain(layers...)\nGNNChain(name = layer, ...)\n\nCollects multiple layers / functions to be called in sequence on given input graph and input node features. \n\nIt allows to compose layers in a sequential fashion as Flux.Chain does, propagating the output of each layer to the next one. In addition, GNNChain handles the input graph as well, providing it  as a first argument only to layers subtyping the GNNLayer abstract type. \n\nGNNChain supports indexing and slicing, m[2] or m[1:end-1], and if names are given, m[:name] == m[1] etc.\n\nExamples\n\njulia> m = GNNChain(GCNConv(2=>5), BatchNorm(5), x -> relu.(x), Dense(5, 4));\n\njulia> x = randn(Float32, 2, 3);\n\njulia> g = GNNGraph([1,1,2,3], [2,3,1,1]);\n\njulia> m(g, x)\n4×3 Matrix{Float32}:\n  0.157941    0.15443     0.193471\n  0.0819516   0.0503105   0.122523\n  0.225933    0.267901    0.241878\n -0.0134364  -0.0120716  -0.0172505\n\n\n\n\n\n","category":"type"},{"location":"api/basic/#GraphNeuralNetworks.GNNLayer","page":"Basic Layers","title":"GraphNeuralNetworks.GNNLayer","text":"abstract type GNNLayer end\n\nAn abstract type from which graph neural network layers are derived.\n\nSee also GNNChain.\n\n\n\n\n\n","category":"type"},{"location":"api/basic/#GraphNeuralNetworks.WithGraph","page":"Basic Layers","title":"GraphNeuralNetworks.WithGraph","text":"WithGraph(model, g::GNNGraph; traingraph=false)\n\nA type wrapping the model and tying it to the graph g. In the forward pass, can only take feature arrays as inputs, returning model(g, x...; kws...).\n\nIf traingraph=false, the graph's parameters, won't be collected when calling Flux.params on a WithGraph object.\n\nExamples\n\ng = GNNGraph([1,2,3], [2,3,1])\nx = rand(Float32, 2, 3)\nmodel = SAGEConv(2 => 3)\nwg = WithGraph(model, g)\n# No need to feed the graph to `wg`\n@assert wg(x) == model(g, x)\n\ng2 = GNNGraph([1,1,2,3], [2,4,1,1])\nx2 = rand(Float32, 2, 4)\n# WithGraph will ignore the internal graph if fed with a new one. \n@assert wg(g2, x2) == model(g2, x2)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"CurrentModule = GraphNeuralNetworks","category":"page"},{"location":"api/conv/#Convolutional-Layers","page":"Convolutional Layers","title":"Convolutional Layers","text":"","category":"section"},{"location":"api/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Many different types of graphs convolutional layers have been proposed in the literature. Choosing the right layer for your application can bould involve a lot of exploration.  Some of the most commonly used layers are the GCNConv and the GATv2Conv. Multiple graph convolutional layers are typically stacked together to create a graph neural network model (see GNNChain).","category":"page"},{"location":"api/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"The table below lists all graph convolutional layers implemented in the GraphNeuralNetworks.jl. It also highlights the presence of some additional capabilities with respect to basic message passing:","category":"page"},{"location":"api/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Sparse Ops: implements message passing as multiplication by sparse adjacency matrix instead of the gather/scatter mechanism. This can lead to better cpu performances but it is not supported on gpu yet. \nEdge Weights: supports scalar weights (or equivalently scalar features) on edges. \nEdge Features: supports feature vectors on edges.","category":"page"},{"location":"api/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Layer Sparse Ops Edge Weight Edge Features\nAGNNConv   ✓\nCGConv   \nChebConv   \nEdgeConv   \nGATConv   ✓\nGATv2Conv   ✓\nGatedGraphConv ✓  \nGCNConv ✓ ✓ \nGINConv ✓  \nGraphConv ✓  \nMEGNetConv   ✓\nNNConv   ✓\nResGatedGraphConv   \nSAGEConv ✓  ","category":"page"},{"location":"api/conv/#Docs","page":"Convolutional Layers","title":"Docs","text":"","category":"section"},{"location":"api/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Modules = [GraphNeuralNetworks]\nPages   = [\"layers/conv.jl\"]\nPrivate = false","category":"page"},{"location":"api/conv/#GraphNeuralNetworks.AGNNConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.AGNNConv","text":"AGNNConv(init_beta=1f0)\n\nAttention-based Graph Neural Network layer from paper Attention-based Graph Neural Network for Semi-Supervised Learning.\n\nThe forward pass is given by\n\nmathbfx_i = sum_j in N(i) cup i alpha_ij W mathbfx_j\n\nwhere the attention coefficients alpha_ij are given by\n\nalpha_ij =frace^beta cos(mathbfx_i mathbfx_j)\n                  sum_je^beta cos(mathbfx_i mathbfx_j)\n\nwith the cosine distance defined by\n\ncos(mathbfx_i mathbfx_j) = \n  fracmathbfx_i cdot mathbfx_jlVertmathbfx_irVert lVertmathbfx_jrVert\n\nand beta a trainable parameter.\n\nArguments\n\ninit_beta: The initial value of beta.\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.CGConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.CGConv","text":"CGConv((in, ein) => out, f, act=identity; bias=true, init=glorot_uniform, residual=false)\nCGConv(in => out, ...)\n\nThe crystal graph convolutional layer from the paper Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties. Performs the operation\n\nmathbfx_i = mathbfx_i + sum_jin N(i)sigma(W_f mathbfz_ij + mathbfb_f) act(W_s mathbfz_ij + mathbfb_s)\n\nwhere mathbfz_ij  is the node and edge features concatenation  mathbfx_i mathbfx_j mathbfe_jto i  and sigma is the sigmoid function. The residual mathbfx_i is added only if residual=true and the output size is the same  as the input size.\n\nArguments\n\nin: The dimension of input node features.\nein: The dimension of input edge features. \n\nIf ein is not given, assumes that no edge features are passed as input in the forward pass.\n\nout: The dimension of output node features.\nact: Activation function.\nbias: Add learnable bias.\ninit: Weights' initializer.\nresidual: Add a residual connection.\n\nExamples\n\ng = rand_graph(5, 6)\nx = rand(Float32, 2, g.num_nodes)\ne = rand(Float32, 3, g.num_edges)\n\nl = CGConv((2, 3) => 4, tanh)\ny = l(g, x, e)    # size: (4, num_nodes)\n\n# No edge features\nl = CGConv(2 => 4, tanh)\ny = l(g, x)    # size: (4, num_nodes)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.ChebConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.ChebConv","text":"ChebConv(in => out, k; bias=true, init=glorot_uniform)\n\nChebyshev spectral graph convolutional layer from paper Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering.\n\nImplements\n\nX = sum^K-1_k=0  W^(k) Z^(k)\n\nwhere Z^(k) is the k-th term of Chebyshev polynomials, and can be calculated by the following recursive form:\n\nZ^(0) = X \nZ^(1) = hatL X \nZ^(k) = 2 hatL Z^(k-1) - Z^(k-2)\n\nwith hatL the scaled_laplacian.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nk: The order of Chebyshev polynomial.\nbias: Add learnable bias.\ninit: Weights' initializer.\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.EdgeConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.EdgeConv","text":"EdgeConv(nn; aggr=max)\n\nEdge convolutional layer from paper Dynamic Graph CNN for Learning on Point Clouds.\n\nPerforms the operation\n\nmathbfx_i = square_j in N(i) nn(mathbfx_i mathbfx_j - mathbfx_i)\n\nwhere nn generally denotes a learnable function, e.g. a linear layer or a multi-layer perceptron.\n\nArguments\n\nnn: A (possibly learnable) function. \naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GATConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.GATConv","text":"GATConv(in => out, [σ; heads, concat, init, bias, negative_slope, add_self_loops])\nGATConv((in, ein) => out, ...)\n\nGraph attentional layer from the paper Graph Attention Networks.\n\nImplements the operation\n\nmathbfx_i = sum_j in N(i) cup i alpha_ij W mathbfx_j\n\nwhere the attention coefficients alpha_ij are given by\n\nalpha_ij = frac1z_i exp(LeakyReLU(mathbfa^T W mathbfx_i W mathbfx_j))\n\nwith z_i a normalization factor. \n\nIn case ein > 0 is given, edge features of dimension ein will be expected in the forward pass  and the attention coefficients will be calculated as   \\alpha_{ij} = \\frac{1}{z_i} \\exp(LeakyReLU(\\mathbf{a}^T [W_e \\mathbf{e}_{j\\to i}; W \\mathbf{x}_i; W \\mathbf{x}_j]))`\n\nArguments\n\nin: The dimension of input node features.\nein: The dimension of input edget features. Default 0 (i.e. no edge features passed in the forward).\nout: The dimension of output node features.\nσ: Activation function. Default identity.\nbias: Learn the additive bias if true. Dafault true. \nheads: Number attention heads. Dafault `1.\nconcat: Concatenate layer output or not. If not, layer output is averaged over the heads. Default true.\nnegative_slope: The parameter of LeakyReLU.Default 0.2.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default true.\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GATv2Conv","page":"Convolutional Layers","title":"GraphNeuralNetworks.GATv2Conv","text":"GATv2Conv(in => out, [σ; heads, concat, init, bias, negative_slope, add_self_loops])\nGATv2Conv((in, ein) => out, ...)\n\nGATv2 attentional layer from the paper How Attentive are Graph Attention Networks?.\n\nImplements the operation\n\nmathbfx_i = sum_j in N(i) cup i alpha_ij W_1 mathbfx_j\n\nwhere the attention coefficients alpha_ij are given by\n\nalpha_ij = frac1z_i exp(mathbfa^T LeakyReLU(W_2 mathbfx_i W_1 mathbfx_j))\n\nwith z_i a normalization factor.\n\nIn case ein > 0 is given, edge features of dimension ein will be expected in the forward pass  and the attention coefficients will be calculated as  \n\nalpha_ij = frac1z_i exp(mathbfa^T LeakyReLU(W_3 mathbfe_jto i W_2 mathbfx_i W_1 mathbfx_j))\n\nArguments\n\nin: The dimension of input node features.\nein: The dimension of input edget features. Default 0 (i.e. no edge features passed in the forward).\nout: The dimension of output node features.\nσ: Activation function. Default identity.\nbias: Learn the additive bias if true. Dafault true. \nheads: Number attention heads. Dafault `1.\nconcat: Concatenate layer output or not. If not, layer output is averaged over the heads. Default true.\nnegative_slope: The parameter of LeakyReLU.Default 0.2.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default true.\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GCNConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.GCNConv","text":"GCNConv(in => out, σ=identity; [bias, init, add_self_loops, use_edge_weight])\n\nGraph convolutional layer from paper Semi-supervised Classification with Graph Convolutional Networks.\n\nPerforms the operation\n\nmathbfx_i = sum_jin N(i) a_ij W mathbfx_j\n\nwhere a_ij = 1  sqrtN(i)N(j) is a normalization factor computed from the node degrees. \n\nIf the input graph has weighted edges and use_edge_weight=true, than a_ij will be computed as\n\na_ij = frace_jto isqrtsum_j in N(i)  e_jto i sqrtsum_i in N(j)  e_ito j\n\nThe input to the layer is a node feature array X of size (num_features, num_nodes) and optionally an edge weight vector.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nσ: Activation function. Default identity.\nbias: Add learnable bias. Default true.\ninit: Weights' initializer. Default glorot_uniform.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default false.\nuse_edge_weight: If true, consider the edge weights in the input graph (if available).                    If add_self_loops=true the new weights will be set to 1. Default false.\n\nExamples\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(3, g.num_nodes)\n\n# create layer\nl = GCNConv(3 => 5) \n\n# forward pass\ny = l(g, x)       # size:  5 × num_nodes\n\n# convolution with edge weights\nw = [1.1, 0.1, 2.3, 0.5]\ny = l(g, x, w)\n\n# Edge weights can also be embedded in the graph.\ng = GNNGraph(s, t, w)\nl = GCNConv(3 => 5, use_edge_weight=true) \ny = l(g, x) # same as l(g, x, w) \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GINConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.GINConv","text":"GINConv(f, ϵ; aggr=+)\n\nGraph Isomorphism convolutional layer from paper How Powerful are Graph Neural Networks?.\n\nImplements the graph convolution\n\nmathbfx_i = f_Thetaleft((1 + epsilon) mathbfx_i + sum_j in N(i) mathbfx_j right)\n\nwhere f_Theta typically denotes a learnable function, e.g. a linear layer or a multi-layer perceptron.\n\nArguments\n\nf: A (possibly learnable) function acting on node features. \nϵ: Weighting factor.\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GatedGraphConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.GatedGraphConv","text":"GatedGraphConv(out, num_layers; aggr=+, init=glorot_uniform)\n\nGated graph convolution layer from Gated Graph Sequence Neural Networks.\n\nImplements the recursion\n\nmathbfh^(0)_i = mathbfx_i mathbf0 \nmathbfh^(l)_i = GRU(mathbfh^(l-1)_i square_j in N(i) W mathbfh^(l-1)_j)\n\nwhere mathbfh^(l)_i denotes the l-th hidden variables passing through GRU. The dimension of input mathbfx_i needs to be less or equal to out.\n\nArguments\n\nout: The dimension of output features.\nnum_layers: The number of gated recurrent unit.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\ninit: Weight initialization function.\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GraphConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.GraphConv","text":"GraphConv(in => out, σ=identity; aggr=+, bias=true, init=glorot_uniform)\n\nGraph convolution layer from Reference: Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks.\n\nPerforms:\n\nmathbfx_i = W_1 mathbfx_i + square_j in mathcalN(i) W_2 mathbfx_j\n\nwhere the aggregation type is selected by aggr.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nσ: Activation function.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\nbias: Add learnable bias.\ninit: Weights' initializer.\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.MEGNetConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.MEGNetConv","text":"MEGNetConv(ϕe, ϕv; aggr=mean)\nMEGNetConv(in => out; aggr=mean)\n\nConvolution from Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals paper. In the forward pass, takes as inputs node features x and edge features e and returns updated features x' and e' according to \n\nmathbfe_ito j  = phi_e(mathbfx_i  mathbfx_j  mathbfe_ito j)\nmathbfx_i  = phi_v(mathbfx_i square_jin mathcalN(i)mathbfe_jto i)\n\naggr defines the aggregation to be performed.\n\nIf the neural networks ϕe and  ϕv are not provided, they will be constructed from the in and out arguments instead as multi-layer perceptron with one hidden layer and relu  activations.\n\nExamples\n\ng = rand_graph(10, 30)\nx = randn(3, 10)\ne = randn(3, 30)\nm = MEGNetConv(3 => 3)\nx′, e′ = m(g, x, e)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.NNConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.NNConv","text":"NNConv(in => out, f, σ=identity; aggr=+, bias=true, init=glorot_uniform)\n\nThe continuous kernel-based convolutional operator from the  Neural Message Passing for Quantum Chemistry paper.  This convolution is also known as the edge-conditioned convolution from the  Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs paper.\n\nPerforms the operation\n\nmathbfx_i = W mathbfx_i + square_j in N(i) f_Theta(mathbfe_jto i)mathbfx_j\n\nwhere f_Theta  denotes a learnable function (e.g. a linear layer or a multi-layer perceptron). Given an input of batched edge features e of size (num_edge_features, num_edges),  the function f will return an batched matrices array whose size is (out, in, num_edges). For convenience, also functions returning a single (out*in, num_edges) matrix are allowed.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nf: A (possibly learnable) function acting on edge features.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\nσ: Activation function.\nbias: Add learnable bias.\ninit: Weights' initializer.\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.ResGatedGraphConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.ResGatedGraphConv","text":"ResGatedGraphConv(in => out, act=identity; init=glorot_uniform, bias=true)\n\nThe residual gated graph convolutional operator from the Residual Gated Graph ConvNets paper.\n\nThe layer's forward pass is given by\n\nmathbfx_i = actbig(Umathbfx_i + sum_j in N(i) eta_ij V mathbfx_jbig)\n\nwhere the edge gates eta_ij are given by\n\neta_ij = sigmoid(Amathbfx_i + Bmathbfx_j)\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nact: Activation function.\ninit: Weight matrices' initializing function. \nbias: Learn an additive bias if true.\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.SAGEConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.SAGEConv","text":"SAGEConv(in => out, σ=identity; aggr=mean, bias=true, init=glorot_uniform)\n\nGraphSAGE convolution layer from paper Inductive Representation Learning on Large Graphs.\n\nPerforms:\n\nmathbfx_i = W cdot mathbfx_i square_j in mathcalN(i) mathbfx_j\n\nwhere the aggregation type is selected by aggr.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nσ: Activation function.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\nbias: Add learnable bias.\ninit: Weights' initializer.\n\n\n\n\n\n","category":"type"},{"location":"api/messagepassing/","page":"Message Passing","title":"Message Passing","text":"CurrentModule = GraphNeuralNetworks","category":"page"},{"location":"api/messagepassing/#Message-Passing","page":"Message Passing","title":"Message Passing","text":"","category":"section"},{"location":"api/messagepassing/#Index","page":"Message Passing","title":"Index","text":"","category":"section"},{"location":"api/messagepassing/","page":"Message Passing","title":"Message Passing","text":"Order = [:type, :function]\nPages   = [\"messagepassing.md\"]","category":"page"},{"location":"api/messagepassing/#Interface","page":"Message Passing","title":"Interface","text":"","category":"section"},{"location":"api/messagepassing/","page":"Message Passing","title":"Message Passing","text":"apply_edges\naggregate_neighbors\npropagate","category":"page"},{"location":"api/messagepassing/#GraphNeuralNetworks.apply_edges","page":"Message Passing","title":"GraphNeuralNetworks.apply_edges","text":"apply_edges(f, g, xi, xj, e)\napply_edges(f, g; [xi, xj, e])\n\nReturns the message from node j to node i . In the message-passing scheme, the incoming messages  from the neighborhood of i will later be aggregated in order to update the features of node i.\n\nThe function operates on batches of edges, therefore xi, xj, and e are tensors whose last dimension is the batch size, or can be named tuples of  such tensors.\n\nArguments\n\ng: A GNNGraph.\nxi: An array or a named tuple containing arrays whose last dimension's size        is g.num_nodes. It will be appropriately materialized on the       target node of each edge (see also edge_index).\nxj: As xj, but to be materialized on edges' sources. \ne: An array or a named tuple containing arrays whose last dimension's size is g.num_edges.\nf: A function that takes as inputs the edge-materialized xi, xj, and e.      These are arrays (or named tuples of arrays) whose last dimension' size is the size of      a batch of edges. The output of f has to be an array (or a named tuple of arrays)      with the same batch size. \n\nSee also propagate and aggregate_neighbors.\n\n\n\n\n\n","category":"function"},{"location":"api/messagepassing/#GraphNeuralNetworks.aggregate_neighbors","page":"Message Passing","title":"GraphNeuralNetworks.aggregate_neighbors","text":"aggregate_neighbors(g::GNNGraph, aggr, m)\n\nGiven a graph g, edge features m, and an aggregation operator aggr (e.g +, min, max, mean), returns the new node features \n\nmathbfx_i = square_j in mathcalN(i) mathbfm_jto i\n\nNeighborhood aggregation is the second step of propagate,  where it comes after apply_edges.\n\n\n\n\n\n","category":"function"},{"location":"api/messagepassing/#GraphNeuralNetworks.propagate","page":"Message Passing","title":"GraphNeuralNetworks.propagate","text":"propagate(f, g, aggr; xi, xj, e)  ->  m̄\n\nPerforms message passing on graph g. Takes care of materializing the node features on each edge,  applying the message function, and returning an aggregated message barmathbfm  (depending on the return value of f, an array or a named tuple of  arrays with last dimension's size g.num_nodes).\n\nIt can be decomposed in two steps:\n\nm = apply_edges(f, g, xi, xj, e)\nm̄ = aggregate_neighbors(g, aggr, m)\n\nGNN layers typically call propagate in their forward pass, providing as input f a closure.  \n\nArguments\n\ng: A GNNGraph.\nxi: An array or a named tuple containing arrays whose last dimension's size        is g.num_nodes. It will be appropriately materialized on the       target node of each edge (see also edge_index).\nxj: As xj, but to be materialized on edges' sources. \ne: An array or a named tuple containing arrays whose last dimension's size is g.num_edges.\nf: A generic function that will be passed over to apply_edges.      Has to take as inputs the edge-materialized xi, xj, and e      (arrays or named tuples of arrays whose last dimension' size is the size of      a batch of edges). Its output has to be an array or a named tuple of arrays     with the same batch size.\naggr: Neighborhood aggregation operator. Use +, mean, max, or min. \n\nExamples\n\nusing GraphNeuralNetworks, Flux\n\nstruct GNNConv <: GNNLayer\n    W\n    b\n    σ\nend\n\nFlux.@functor GNNConv\n\nfunction GNNConv(ch::Pair{Int,Int}, σ=identity)\n    in, out = ch\n    W = Flux.glorot_uniform(out, in)\n    b = zeros(Float32, out)\n    GNNConv(W, b, σ)\nend\n\nfunction (l::GNNConv)(g::GNNGraph, x::AbstractMatrix)\n    message(xi, xj, e) = l.W * xj\n    m̄ = propagate(message, g, +, xj=x)\n    return l.σ.(m̄ .+ l.bias)\nend\n\nl = GNNConv(10 => 20)\nl(g, x)\n\nSee also apply_edges and aggregate_neighbors.\n\n\n\n\n\n","category":"function"},{"location":"api/messagepassing/#Built-in-message-functions","page":"Message Passing","title":"Built-in message functions","text":"","category":"section"},{"location":"api/messagepassing/","page":"Message Passing","title":"Message Passing","text":"copy_xi\ncopy_xj\nxi_dot_xj\ne_mul_xj\nw_mul_xj","category":"page"},{"location":"api/messagepassing/#GraphNeuralNetworks.copy_xi","page":"Message Passing","title":"GraphNeuralNetworks.copy_xi","text":"copy_xi(xi, xj, e) = xi\n\n\n\n\n\n","category":"function"},{"location":"api/messagepassing/#GraphNeuralNetworks.copy_xj","page":"Message Passing","title":"GraphNeuralNetworks.copy_xj","text":"copy_xj(xi, xj, e) = xj\n\n\n\n\n\n","category":"function"},{"location":"api/messagepassing/#GraphNeuralNetworks.xi_dot_xj","page":"Message Passing","title":"GraphNeuralNetworks.xi_dot_xj","text":"xi_dot_xj(xi, xj, e) = sum(xi .* xj, dims=1)\n\n\n\n\n\n","category":"function"},{"location":"api/messagepassing/#GraphNeuralNetworks.e_mul_xj","page":"Message Passing","title":"GraphNeuralNetworks.e_mul_xj","text":"e_mul_xj(xi, xj, e) = reshape(e, (...)) .* xj\n\nReshape e into broadcast compatible shape with xj (by prepending singleton dimensions) then perform broadcasted multiplication.\n\n\n\n\n\n","category":"function"},{"location":"api/messagepassing/#GraphNeuralNetworks.w_mul_xj","page":"Message Passing","title":"GraphNeuralNetworks.w_mul_xj","text":"w_mul_xj(xi, xj, w) = reshape(w, (...)) .* xj\n\nSimilar to e_mul_xj but specialized on scalar edge feautures (weights).\n\n\n\n\n\n","category":"function"},{"location":"#GraphNeuralNetworks","page":"Home","title":"GraphNeuralNetworks","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is the documentation page for GraphNeuralNetworks.jl, a graph neural network library written in Julia and based on the deep learning framework Flux.jl. GraphNeuralNetworks.jl is largely inspired by PyTorch Geometric,Deep Graph Library, and GeometricFlux.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Among its features:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Implements common graph convolutional layers.\nSupports computations on batched graphs. \nEasy to define custom layers.\nCUDA support.\nIntegration with Graphs.jl.\nExamples of node, edge, and graph level machine learning tasks. ","category":"page"},{"location":"#Package-overview","page":"Home","title":"Package overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Let's give a brief overview of the package by solving a   graph regression problem with synthetic data. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Usage examples on real datasets can be found in the examples folder. ","category":"page"},{"location":"#Data-preparation","page":"Home","title":"Data preparation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"First, we create our dataset consisting in multiple random graphs and associated data features.  Then we batch the graphs together into a unique graph.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using GraphNeuralNetworks, Graphs, Flux, CUDA, Statistics\n\njulia> all_graphs = GNNGraph[];\n\njulia> for _ in 1:1000\n           g = GNNGraph(random_regular_graph(10, 4),  \n                       ndata=(; x = randn(Float32, 16,10)),  # input node features\n                       gdata=(; y = randn(Float32)))         # regression target   \n           push!(all_graphs, g)\n       end\n\njulia> gbatch = Flux.batch(all_graphs)\nGNNGraph:\n    num_nodes = 10000\n    num_edges = 40000\n    num_graphs = 1000\n    ndata:\n        x => (16, 10000)\n    gdata:\n        y => (1000,)","category":"page"},{"location":"#Model-building","page":"Home","title":"Model building","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We concisely define our model as a GNNChain containing 2 graph convolutional  layers. If CUDA is available, our model will live on the gpu.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> device = CUDA.functional() ? Flux.gpu : Flux.cpu;\n\njulia> model = GNNChain(GCNConv(16 => 64),\n                        BatchNorm(64),     # Apply batch normalization on node features (nodes dimension is batch dimension)\n                        x -> relu.(x),     \n                        GCNConv(64 => 64, relu),\n                        GlobalPool(mean),  # aggregate node-wise features into graph-wise features\n                        Dense(64, 1)) |> device;\n\njulia> ps = Flux.params(model);\n\njulia> opt = ADAM(1f-4);","category":"page"},{"location":"#Training","page":"Home","title":"Training","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Finally, we use a standard Flux training pipeline to fit our dataset. Flux's DataLoader iterates over mini-batches of graphs  (batched together into a GNNGraph object). ","category":"page"},{"location":"","page":"Home","title":"Home","text":"gtrain = getgraph(gbatch, 1:800)\ngtest = getgraph(gbatch, 801:gbatch.num_graphs)\ntrain_loader = Flux.Data.DataLoader(gtrain, batchsize=32, shuffle=true)\ntest_loader = Flux.Data.DataLoader(gtest, batchsize=32, shuffle=false)\n\nloss(g::GNNGraph) = mean((vec(model(g, g.ndata.x)) - g.gdata.y).^2)\n\nloss(loader) = mean(loss(g |> device) for g in loader)\n\nfor epoch in 1:100\n    for g in train_loader\n        g = g |> device\n        grad = gradient(() -> loss(g), ps)\n        Flux.Optimise.update!(opt, ps, grad)\n    end\n\n    @info (; epoch, train_loss=loss(train_loader), test_loss=loss(test_loader))\nend","category":"page"}]
}
