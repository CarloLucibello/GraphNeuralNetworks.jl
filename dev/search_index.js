var documenterSearchIndex = {"docs":
[{"location":"api/conv/#Convolutional-Layers","page":"Convolutional Layers","title":"Convolutional Layers","text":"","category":"section"},{"location":"api/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Order = [:type, :function]\nPages   = [\"api/conv.md\"]","category":"page"},{"location":"api/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Modules = [GraphNeuralNetworks]\nPages   = [\"layers/conv.jl\"]\nPrivate = false","category":"page"},{"location":"api/conv/#GraphNeuralNetworks.ChebConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.ChebConv","text":"ChebConv(in=>out, k; bias=true, init=glorot_uniform)\n\nChebyshev spectral graph convolutional layer from paper Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering.\n\nImplements\n\nX = sum^K-1_k=0  W^(k) Z^(k)\n\nwhere Z^(k) is the k-th term of Chebyshev polynomials, and can be calculated by the following recursive form:\n\nZ^(0) = X \nZ^(1) = hatL X \nZ^(k) = 2 hatL Z^(k-1) - Z^(k-2)\n\nwith hatL the scaled_laplacian.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nk: The order of Chebyshev polynomial.\nbias: Add learnable bias.\ninit: Weights' initializer.\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.EdgeConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.EdgeConv","text":"EdgeConv(f; aggr=max)\n\nEdge convolutional layer from paper Dynamic Graph CNN for Learning on Point Clouds.\n\nPerforms the operation\n\nmathbfx_i = box_j in N(i) f(mathbfx_i  mathbfx_j - mathbfx_i)\n\nwhere f typically denotes a learnable function, e.g. a linear layer or a multi-layer perceptron.\n\nArguments\n\nf: A (possibly learnable) function acting on edge features. \naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GATConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.GATConv","text":"GATConv(in => out;\n        heads=1,\n        concat=true,\n        init=glorot_uniform    \n        bias=true, \n        negative_slope=0.2)\n\nGraph attentional layer from the paper Graph Attention Networks.\n\nImplements the operation\n\nmathbfx_i = sum_j in N(i) alpha_ij W mathbfx_j\n\nwhere the attention coefficient alpha_ij is given by\n\nalpha_ij = frac1z_i exp(LeakyReLU(mathbfa^T W mathbfx_i  W mathbfx_j))\n\nwith z_i a normalization factor.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nbias::Bool: Keyword argument, whether to learn the additive bias.\nheads: Number attention heads \nconcat: Concatenate layer output or not. If not, layer output is averaged over the heads.\nnegative_slope::Real: Keyword argument, the parameter of LeakyReLU.\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GCNConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.GCNConv","text":"GCNConv(in => out, σ=identity; bias=true, init=glorot_uniform)\n\nGraph convolutional layer from paper Semi-supervised Classification with Graph Convolutional Networks.\n\nPerforms the operation\n\nmathbfx_i = sum_jin N(i) frac1c_ij W mathbfx_j\n\nwhere c_ij = sqrtN(i)N(j).\n\nThe input to the layer is a node feature array X  of size (num_features, num_nodes).\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nσ: Activation function.\nbias: Add learnable bias.\ninit: Weights' initializer.\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GINConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.GINConv","text":"GINConv(f; eps = 0f0)\n\nGraph Isomorphism convolutional layer from paper How Powerful are Graph Neural Networks?\n\nmathbfx_i = fleft((1 + epsilon) mathbfx_i + sum_j in N(i) mathbfx_j right)\n\nwhere f typically denotes a learnable function, e.g. a linear layer or a multi-layer perceptron.\n\nArguments\n\nf: A (possibly learnable) function acting on node features. \neps: Weighting factor.\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GatedGraphConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.GatedGraphConv","text":"GatedGraphConv(out, num_layers; aggr=+, init=glorot_uniform)\n\nGated graph convolution layer from Gated Graph Sequence Neural Networks.\n\nImplements the recursion\n\nmathbfh^(0)_i = mathbfx_i  mathbf0 \nmathbfh^(l)_i = GRU(mathbfh^(l-1)_i box_j in N(i) W mathbfh^(l-1)_j)\n\nwhere mathbfh^(l)_i denotes the l-th hidden variables passing through GRU. The dimension of input mathbfx_i needs to be less or equal to out.\n\nArguments\n\nout: The dimension of output features.\nnum_layers: The number of gated recurrent unit.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\ninit: Weight initialization function.\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GraphConv","page":"Convolutional Layers","title":"GraphNeuralNetworks.GraphConv","text":"GraphConv(in => out, σ=identity, aggr=+; bias=true, init=glorot_uniform)\n\nGraph convolution layer from Reference: Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks.\n\nPerforms:\n\nmathbfx_i = W^1 mathbfx_i + box_j in mathcalN(i) W^2 mathbfx_j)\n\nwhere the aggregation type is selected by aggr.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nσ: Activation function.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\nbias: Add learnable bias.\ninit: Weights' initializer.\n\n\n\n\n\n","category":"type"},{"location":"dev/#Developer-Notes","page":"Developer Notes","title":"Developer Notes","text":"","category":"section"},{"location":"dev/#Benchmarking","page":"Developer Notes","title":"Benchmarking","text":"","category":"section"},{"location":"dev/","page":"Developer Notes","title":"Developer Notes","text":"You can benchmark the effect on performance of your commits using the script perf/perf.jl.","category":"page"},{"location":"dev/","page":"Developer Notes","title":"Developer Notes","text":"First, checkout and benchmark the master branch:","category":"page"},{"location":"dev/","page":"Developer Notes","title":"Developer Notes","text":"julia> include(\"perf.jl\")\n\njulia> df = run_benchmarks()\n\n# observe results\njulia> for g in groupby(df, :layer); println(g, \"\\n\"); end\n\njulia> @save \"perf_master_20210803_mymachine.jld2\" dfmaster=df","category":"page"},{"location":"dev/","page":"Developer Notes","title":"Developer Notes","text":"Now checkout your branch and do the same:","category":"page"},{"location":"dev/","page":"Developer Notes","title":"Developer Notes","text":"julia> df = run_benchmarks()\n\njulia> @save \"perf_pr_20210803_mymachine.jld2\" dfpr=df","category":"page"},{"location":"dev/","page":"Developer Notes","title":"Developer Notes","text":"Finally, compare the results:","category":"page"},{"location":"dev/","page":"Developer Notes","title":"Developer Notes","text":"julia> @load \"perf_master_20210803_mymachine.jld2\"\n\njulia> @load \"perf_pr_20210803_mymachine.jld2\"\n\njulia> compare(dfpr, dfmaster)","category":"page"},{"location":"graphs/#Graphs","page":"Graphs","title":"Graphs","text":"","category":"section"},{"location":"models/#Models","page":"Building models","title":"Models","text":"","category":"section"},{"location":"api/graphs/#Graphs","page":"Graphs","title":"Graphs","text":"","category":"section"},{"location":"api/graphs/","page":"Graphs","title":"Graphs","text":"Order = [:type, :function]\nPages   = [\"api/graphs.md\"]","category":"page"},{"location":"api/graphs/","page":"Graphs","title":"Graphs","text":"Modules = [GraphNeuralNetworks]\nPages   = [\"featuredgraph.jl\"]\nPrivate = false","category":"page"},{"location":"api/graphs/#GraphNeuralNetworks.FeaturedGraph","page":"Graphs","title":"GraphNeuralNetworks.FeaturedGraph","text":"FeaturedGraph(g; [graph_type, dir, num_nodes, nf, ef, gf])\nFeaturedGraph(fg::FeaturedGraph; [nf, ef, gf])\n\nA type representing a graph structure and storing also arrays  that contain features associated to nodes, edges, and the whole graph. \n\nA FeaturedGraph can be constructed out of different objects g representing the connections inside the graph, while the internal representation type is governed by graph_type.  When constructed from another featured graph fg, the internal graph representation is preserved and shared. \n\nA FeaturedGraph is a LightGraphs' AbstractGraph, therefore any functionality from the LightGraphs' graph library can be used on it.\n\nArguments\n\ng: Some data representing the graph topology. Possible type are \nAn adjacency matrix\nAn adjacency list.\nA tuple containing the source and target vectors (COO representation)\nA LightGraphs' graph.\ngraph_type: A keyword argument that specifies                the underlying representation used by the FeaturedGraph.                Currently supported values are \n:coo. Graph represented as a tuple (source, target), such that the k-th edge          connects the node source[k] to node target[k].         Optionally, also edge weights can be given: (source, target, weights).\n:sparse. A sparse adjacency matrix representation.\n:dense. A dense adjacency matrix representation.  \nDefault :coo.\ndir. The assumed edge direction when given adjacency matrix or adjacency list input data g.        Possible values are :out and :in. Defaul :out.\nnum_nodes. The number of nodes. If not specified, inferred from g. Default nothing.\nnf: Node features. Either nothing, or an array whose last dimension has size num_nodes. Default nothing.\nef: Edge features. Either nothing, or an array whose last dimension has size num_edges. Default nothing.\ngf: Global features. Default nothing. \n\nUsage.\n\nusing Flux, GraphNeuralNetworks\n\n# Construct from adjacency list representation\ng = [[2,3], [1,4,5], [1], [2,5], [2,4]]\nfg = FeaturedGraph(g)\n\n# Number of nodes and edges\nfg.num_nodes  # 5\nfg.num_edges  # 10 \n\n# Same graph in COO representation\ns = [1,1,2,2,2,3,4,4,5,5]\nt = [2,3,1,4,5,3,2,5,2,4]\nfg = FeaturedGraph((s, t))\nfg = FeaturedGraph(s, t) # other convenience constructor\n\n# From a LightGraphs' graph\nfg = FeaturedGraph(erdos_renyi(100, 20))\n\n# Copy featured graph while also adding node features\nfg = FeaturedGraph(fg, nf=rand(100, 5))\n\n# Send to gpu\nfg = fg |> gpu\n\n# Collect edges' source and target nodes.\n# Both source and target are vectors of length num_edges\nsource, target = edge_index(fg)\n\nSee also graph, edge_index, node_feature, edge_feature, and global_feature \n\n\n\n\n\n","category":"type"},{"location":"api/graphs/#GraphNeuralNetworks.add_self_loops-Tuple{FeaturedGraph{var\"#s10\"} where var\"#s10\"<:(Tuple{T, T, V} where {T<:(AbstractVector{T} where T), V})}","page":"Graphs","title":"GraphNeuralNetworks.add_self_loops","text":"add_self_loops(fg::FeaturedGraph)\n\nReturn a featured graph with the same features as fg but also adding edges connecting the nodes to themselves.\n\nNodes with already existing self-loops will obtain a second self-loop.\n\n\n\n\n\n","category":"method"},{"location":"api/graphs/#GraphNeuralNetworks.edge_feature-Tuple{FeaturedGraph}","page":"Graphs","title":"GraphNeuralNetworks.edge_feature","text":"edge_feature(fg::FeaturedGraph)\n\nReturn the edge features of fg.\n\n\n\n\n\n","category":"method"},{"location":"api/graphs/#GraphNeuralNetworks.edge_index-Tuple{FeaturedGraph{var\"#s11\"} where var\"#s11\"<:(Tuple{T, T, V} where {T<:(AbstractVector{T} where T), V})}","page":"Graphs","title":"GraphNeuralNetworks.edge_index","text":"edge_index(fg::FeaturedGraph)\n\nReturn a tuple containing two vectors, respectively storing  the source and target nodes for each edges in fg.\n\ns, t = edge_index(fg)\n\n\n\n\n\n","category":"method"},{"location":"api/graphs/#GraphNeuralNetworks.global_feature-Tuple{FeaturedGraph}","page":"Graphs","title":"GraphNeuralNetworks.global_feature","text":"global_feature(fg::FeaturedGraph)\n\nReturn the global features of fg.\n\n\n\n\n\n","category":"method"},{"location":"api/graphs/#GraphNeuralNetworks.node_feature-Tuple{FeaturedGraph}","page":"Graphs","title":"GraphNeuralNetworks.node_feature","text":"node_feature(fg::FeaturedGraph)\n\nReturn the node features of fg.\n\n\n\n\n\n","category":"method"},{"location":"api/graphs/#GraphNeuralNetworks.normalized_laplacian","page":"Graphs","title":"GraphNeuralNetworks.normalized_laplacian","text":"normalized_laplacian(fg, T=Float32; add_self_loops=false, dir=:out)\n\nNormalized Laplacian matrix of graph g.\n\nArguments\n\nfg: A FeaturedGraph.\nT: result element type.\nadd_self_loops: add self-loops while calculating the matrix.\ndir: the edge directionality considered (:out, :in, :both).\n\n\n\n\n\n","category":"function"},{"location":"api/graphs/#GraphNeuralNetworks.scaled_laplacian","page":"Graphs","title":"GraphNeuralNetworks.scaled_laplacian","text":"scaled_laplacian(fg, T=Float32; dir=:out)\n\nScaled Laplacian matrix of graph g, defined as hatL = frac2lambda_max L - I where L is the normalized Laplacian matrix.\n\nArguments\n\nfg: A FeaturedGraph.\nT: result element type.\ndir: the edge directionality considered (:out, :in, :both).\n\n\n\n\n\n","category":"function"},{"location":"api/pool/#Pooling-Layers","page":"Pooling Layers","title":"Pooling Layers","text":"","category":"section"},{"location":"api/pool/","page":"Pooling Layers","title":"Pooling Layers","text":"Order = [:type, :function]\nPages   = [\"api/pool.md\"]","category":"page"},{"location":"api/pool/","page":"Pooling Layers","title":"Pooling Layers","text":"Modules = [GraphNeuralNetworks]\nPages   = [\"layers/pool.jl\"]\nPrivate = false","category":"page"},{"location":"api/pool/#GraphNeuralNetworks.GlobalPool","page":"Pooling Layers","title":"GraphNeuralNetworks.GlobalPool","text":"GlobalPool(aggr, dim...)\n\nGlobal pooling layer.\n\nIt pools all features with aggr operation.\n\nArguments\n\naggr: An aggregate function applied to pool all features.\n\n\n\n\n\n","category":"type"},{"location":"api/pool/#GraphNeuralNetworks.LocalPool","page":"Pooling Layers","title":"GraphNeuralNetworks.LocalPool","text":"LocalPool(aggr, cluster)\n\nLocal pooling layer.\n\nIt pools features with aggr operation accroding to cluster. It is implemented with scatter operation.\n\nArguments\n\naggr: An aggregate function applied to pool all features.\ncluster: An index structure which indicates what features to aggregate with.\n\n\n\n\n\n","category":"type"},{"location":"api/pool/#GraphNeuralNetworks.TopKPool","page":"Pooling Layers","title":"GraphNeuralNetworks.TopKPool","text":"TopKPool(adj, k, in_channel)\n\nTop-k pooling layer.\n\nArguments\n\nadj: Adjacency matrix  of a graph.\nk: Top-k nodes are selected to pool together.\nin_channel: The dimension of input channel.\n\n\n\n\n\n","category":"type"},{"location":"messagepassing/#Message-Passing","page":"Message passing","title":"Message Passing","text":"","category":"section"},{"location":"#GraphNeuralNetworks","page":"Home","title":"GraphNeuralNetworks","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for GraphNeuralNetworks.","category":"page"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"}]
}
