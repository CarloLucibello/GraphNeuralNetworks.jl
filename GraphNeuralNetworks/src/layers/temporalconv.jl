function scan(cell, g::GNNGraph, x::AbstractArray{T,3}, state) where {T}
    y = []
    for x_t in eachslice(x, dims = 2)
        yt, state = cell(g, x_t, state)
        y = vcat(y, [yt])
    end
    return stack(y, dims = 2)
end


"""
    GNNRecurrence(cell)

Construct a recurrent layer that applies the `cell`
to process an entire temporal sequence of node features at once.

# Forward 

    layer(g::GNNGraph, x, [state])

- `g`: The input graph.
- `x`: The time-varying node features. An array of size `in x timesteps x num_nodes`.
- `state`: The initial state of the cell. 
   If not provided, it is generated by calling `Flux.initialstates(cell)`.

Applies the recurrent cell to each timestep of the input sequence and returns the output as
an array of size `out_features x timesteps x num_nodes`.

# Examples

```jldoctest
julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);

julia> x = rand(Float32, d_in, timesteps, num_nodes);

julia> cell = GConvLSTMCell(d_in => d_out, 2)
GConvLSTMCell(2 => 3, 2)  # 168 parameters

julia> layer = GNNRecurrence(cell)
GNNRecurrence(
  GConvLSTMCell(2 => 3, 2),             # 168 parameters
)                   # Total: 24 arrays, 168 parameters, 2.023 KiB.

julia> y = layer(g, x);

julia> size(y) # (d_out, timesteps, num_nodes)
(3, 5, 5)
```
""" 
struct GNNRecurrence{G} <: GNNLayer
    cell::G
end

Flux.@layer GNNRecurrence

Flux.initialstates(rnn::GNNRecurrence) = Flux.initialstates(rnn.cell)

function (rnn::GNNRecurrence)(g::GNNGraph, x::AbstractArray{T,3}) where {T}
    return rnn(g, x, initialstates(rnn))
end

function (rnn::GNNRecurrence)(g::GNNGraph, x::AbstractArray{T,3}, state) where {T}
    return scan(rnn.cell, g, x, state)
end

function Base.show(io::IO, rnn::GNNRecurrence)
    print(io, "GNNRecurrence($(rnn.cell))")
end


"""
    GConvGRUCell(in => out, k; [bias, init])

Graph Convolutional Gated Recurrent Unit (GConvGRU) recurrent cell from the paper 
[Structured Sequence Modeling with Graph Convolutional Recurrent Networks](https://arxiv.org/abs/1612.07659).

Uses [`ChebConv`](@ref) to model spatial dependencies, 
followed by a Gated Recurrent Unit (GRU) cell to model temporal dependencies.

# Arguments

- `in => out`: A pair  where `in` is the number of input node features and `out` 
  is the number of output node features.
- `k`: Chebyshev polynomial order.
- `bias`: Add learnable bias. Default `true`.
- `init`: Weights' initializer. Default `glorot_uniform`.

# Forward 

    cell(g::GNNGraph, x, [h])

- `g`: The input graph.
- `x`: The node features. It should be a matrix of size `in x num_nodes`.
- `h`: The initial hidden state of the GRU cell. If given, it is a matrix of size `out x num_nodes`.
       If not provided, it is assumed to be a matrix of zeros.

Performs one recurrence step and returns a tuple `(h, h)`, 
where `h` is the updated hidden state of the GRU cell.

# Examples

```jldoctest
julia> using GraphNeuralNetworks, Flux

julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);

julia> x = [rand(Float32, d_in, num_nodes) for t in 1:timesteps];

julia> cell = GConvGRUCell(d_in => d_out, 2);

julia> state = Flux.initialstates(cell);

julia> y = state;

julia> for xt in x
           y, state = cell(g, xt, state)
       end

julia> size(y) # (d_out, num_nodes)
(3, 5)
```
"""
@concrete struct GConvGRUCell <: GNNLayer
    conv_x_r
    conv_h_r
    conv_x_z
    conv_h_z
    conv_x_h
    conv_h_h
    k::Int
    in::Int
    out::Int
end

Flux.@layer :noexpand GConvGRUCell

function GConvGRUCell(ch::Pair{Int, Int}, k::Int;
                   bias::Bool = true,
                   init = Flux.glorot_uniform,
                   )
    in, out = ch
    # reset gate
    conv_x_r = ChebConv(in => out, k; bias, init)
    conv_h_r = ChebConv(out => out, k; bias, init)
    # update gate
    conv_x_z = ChebConv(in => out, k; bias, init)
    conv_h_z = ChebConv(out => out, k; bias, init)
    # new gate
    conv_x_h = ChebConv(in => out, k; bias, init)
    conv_h_h = ChebConv(out => out, k; bias, init)
    return GConvGRUCell(conv_x_r, conv_h_r, conv_x_z, conv_h_z, conv_x_h, conv_h_h, k, in, out)
end

function Flux.initialstates(cell::GConvGRUCell)
    zeros_like(cell.conv_x_r.weight, cell.out)
end

(cell::GConvGRUCell)(g::GNNGraph, x::AbstractMatrix) = cell(g, x, initialstates(cell))

function (cell::GConvGRUCell)(g::GNNGraph, x::AbstractMatrix, h::AbstractVector)
    h = repeat(h, 1, g.num_nodes)
    return cell(g, x, h)
end

function (cell::GConvGRUCell)(g::GNNGraph, x::AbstractMatrix, h::AbstractMatrix)
    # reset gate
    r = cell.conv_x_r(g, x) .+ cell.conv_h_r(g, h)
    r = Flux.sigmoid_fast(r)
    # update gate
    z = cell.conv_x_z(g, x) .+ cell.conv_h_z(g, h)
    z = Flux.sigmoid_fast(z)
    # new gate
    h̃ = cell.conv_x_h(g, x) .+ cell.conv_h_h(g, r .* h)
    h̃ = Flux.tanh_fast(h̃)
    h = (1 .- z) .* h̃ .+ z .* h 
    return h, h
end

function Base.show(io::IO, cell::GConvGRUCell)
    print(io, "GConvGRUCell($(cell.in) => $(cell.out), $(cell.k))")
end

"""
    GConvGRU(args...; kws...)

Construct a recurrent layer corresponding to the [`GConvGRUCell`](@ref) cell.
It can be used to process an entire temporal sequence of node features at once.

The arguments are passed to the [`GConvGRUCell`](@ref) constructor.
See [`GNNRecurrence`](@ref) for more details.

# Examples

```jldoctest
julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);

julia> x = rand(Float32, d_in, timesteps, num_nodes);

julia> layer = GConvGRU(d_in => d_out, 2)
GConvGRU(
  GConvGRUCell(2 => 3, 2),              # 108 parameters
)                   # Total: 12 arrays, 108 parameters, 1.148 KiB.

julia> y = layer(g, x);

julia> size(y) # (d_out, timesteps, num_nodes)
(3, 5, 5)
```
""" 
GConvGRU(args...; kws...) = GNNRecurrence(GConvGRUCell(args...; kws...))


"""
    GConvLSTMCell(in => out, k; [bias, init])

Graph Convolutional LSTM recurrent cell from the paper 
[Structured Sequence Modeling with Graph Convolutional Recurrent Networks](https://arxiv.org/abs/1612.07659).

Uses [`ChebConv`](@ref) to model spatial dependencies, 
followed by a Long Short-Term Memory (LSTM) cell to model temporal dependencies.

# Arguments

- `in => out`: A pair  where `in` is the number of input node features and `out` 
  is the number of output node features.
- `k`: Chebyshev polynomial order.
- `bias`: Add learnable bias. Default `true`.
- `init`: Weights' initializer. Default `glorot_uniform`.

# Forward 

    cell(g::GNNGraph, x, [state])

- `g`: The input graph.
- `x`: The node features. It should be a matrix of size `in x num_nodes`.
- `state`: The initial hidden state of the LSTM cell.  
       If given, it is a tuple `(h, c)` where both `h` and `c` are arrays of size `out x num_nodes`.
       If not provided, the initial hidden state is assumed to be a tuple of matrices of zeros.

Performs one recurrence step and returns a tuple `(output, state)`, 
where `output` is the updated hidden state `h` of the LSTM cell and `state` is the updated tuple `(h, c)`.

# Examples

```jldoctest
julia> using GraphNeuralNetworks, Flux

julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);

julia> x = [rand(Float32, d_in, num_nodes) for t in 1:timesteps];

julia> cell = GConvLSTMCell(d_in => d_out, 2);

julia> state = Flux.initialstates(cell);

julia> y = state[1];

julia> for xt in x
           y, state = cell(g, xt, state)
       end

julia> size(y) # (d_out, num_nodes)
(3, 5)
```
"""
@concrete struct GConvLSTMCell <: GNNLayer
    conv_x_i
    conv_h_i
    w_i
    b_i
    conv_x_f
    conv_h_f
    w_f
    b_f
    conv_x_c
    conv_h_c
    w_c
    b_c
    conv_x_o
    conv_h_o
    w_o
    b_o
    k::Int
    in::Int
    out::Int
end

Flux.@layer :noexpand GConvLSTMCell

function GConvLSTMCell(ch::Pair{Int, Int}, k::Int;
                        bias::Bool = true,
                        init = Flux.glorot_uniform)
    in, out = ch
    # input gate
    conv_x_i = ChebConv(in => out, k; bias, init)
    conv_h_i = ChebConv(out => out, k; bias, init)
    w_i = init(out, 1)
    b_i = bias ? Flux.create_bias(w_i, true, out) : false
    # forget gate
    conv_x_f = ChebConv(in => out, k; bias, init)
    conv_h_f = ChebConv(out => out, k; bias, init)
    w_f = init(out, 1)
    b_f = bias ? Flux.create_bias(w_f, true, out) : false
    # cell state
    conv_x_c = ChebConv(in => out, k; bias, init)
    conv_h_c = ChebConv(out => out, k; bias, init)
    w_c = init(out, 1)
    b_c = bias ? Flux.create_bias(w_c, true, out) : false
    # output gate
    conv_x_o = ChebConv(in => out, k; bias, init)
    conv_h_o = ChebConv(out => out, k; bias, init)
    w_o = init(out, 1)
    b_o = bias ? Flux.create_bias(w_o, true, out) : false
    return GConvLSTMCell(conv_x_i, conv_h_i, w_i, b_i,
                         conv_x_f, conv_h_f, w_f, b_f,
                         conv_x_c, conv_h_c, w_c, b_c,
                         conv_x_o, conv_h_o, w_o, b_o,
                         k, in, out)
end

function Flux.initialstates(cell::GConvLSTMCell)
    (zeros_like(cell.conv_x_i.weight, cell.out), zeros_like(cell.conv_x_i.weight, cell.out))
end

(cell::GConvLSTMCell)(g::GNNGraph, x::AbstractMatrix) = cell(g, x, initialstates(cell))

function (cell::GConvLSTMCell)(g::GNNGraph, x::AbstractMatrix, (h, c))
    if h isa AbstractVector
        h = repeat(h, 1, g.num_nodes)
    end
    if c isa AbstractVector
        c = repeat(c, 1, g.num_nodes)
    end
    @assert ndims(h) == 2 && ndims(c) == 2
    # input gate
    i = cell.conv_x_i(g, x) .+ cell.conv_h_i(g, h) .+ cell.w_i .* c .+ cell.b_i 
    i = Flux.sigmoid_fast(i)
    # forget gate
    f = cell.conv_x_f(g, x) .+ cell.conv_h_f(g, h) .+ cell.w_f .* c .+ cell.b_f
    f = Flux.sigmoid_fast(f)
    # cell state
    c = f .* c .+ i .* Flux.tanh_fast(cell.conv_x_c(g, x) .+ cell.conv_h_c(g, h) .+ cell.w_c .* c .+ cell.b_c)
    # output gate
    o = cell.conv_x_o(g, x) .+ cell.conv_h_o(g, h) .+ cell.w_o .* c .+ cell.b_o
    o = Flux.sigmoid_fast(o)
    h =  o .* Flux.tanh_fast(c)
    return h, (h, c)
end

function Base.show(io::IO, cell::GConvLSTMCell)
    print(io, "GConvLSTMCell($(cell.in) => $(cell.out), $(cell.k))")
end


"""
    GConvLSTM(args...; kws...)

Construct a recurrent layer corresponding to the [`GConvLSTMCell`](@ref) cell.
It can be used to process an entire temporal sequence of node features at once.

The arguments are passed to the [`GConvLSTMCell`](@ref) constructor.
See [`GNNRecurrence`](@ref) for more details.

# Examples

```jldoctest
julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);

julia> x = rand(Float32, d_in, timesteps, num_nodes);

julia> layer = GConvLSTM(d_in => d_out, 2)
GNNRecurrence(
  GConvLSTMCell(2 => 3, 2),             # 168 parameters
)                   # Total: 24 arrays, 168 parameters, 2.023 KiB.

julia> y = layer(g, x);

julia> size(y) # (d_out, timesteps, num_nodes)
(3, 5, 5)
```
""" 
GConvLSTM(args...; kws...) = GNNRecurrence(GConvLSTMCell(args...; kws...))

"""
    DCGRUCell(in => out, k; [bias, init])

Diffusion Convolutional Recurrent Neural Network (DCGRU) cell from the paper 
[Diffusion Convolutional Recurrent Neural Network: Data-driven Traffic Forecasting](https://arxiv.org/abs/1707.01926).

Applyis a [`DConv`](@ref) layer to model spatial dependencies, 
in combination with a Gated Recurrent Unit (GRU) cell to model temporal dependencies.

# Arguments

- `in`: Number of input node features.
- `out`: Number of output node features.
- `k`: Diffusion step for the `DConv`.
- `bias`: Add learnable bias. Default `true`.
- `init`: Weights' initializer. Default `glorot_uniform`.

# Forward 

    cell(g::GNNGraph, x, [h])

- `g`: The input graph.
- `x`: The node features. It should be a matrix of size `in x num_nodes`.
- `h`: The initial hidden state of the GRU cell. If given, it is a matrix of size `out x num_nodes`.
       If not provided, it is assumed to be a matrix of zeros.

Performs one recurrence step and returns a tuple `(h, h)`,
where `h` is the updated hidden state of the GRU cell.

# Examples

```jldoctest
julia> using GraphNeuralNetworks, Flux

julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);

julia> x = [rand(Float32, d_in, num_nodes) for t in 1:timesteps];

julia> cell = DCGRUCell(d_in => d_out, 2);

julia> state = Flux.initialstates(cell);

julia> y = state;

julia> for xt in x
           y, state = cell(g, xt, state)
       end

julia> size(y) # (d_out, num_nodes)
(3, 5)
```
"""
struct DCGRUCell
    in::Int
    out::Int
    k::Int
    dconv_u::DConv
    dconv_r::DConv
    dconv_c::DConv
end

Flux.@layer :noexpand DCGRUCell

function DCGRUCell(ch::Pair{Int,Int}, k::Int; bias = true, init = glorot_uniform)
    in, out = ch
    dconv_u = DConv((in + out) => out, k; bias, init)
    dconv_r = DConv((in + out) => out, k; bias, init)
    dconv_c = DConv((in + out) => out, k; bias, init)
    return DCGRUCell(in, out, k, dconv_u, dconv_r, dconv_c)
end

Flux.initialstates(cell::DCGRUCell) = zeros_like(cell.dconv_u.weights, cell.out)

(cell::DCGRUCell)(g::GNNGraph, x::AbstractMatrix) = cell(g, x, initialstates(cell))

function (cell::DCGRUCell)(g::GNNGraph, x::AbstractMatrix, h::AbstractVector) 
    return cell(g, x, repeat(h, 1, g.num_nodes))
end

function (cell::DCGRUCell)(g::GNNGraph, x::AbstractMatrix, h::AbstractMatrix)
    h̃ = vcat(x, h)
    z = cell.dconv_u(g, h̃)
    z = NNlib.sigmoid_fast.(z)
    r = cell.dconv_r(g, h̃)
    r = NNlib.sigmoid_fast.(r)
    ĥ = vcat(x, h .* r)
    c = cell.dconv_c(g, ĥ)
    c = NNlib.tanh_fast.(c)
    h = z.* h + (1 .- z) .* c
    return h, h
end

function Base.show(io::IO, cell::DCGRUCell)
    print(io, "DCGRUCell($(cell.in) => $(cell.out), $(cell.k))")
end

"""
    DCGRU(args...; kws...)

Construct a recurrent layer corresponding to the [`DCGRUCell`](@ref) cell.
It can be used to process an entire temporal sequence of node features at once.

The arguments are passed to the [`DCGRUCell`](@ref) constructor.
See [`GNNRecurrence`](@ref) for more details.

# Examples
```jldoctest
julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);

julia> x = rand(Float32, d_in, timesteps, num_nodes);

julia> layer = DCGRU(d_in => d_out, 2)
GNNRecurrence(
  DCGRUCell(2 => 3, 2),                 # 189 parameters
)                   # Total: 6 arrays, 189 parameters, 1.184 KiB.

julia> y = layer(g, x);

julia> size(y) # (d_out, timesteps, num_nodes)
(3, 5, 5)
```
"""
DCGRU(args...; kws...) = GNNRecurrence(DCGRUCell(args...; kws...))


