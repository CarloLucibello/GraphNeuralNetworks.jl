function scan(cell, g::GNNGraph, x::AbstractArray{T,3}, state) where {T}
    y = []
    for xt in eachslice(x, dims = 2)
        yt, state = cell(g, xt, state)
        y = vcat(y, [yt])
    end
    return stack(y, dims = 2)
end

function scan(cell, tg::TemporalSnapshotsGNNGraph, x::AbstractVector, state)
    # @assert length(x) == length(tg.snapshots)
    y = []
    for (t, xt) in enumerate(x)
        gt = tg.snapshots[t]
        yt, state = cell(gt, xt, state)
        y = vcat(y, [yt])
    end
    return y
end


"""
    GNNRecurrence(cell)

Construct a recurrent layer that applies the graph recurrent `cell` forward
multiple times to process an entire temporal sequence of node features at once.

The `cell` has to satisfy the following interface for the forward pass: 
`yt, state = cell(g, xt, state)`, where `xt` is the input node features,
`yt` is the updated node features, `state` is the cell state to be updated.

# Forward 

    layer(g, x, [state])

Applies the recurrent cell to each timestep of the input sequence.

## Arguments

- `g`: The input `GNNGraph` or `TemporalSnapshotsGNNGraph`.
    - If `GNNGraph`, the same graph is used for all timesteps.
    - If `TemporalSnapshotsGNNGraph`, a different graph is used for each timestep. Not all cells support this.
- `x`: The time-varying node features. 
    - If `g` is `GNNGraph`, it is an array of size `in x timesteps x num_nodes`.
    - If `g` is `TemporalSnapshotsGNNGraph`, it is an vector of length `timesteps`,
      with element `t` of size `in x num_nodes_t`.
- `state`: The initial state for the cell. 
   If not provided, it is generated by calling `Flux.initialstates(cell)`.

## Return

Returns the updated node features:
- If `g` is `GNNGraph`, returns an array of size `out_features x timesteps x num_nodes`.
- If `g` is `TemporalSnapshotsGNNGraph`, returns a vector of length `timesteps`, 
  with element `t` of size `out_features x num_nodes_t`.

# Examples

The following example considers a static graph and a time-varying node features.

```jldoctest
julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);
GNNGraph:
  num_nodes: 5
  num_edges: 10

julia> x = rand(Float32, d_in, timesteps, num_nodes);

julia> cell = GConvLSTMCell(d_in => d_out, 2)
GConvLSTMCell(2 => 3, 2)  # 168 parameters

julia> layer = GNNRecurrence(cell)
GNNRecurrence(
  GConvLSTMCell(2 => 3, 2),             # 168 parameters
)                   # Total: 24 arrays, 168 parameters, 2.023 KiB.

julia> y = layer(g, x);

julia> size(y) # (d_out, timesteps, num_nodes)
(3, 5, 5)
```
Now consider a time-varying graph and time-varying node features.
```jldoctest
julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> num_nodes = [10, 10, 10, 10, 10];

julia> num_edges = [10, 12, 14, 16, 18];

julia> snapshots = [rand_graph(n, m) for (n, m) in zip(num_nodes, num_edges)];

julia> tg = TemporalSnapshotsGNNGraph(snapshots)

julia> x = [rand(Float32, d_in, n) for n in num_nodes];

julia> cell = EvolveGCNOCell(d_in => d_out)
EvolveGCNOCell(2 => 3)  # 321 parameters

julia> layer = GNNRecurrence(cell)
GNNRecurrence(
  EvolveGCNOCell(2 => 3),               # 321 parameters
)                   # Total: 5 arrays, 321 parameters, 1.535 KiB.

julia> y = layer(tg, x);

julia> length(y)    # timesteps
5

julia> size(y[end]) # (d_out, num_nodes[end])
(3, 10)
```
""" 
struct GNNRecurrence{G} <: GNNLayer
    cell::G
end

Flux.@layer GNNRecurrence

Flux.initialstates(rnn::GNNRecurrence) = Flux.initialstates(rnn.cell)

function (rnn::GNNRecurrence)(g, x)
    return rnn(g, x, initialstates(rnn))
end

function (rnn::GNNRecurrence)(g, x, state)
    return scan(rnn.cell, g, x, state)
end

function Base.show(io::IO, rnn::GNNRecurrence)
    print(io, "GNNRecurrence($(rnn.cell))")
end


"""
    GConvGRUCell(in => out, k; [bias, init])

Graph Convolutional Gated Recurrent Unit (GConvGRU) recurrent cell from the paper 
[Structured Sequence Modeling with Graph Convolutional Recurrent Networks](https://arxiv.org/abs/1612.07659).

Uses [`ChebConv`](@ref) to model spatial dependencies, 
followed by a Gated Recurrent Unit (GRU) cell to model temporal dependencies.

# Arguments

- `in => out`: A pair  where `in` is the number of input node features and `out` 
  is the number of output node features.
- `k`: Chebyshev polynomial order.
- `bias`: Add learnable bias. Default `true`.
- `init`: Weights' initializer. Default `glorot_uniform`.

# Forward 

    cell(g::GNNGraph, x, [h])

- `g`: The input graph.
- `x`: The node features. It should be a matrix of size `in x num_nodes`.
- `h`: The current hidden state of the GRU cell. If given, it is a matrix of size `out x num_nodes`.
       If not provided, it is assumed to be a matrix of zeros.

Performs one recurrence step and returns a tuple `(h, h)`, 
where `h` is the updated hidden state of the GRU cell.

# Examples

```jldoctest
julia> using GraphNeuralNetworks, Flux

julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);

julia> x = [rand(Float32, d_in, num_nodes) for t in 1:timesteps];

julia> cell = GConvGRUCell(d_in => d_out, 2);

julia> state = Flux.initialstates(cell);

julia> y = state;

julia> for xt in x
           y, state = cell(g, xt, state)
       end

julia> size(y) # (d_out, num_nodes)
(3, 5)
```
"""
@concrete struct GConvGRUCell <: GNNLayer
    conv_x_r
    conv_h_r
    conv_x_z
    conv_h_z
    conv_x_h
    conv_h_h
    k::Int
    in::Int
    out::Int
end

Flux.@layer :noexpand GConvGRUCell

function GConvGRUCell(ch::Pair{Int, Int}, k::Int;
                   bias::Bool = true,
                   init = Flux.glorot_uniform,
                   )
    in, out = ch
    # reset gate
    conv_x_r = ChebConv(in => out, k; bias, init)
    conv_h_r = ChebConv(out => out, k; bias, init)
    # update gate
    conv_x_z = ChebConv(in => out, k; bias, init)
    conv_h_z = ChebConv(out => out, k; bias, init)
    # new gate
    conv_x_h = ChebConv(in => out, k; bias, init)
    conv_h_h = ChebConv(out => out, k; bias, init)
    return GConvGRUCell(conv_x_r, conv_h_r, conv_x_z, conv_h_z, conv_x_h, conv_h_h, k, in, out)
end

function Flux.initialstates(cell::GConvGRUCell)
    zeros_like(cell.conv_x_r.weight, cell.out)
end

(cell::GConvGRUCell)(g::GNNGraph, x::AbstractMatrix) = cell(g, x, initialstates(cell))

function (cell::GConvGRUCell)(g::GNNGraph, x::AbstractMatrix, h::AbstractVector)
    h = repeat(h, 1, g.num_nodes)
    return cell(g, x, h)
end

function (cell::GConvGRUCell)(g::GNNGraph, x::AbstractMatrix, h::AbstractMatrix)
    # reset gate
    r = cell.conv_x_r(g, x) .+ cell.conv_h_r(g, h)
    r = Flux.sigmoid_fast(r)
    # update gate
    z = cell.conv_x_z(g, x) .+ cell.conv_h_z(g, h)
    z = Flux.sigmoid_fast(z)
    # new gate
    h̃ = cell.conv_x_h(g, x) .+ cell.conv_h_h(g, r .* h)
    h̃ = Flux.tanh_fast(h̃)
    h = (1 .- z) .* h̃ .+ z .* h 
    return h, h
end

function Base.show(io::IO, cell::GConvGRUCell)
    print(io, "GConvGRUCell($(cell.in) => $(cell.out), $(cell.k))")
end

"""
    GConvGRU(args...; kws...)

Construct a recurrent layer corresponding to the [`GConvGRUCell`](@ref) cell.
It can be used to process an entire temporal sequence of node features at once.

The arguments are passed to the [`GConvGRUCell`](@ref) constructor.
See [`GNNRecurrence`](@ref) for more details.

# Examples

```jldoctest
julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);

julia> x = rand(Float32, d_in, timesteps, num_nodes);

julia> layer = GConvGRU(d_in => d_out, 2)
GConvGRU(
  GConvGRUCell(2 => 3, 2),              # 108 parameters
)                   # Total: 12 arrays, 108 parameters, 1.148 KiB.

julia> y = layer(g, x);

julia> size(y) # (d_out, timesteps, num_nodes)
(3, 5, 5)
```
""" 
GConvGRU(args...; kws...) = GNNRecurrence(GConvGRUCell(args...; kws...))


"""
    GConvLSTMCell(in => out, k; [bias, init])

Graph Convolutional LSTM recurrent cell from the paper 
[Structured Sequence Modeling with Graph Convolutional Recurrent Networks](https://arxiv.org/abs/1612.07659).

Uses [`ChebConv`](@ref) to model spatial dependencies, 
followed by a Long Short-Term Memory (LSTM) cell to model temporal dependencies.

# Arguments

- `in => out`: A pair  where `in` is the number of input node features and `out` 
  is the number of output node features.
- `k`: Chebyshev polynomial order.
- `bias`: Add learnable bias. Default `true`.
- `init`: Weights' initializer. Default `glorot_uniform`.

# Forward 

    cell(g::GNNGraph, x, [state])

- `g`: The input graph.
- `x`: The node features. It should be a matrix of size `in x num_nodes`.
- `state`: The current state of the LSTM cell.  
       If given, it is a tuple `(h, c)` where both `h` and `c` are arrays of size `out x num_nodes`.
       If not provided, it is assumed to be a tuple of matrices of zeros.

Performs one recurrence step and returns a tuple `(output, state)`, 
where `output` is the updated hidden state `h` of the LSTM cell and `state` is the updated tuple `(h, c)`.

# Examples

```jldoctest
julia> using GraphNeuralNetworks, Flux

julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);

julia> x = [rand(Float32, d_in, num_nodes) for t in 1:timesteps];

julia> cell = GConvLSTMCell(d_in => d_out, 2);

julia> state = Flux.initialstates(cell);

julia> y = state[1];

julia> for xt in x
           y, state = cell(g, xt, state)
       end

julia> size(y) # (d_out, num_nodes)
(3, 5)
```
"""
@concrete struct GConvLSTMCell <: GNNLayer
    conv_x_i
    conv_h_i
    w_i
    b_i
    conv_x_f
    conv_h_f
    w_f
    b_f
    conv_x_c
    conv_h_c
    w_c
    b_c
    conv_x_o
    conv_h_o
    w_o
    b_o
    k::Int
    in::Int
    out::Int
end

Flux.@layer :noexpand GConvLSTMCell

function GConvLSTMCell(ch::Pair{Int, Int}, k::Int;
                        bias::Bool = true,
                        init = Flux.glorot_uniform)
    in, out = ch
    # input gate
    conv_x_i = ChebConv(in => out, k; bias, init)
    conv_h_i = ChebConv(out => out, k; bias, init)
    w_i = init(out, 1)
    b_i = bias ? Flux.create_bias(w_i, true, out) : false
    # forget gate
    conv_x_f = ChebConv(in => out, k; bias, init)
    conv_h_f = ChebConv(out => out, k; bias, init)
    w_f = init(out, 1)
    b_f = bias ? Flux.create_bias(w_f, true, out) : false
    # cell state
    conv_x_c = ChebConv(in => out, k; bias, init)
    conv_h_c = ChebConv(out => out, k; bias, init)
    w_c = init(out, 1)
    b_c = bias ? Flux.create_bias(w_c, true, out) : false
    # output gate
    conv_x_o = ChebConv(in => out, k; bias, init)
    conv_h_o = ChebConv(out => out, k; bias, init)
    w_o = init(out, 1)
    b_o = bias ? Flux.create_bias(w_o, true, out) : false
    return GConvLSTMCell(conv_x_i, conv_h_i, w_i, b_i,
                         conv_x_f, conv_h_f, w_f, b_f,
                         conv_x_c, conv_h_c, w_c, b_c,
                         conv_x_o, conv_h_o, w_o, b_o,
                         k, in, out)
end

function Flux.initialstates(cell::GConvLSTMCell)
    (zeros_like(cell.conv_x_i.weight, cell.out), zeros_like(cell.conv_x_i.weight, cell.out))
end

(cell::GConvLSTMCell)(g::GNNGraph, x::AbstractMatrix) = cell(g, x, initialstates(cell))

function (cell::GConvLSTMCell)(g::GNNGraph, x::AbstractMatrix, (h, c))
    if h isa AbstractVector
        h = repeat(h, 1, g.num_nodes)
    end
    if c isa AbstractVector
        c = repeat(c, 1, g.num_nodes)
    end
    @assert ndims(h) == 2 && ndims(c) == 2
    # input gate
    i = cell.conv_x_i(g, x) .+ cell.conv_h_i(g, h) .+ cell.w_i .* c .+ cell.b_i 
    i = Flux.sigmoid_fast(i)
    # forget gate
    f = cell.conv_x_f(g, x) .+ cell.conv_h_f(g, h) .+ cell.w_f .* c .+ cell.b_f
    f = Flux.sigmoid_fast(f)
    # cell state
    c = f .* c .+ i .* Flux.tanh_fast(cell.conv_x_c(g, x) .+ cell.conv_h_c(g, h) .+ cell.w_c .* c .+ cell.b_c)
    # output gate
    o = cell.conv_x_o(g, x) .+ cell.conv_h_o(g, h) .+ cell.w_o .* c .+ cell.b_o
    o = Flux.sigmoid_fast(o)
    h =  o .* Flux.tanh_fast(c)
    return h, (h, c)
end

function Base.show(io::IO, cell::GConvLSTMCell)
    print(io, "GConvLSTMCell($(cell.in) => $(cell.out), $(cell.k))")
end


"""
    GConvLSTM(args...; kws...)

Construct a recurrent layer corresponding to the [`GConvLSTMCell`](@ref) cell.
It can be used to process an entire temporal sequence of node features at once.

The arguments are passed to the [`GConvLSTMCell`](@ref) constructor.
See [`GNNRecurrence`](@ref) for more details.

# Examples

```jldoctest
julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);

julia> x = rand(Float32, d_in, timesteps, num_nodes);

julia> layer = GConvLSTM(d_in => d_out, 2)
GNNRecurrence(
  GConvLSTMCell(2 => 3, 2),             # 168 parameters
)                   # Total: 24 arrays, 168 parameters, 2.023 KiB.

julia> y = layer(g, x);

julia> size(y) # (d_out, timesteps, num_nodes)
(3, 5, 5)
```
""" 
GConvLSTM(args...; kws...) = GNNRecurrence(GConvLSTMCell(args...; kws...))

"""
    DCGRUCell(in => out, k; [bias, init])

Diffusion Convolutional Recurrent Neural Network (DCGRU) cell from the paper 
[Diffusion Convolutional Recurrent Neural Network: Data-driven Traffic Forecasting](https://arxiv.org/abs/1707.01926).

Applyis a [`DConv`](@ref) layer to model spatial dependencies, 
in combination with a Gated Recurrent Unit (GRU) cell to model temporal dependencies.

# Arguments

- `in`: Number of input node features.
- `out`: Number of output node features.
- `k`: Diffusion step for the `DConv`.
- `bias`: Add learnable bias. Default `true`.
- `init`: Convolution weights' initializer. Default `glorot_uniform`.

# Forward 

    cell(g::GNNGraph, x, [h])

- `g`: The input graph.
- `x`: The node features. It should be a matrix of size `in x num_nodes`.
- `h`: The current state of the GRU cell. It is a matrix of size `out x num_nodes`.
       If not provided, it is assumed to be a matrix of zeros.

Performs one recurrence step and returns a tuple `(h, h)`,
where `h` is the updated hidden state of the GRU cell.

# Examples

```jldoctest
julia> using GraphNeuralNetworks, Flux

julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);

julia> x = [rand(Float32, d_in, num_nodes) for t in 1:timesteps];

julia> cell = DCGRUCell(d_in => d_out, 2);

julia> state = Flux.initialstates(cell);

julia> y = state;

julia> for xt in x
           y, state = cell(g, xt, state)
       end

julia> size(y) # (d_out, num_nodes)
(3, 5)
```
"""
struct DCGRUCell
    in::Int
    out::Int
    k::Int
    dconv_u::DConv
    dconv_r::DConv
    dconv_c::DConv
end

Flux.@layer :noexpand DCGRUCell

function DCGRUCell(ch::Pair{Int,Int}, k::Int; bias = true, init = glorot_uniform)
    in, out = ch
    dconv_u = DConv((in + out) => out, k; bias, init)
    dconv_r = DConv((in + out) => out, k; bias, init)
    dconv_c = DConv((in + out) => out, k; bias, init)
    return DCGRUCell(in, out, k, dconv_u, dconv_r, dconv_c)
end

Flux.initialstates(cell::DCGRUCell) = zeros_like(cell.dconv_u.weights, cell.out)

(cell::DCGRUCell)(g::GNNGraph, x::AbstractMatrix) = cell(g, x, initialstates(cell))

function (cell::DCGRUCell)(g::GNNGraph, x::AbstractMatrix, h::AbstractVector) 
    return cell(g, x, repeat(h, 1, g.num_nodes))
end

function (cell::DCGRUCell)(g::GNNGraph, x::AbstractMatrix, h::AbstractMatrix)
    h̃ = vcat(x, h)
    z = cell.dconv_u(g, h̃)
    z = NNlib.sigmoid_fast.(z)
    r = cell.dconv_r(g, h̃)
    r = NNlib.sigmoid_fast.(r)
    ĥ = vcat(x, h .* r)
    c = cell.dconv_c(g, ĥ)
    c = NNlib.tanh_fast.(c)
    h = z.* h + (1 .- z) .* c
    return h, h
end

function Base.show(io::IO, cell::DCGRUCell)
    print(io, "DCGRUCell($(cell.in) => $(cell.out), $(cell.k))")
end

"""
    DCGRU(args...; kws...)

Construct a recurrent layer corresponding to the [`DCGRUCell`](@ref) cell.
It can be used to process an entire temporal sequence of node features at once.

The arguments are passed to the [`DCGRUCell`](@ref) constructor.
See [`GNNRecurrence`](@ref) for more details.

# Examples
```jldoctest
julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);

julia> x = rand(Float32, d_in, timesteps, num_nodes);

julia> layer = DCGRU(d_in => d_out, 2)
GNNRecurrence(
  DCGRUCell(2 => 3, 2),                 # 189 parameters
)                   # Total: 6 arrays, 189 parameters, 1.184 KiB.

julia> y = layer(g, x);

julia> size(y) # (d_out, timesteps, num_nodes)
(3, 5, 5)
```
"""
DCGRU(args...; kws...) = GNNRecurrence(DCGRUCell(args...; kws...))

""""
    EvolveGCNOCell(in => out; bias = true, init = glorot_uniform)

Evolving Graph Convolutional Network cell of type "-O" from the paper 
[EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs](https://arxiv.org/abs/1902.10191).

Uses a [`GCNConv`](@ref) layer to model spatial dependencies, and an `LSTMCell` to model temporal dependencies.
Can work with time-varying graphs and node features.

# Arguments

- `in => out`: A pair where `in` is the number of input node features and `out` 
  is the number of output node features.
- `bias`: Add learnable bias for the convolution and the lstm cell. Default `true`.
- `init`: Weights' initializer for the convolution. Default `glorot_uniform`.

# Forward 

    cell(g::GNNGraph, x, [state]) -> x, state

- `g`: The input graph.
- `x`: The node features. It should be a matrix of size `in x num_nodes`.
- `state`: The current state of the cell.
    A state is a tuple `(weight, lstm)` where `weight` is the convolution's weight and `lstm` is the lstm's state.
    If not provided, it is generated by calling `Flux.initialstates(cell)`.

Returns the updated node features `x` and the updated state.

```jldoctest
julia> using GraphNeuralNetworks, Flux

julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = [rand_graph(num_nodes, num_edges) for t in 1:timesteps];

julia> x = [rand(Float32, d_in, num_nodes) for t in 1:timesteps];

julia> cell1 = EvolveGCNOCell(d_in => d_out)
EvolveGCNOCell(2 => 3)  # 321 parameters

julia> cell2 = EvolveGCNOCell(d_out => d_out)
EvolveGCNOCell(3 => 3)  # 696 parameters

julia> state1 = Flux.initialstates(cell1);

julia> state2 = Flux.initialstates(cell2);

julia> outputs = [];

julia> for t in 1:timesteps
           zt, state1 = cell1(g[t], x[t], state1)
           yt, state2 = cell2(g[t], zt, state2)
           outputs = vcat(outputs, [yt])
       end

julia> size(outputs[end]) # (d_out, num_nodes)
(3, 5)
```
"""
struct EvolveGCNOCell{C,L} <: GNNLayer
    in::Int
    out::Int
    conv::C
    lstm::L
end

Flux.@layer :noexpand EvolveGCNOCell

function EvolveGCNOCell((in,out)::Pair{Int,Int}; bias = true, init = glorot_uniform)
    conv = GCNConv(in => out; bias, init)
    lstm = LSTMCell(in*out => in*out; bias)
    return EvolveGCNOCell(in, out, conv, lstm)
end

function Flux.initialstates(cell::EvolveGCNOCell)
    weight = reshape(cell.conv.weight, :)
    lstm = Flux.initialstates(cell.lstm)
    return (; weight, lstm)
end

function (cell::EvolveGCNOCell)(g::GNNGraph, x::AbstractMatrix, state)
    weight, state_lstm = cell.lstm(state.weight, state.lstm)
    x = cell.conv(g, x, conv_weight = reshape(weight, (cell.out, cell.in)))
    return x, (; weight, lstm = state_lstm)
end

function Base.show(io::IO, egcno::EvolveGCNOCell)
    print(io, "EvolveGCNOCell($(egcno.in) => $(egcno.out))")
end


"""
    EvolveGCNO(args...; kws...)

Construct a recurrent layer corresponding to the [`EvolveGCNOCell`](@ref) cell.
It can be used to process an entire temporal sequence of graphs and node features at once.

The arguments are passed to the [`EvolveGCNOCell`](@ref) constructor.
See [`GNNRecurrence`](@ref) for more details.

# Examples

```jldoctest
julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> num_nodes = [10, 10, 10, 10, 10];

julia> num_edges = [10, 12, 14, 16, 18];

julia> snapshots = [rand_graph(n, m) for (n, m) in zip(num_nodes, num_edges)];

julia> tg = TemporalSnapshotsGNNGraph(snapshots)

julia> x = [rand(Float32, d_in, n) for n in num_nodes];

julia> cell = EvolveGCNO(d_in => d_out)
GNNRecurrence(
  EvolveGCNOCell(2 => 3),               # 321 parameters
)                   # Total: 5 arrays, 321 parameters, 1.535 KiB.

julia> y = layer(tg, x);

julia> length(y)    # timesteps
5 

julia> size(y[end]) # (d_out, num_nodes[end])
(3, 10)
```
"""
EvolveGCNO(args...; kws...) = GNNRecurrence(EvolveGCNOCell(args...; kws...))


"""
    TGCNCell(in => out; kws...)

Recurrent graph convolutional cell from the paper
[T-GCN: A Temporal Graph Convolutional
Network for Traffic Prediction](https://arxiv.org/pdf/1811.05320).

Uses two stacked [`GCNConv`](@ref) layers to model spatial dependencies,
and a GRU mechanism to model temporal dependencies.

`in` and `out` are the number of input and output node features, respectively.
The keyword arguments are passed to the [`GCNConv`](@ref) constructor.

# Forward 

    cell(g::GNNGraph, x, [state])

- `g`: The input graph.
- `x`: The node features. It should be a matrix of size `in x num_nodes`.
- `state`: The current state of the cell.
    If not provided, it is generated by calling `Flux.initialstates(cell)`.
    The state is a matrix of size `out x num_nodes`.

Returns the updated node features and the updated state.

# Examples

```jldoctest
julia> using GraphNeuralNetworks, Flux

julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);

julia> x = [rand(Float32, d_in, num_nodes) for t in 1:timesteps];

julia> cell = DCGRUCell(d_in => d_out, 2);

julia> state = Flux.initialstates(cell);

julia> y = state;

julia> for xt in x
           y, state = cell(g, xt, state)
       end

julia> size(y) # (d_out, num_nodes)
(3, 5)
```
"""
@concrete struct TGCNCell <: GNNLayer
    in::Int
    out::Int
    conv_z
    dense_z
    conv_r
    dense_r
    conv_h
    dense_h
end

Flux.@layer :noexpand TGCNCell

function TGCNCell((in, out)::Pair{Int, Int}; kws...)
    conv_z = GNNChain(GCNConv(in => out, relu; kws...), GCNConv(out => out; kws...))
    dense_z = Dense(2*out => out, sigmoid)
    conv_r = GNNChain(GCNConv(in => out, relu; kws...), GCNConv(out => out; kws...))
    dense_r = Dense(2*out => out, sigmoid)
    conv_h = GNNChain(GCNConv(in => out, relu; kws...), GCNConv(out => out; kws...))
    dense_h = Dense(2*out => out, tanh)
    return TGCNCell(in, out, conv_z, dense_z, conv_r, dense_r, conv_h, dense_h)
end

Flux.initialstates(cell::TGCNCell) = zeros_like(cell.dense_z.weight, cell.out)

(cell::TGCNCell)(g::GNNGraph, x::AbstractMatrix) = cell(g, x, initialstates(cell))

function (cell::TGCNCell)(g::GNNGraph, x::AbstractMatrix, h::AbstractVector)
    return cell(g, x, repeat(h, 1, g.num_nodes))
end

function (cell::TGCNCell)(g::GNNGraph, x::AbstractMatrix, h::AbstractMatrix)
    z = cell.conv_z(g, x)
    z = cell.dense_z(vcat(z, h))
    r = cell.conv_r(g, x)
    r = cell.dense_r(vcat(r, h))
    h̃ = cell.conv_h(g, x)
    h̃ = cell.dense_h(vcat(h̃, r .* h))
    h = (1 .- z) .* h .+ z .* h̃
    return h, h
end

function Base.show(io::IO, cell::TGCNCell)
    print(io, "TGCNCell($(cell.in) => $(cell.out))")
end

"""
    TGCN(args...; kws...)

Construct a recurrent layer corresponding to the [`TGCNCell`](@ref) cell.

The arguments are passed to the [`TGCNCell`](@ref) constructor.
See [`GNNRecurrence`](@ref) for more details.

# Examples

```jldoctest
julia> num_nodes, num_edges = 5, 10;

julia> d_in, d_out = 2, 3;

julia> timesteps = 5;

julia> g = rand_graph(num_nodes, num_edges);

julia> x = rand(Float32, d_in, timesteps, num_nodes);

julia> layer = TGCN(d_in => d_out)

julia> y = layer(g, x);

julia> size(y) # (d_out, timesteps, num_nodes)
(3, 5, 5)
```
"""
TGCN(args...; kws...) = GNNRecurrence(TGCNCell(args...; kws...))

