var documenterSearchIndex = {"docs":
[{"location":"api/heteroconv/","page":"Hetero Convolutional layers","title":"Hetero Convolutional layers","text":"CurrentModule = GraphNeuralNetworks","category":"page"},{"location":"api/heteroconv/#Hetero-Graph-Convolutional-Layers","page":"Hetero Convolutional layers","title":"Hetero Graph-Convolutional Layers","text":"","category":"section"},{"location":"api/heteroconv/","page":"Hetero Convolutional layers","title":"Hetero Convolutional layers","text":"Heterogeneous graph convolutions are implemented in the type HeteroGraphConv. HeteroGraphConv relies on standard graph convolutional layers to perform message passing on the different relations.","category":"page"},{"location":"api/heteroconv/#Docs","page":"Hetero Convolutional layers","title":"Docs","text":"","category":"section"},{"location":"api/heteroconv/","page":"Hetero Convolutional layers","title":"Hetero Convolutional layers","text":"Modules = [GraphNeuralNetworks]\nPages   = [\"layers/heteroconv.jl\"]\nPrivate = false","category":"page"},{"location":"api/heteroconv/#GraphNeuralNetworks.HeteroGraphConv","page":"Hetero Convolutional layers","title":"GraphNeuralNetworks.HeteroGraphConv","text":"HeteroGraphConv(itr; aggr = +)\nHeteroGraphConv(pairs...; aggr = +)\n\nA convolutional layer for heterogeneous graphs.\n\nThe itr argument is an iterator of pairs of the form edge_t => layer, where edge_t is a 3-tuple of the form (src_node_type, edge_type, dst_node_type), and layer is a  convolutional layers for homogeneous graphs. \n\nEach convolution is applied to the corresponding relation.  Since a node type can be involved in multiple relations, the single convolution outputs  have to be aggregated using the aggr function. The default is to sum the outputs.\n\nForward Arguments\n\ng::GNNHeteroGraph: The input graph.\nx::Union{NamedTuple,Dict}: The input node features. The keys are node types and the values are node feature tensors.\n\nExamples\n\njulia> g = rand_bipartite_heterograph((10, 15), 20)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 10, :B => 15)\n  num_edges: Dict((:A, :to, :B) => 20, (:B, :to, :A) => 20)\n\njulia> x = (A = rand(Float32, 64, 10), B = rand(Float32, 64, 15));\n\njulia> layer = HeteroGraphConv((:A, :to, :B) => GraphConv(64 => 32, relu),\n                               (:B, :to, :A) => GraphConv(64 => 32, relu));\n\njulia> y = layer(g, x); # output is a named tuple\n\njulia> size(y.A) == (32, 10) && size(y.B) == (32, 15)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"datasets/#Datasets","page":"Datasets","title":"Datasets","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"GraphNeuralNetworks.jl doesn't come with its own datasets, but leverages those available in the Julia (and non-Julia) ecosystem. In particular, the examples in the GraphNeuralNetworks.jl repository make use of the MLDatasets.jl package. There you will find common graph datasets such as Cora, PubMed, Citeseer, TUDataset and many others.","category":"page"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"GraphNeuralNetworks.jl provides the GNNGraphs.mldataset2gnngraph method for interfacing with MLDatasets.jl.","category":"page"},{"location":"home/#GraphNeuralNetworks","page":"Home","title":"GraphNeuralNetworks","text":"","category":"section"},{"location":"home/","page":"Home","title":"Home","text":"This is the documentation page for GraphNeuralNetworks.jl, a graph neural network library written in Julia and based on the deep learning framework Flux.jl. GraphNeuralNetworks.jl is largely inspired by PyTorch Geometric, Deep Graph Library, and GeometricFlux.jl.","category":"page"},{"location":"home/","page":"Home","title":"Home","text":"Among its features:","category":"page"},{"location":"home/","page":"Home","title":"Home","text":"Implements common graph convolutional layers.\nSupports computations on batched graphs. \nEasy to define custom layers.\nCUDA support.\nIntegration with Graphs.jl.\nExamples of node, edge, and graph level machine learning tasks. ","category":"page"},{"location":"home/#Package-overview","page":"Home","title":"Package overview","text":"","category":"section"},{"location":"home/","page":"Home","title":"Home","text":"Let's give a brief overview of the package by solving a   graph regression problem with synthetic data. ","category":"page"},{"location":"home/","page":"Home","title":"Home","text":"Usage examples on real datasets can be found in the examples folder. ","category":"page"},{"location":"home/#Data-preparation","page":"Home","title":"Data preparation","text":"","category":"section"},{"location":"home/","page":"Home","title":"Home","text":"We create a dataset consisting in multiple random graphs and associated data features. ","category":"page"},{"location":"home/","page":"Home","title":"Home","text":"using GraphNeuralNetworks, Graphs, Flux, CUDA, Statistics, MLUtils\nusing Flux: DataLoader\n\nall_graphs = GNNGraph[]\n\nfor _ in 1:1000\n    g = rand_graph(10, 40,  \n            ndata=(; x = randn(Float32, 16,10)),  # input node features\n            gdata=(; y = randn(Float32)))         # regression target   \n    push!(all_graphs, g)\nend","category":"page"},{"location":"home/#Model-building","page":"Home","title":"Model building","text":"","category":"section"},{"location":"home/","page":"Home","title":"Home","text":"We concisely define our model as a GraphNeuralNetworks.GNNChain containing two graph convolutional layers. If CUDA is available, our model will live on the gpu.","category":"page"},{"location":"home/","page":"Home","title":"Home","text":"device = CUDA.functional() ? Flux.gpu : Flux.cpu;\n\nmodel = GNNChain(GCNConv(16 => 64),\n                BatchNorm(64),     # Apply batch normalization on node features (nodes dimension is batch dimension)\n                x -> relu.(x),     \n                GCNConv(64 => 64, relu),\n                GlobalPool(mean),  # aggregate node-wise features into graph-wise features\n                Dense(64, 1)) |> device\n\nopt = Flux.setup(Adam(1f-4), model)","category":"page"},{"location":"home/#Training","page":"Home","title":"Training","text":"","category":"section"},{"location":"home/","page":"Home","title":"Home","text":"Finally, we use a standard Flux training pipeline to fit our dataset. We use Flux's DataLoader to iterate over mini-batches of graphs  that are glued together into a single GNNGraph using the MLUtils.batch method. This is what happens under the hood when creating a DataLoader with the collate=true option. ","category":"page"},{"location":"home/","page":"Home","title":"Home","text":"train_graphs, test_graphs = MLUtils.splitobs(all_graphs, at=0.8)\n\ntrain_loader = DataLoader(train_graphs, \n                batchsize=32, shuffle=true, collate=true)\ntest_loader = DataLoader(test_graphs, \n                batchsize=32, shuffle=false, collate=true)\n\nloss(model, g::GNNGraph) = mean((vec(model(g, g.x)) - g.y).^2)\n\nloss(model, loader) = mean(loss(model, g |> device) for g in loader)\n\nfor epoch in 1:100\n    for g in train_loader\n        g = g |> device\n        grad = gradient(model -> loss(model, g), model)\n        Flux.update!(opt, model, grad[1])\n    end\n\n    @info (; epoch, train_loss=loss(model, train_loader), test_loss=loss(model, test_loader))\nend","category":"page"},{"location":"dev/#Developer-Notes","page":"Developer guide","title":"Developer Notes","text":"","category":"section"},{"location":"dev/#Develop-and-Managing-the-Monorepo","page":"Developer guide","title":"Develop and Managing the Monorepo","text":"","category":"section"},{"location":"dev/#Development-Enviroment","page":"Developer guide","title":"Development Enviroment","text":"","category":"section"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"GraphNeuralNetworks.jl is package hosted in a monorepo that contains multiple packages.  The GraphNeuralNetworks.jl package depends on GNNGraphs.jl, also hosted in the same monorepo.","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"pkg> activate .\n\npkg> dev ./GNNGraphs","category":"page"},{"location":"dev/#Add-a-New-Layer","page":"Developer guide","title":"Add a New Layer","text":"","category":"section"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"To add a new graph convolutional layer and make it available in both the Flux-based frontend (GraphNeuralNetworks.jl) and the Lux-based frontend (GNNLux), you need to:","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"Add the functional version to GNNlib\nAdd the stateful version to GraphNeuralNetworks\nAdd the stateless version to GNNLux\nAdd the layer to the table in docs/api/conv.md","category":"page"},{"location":"dev/#Versions-and-Tagging","page":"Developer guide","title":"Versions and Tagging","text":"","category":"section"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"Each PR should update the version number in the Porject.toml file of each involved package if needed by semnatic versioning. For instance, when adding new features GNNGraphs could move from \"1.17.5\" to \"1.18.0-DEV\". The \"DEV\" will be removed when the package is tagged and released. Pay also attention to updating the compat bounds, e.g. GraphNeuralNetworks might require a newer version of GNNGraphs.","category":"page"},{"location":"dev/#Generate-Documentation-Locally","page":"Developer guide","title":"Generate Documentation Locally","text":"","category":"section"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"For generating the documentation locally","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"cd docs\njulia","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"(@v1.10) pkg> activate .\n  Activating project at `~/.julia/dev/GraphNeuralNetworks/docs`\n\n(docs) pkg> dev ../ ../GNNGraphs/\n   Resolving package versions...\n  No Changes to `~/.julia/dev/GraphNeuralNetworks/docs/Project.toml`\n  No Changes to `~/.julia/dev/GraphNeuralNetworks/docs/Manifest.toml`\n\njulia> include(\"make.jl\")","category":"page"},{"location":"dev/#Benchmarking","page":"Developer guide","title":"Benchmarking","text":"","category":"section"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"You can benchmark the effect on performance of your commits using the script perf/perf.jl.","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"First, checkout and benchmark the master branch:","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"julia> include(\"perf.jl\")\n\njulia> df = run_benchmarks()\n\n# observe results\njulia> for g in groupby(df, :layer); println(g, \"\\n\"); end\n\njulia> @save \"perf_master_20210803_mymachine.jld2\" dfmaster=df","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"Now checkout your branch and do the same:","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"julia> df = run_benchmarks()\n\njulia> @save \"perf_pr_20210803_mymachine.jld2\" dfpr=df","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"Finally, compare the results:","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"julia> @load \"perf_master_20210803_mymachine.jld2\"\n\njulia> @load \"perf_pr_20210803_mymachine.jld2\"\n\njulia> compare(dfpr, dfmaster)","category":"page"},{"location":"dev/#Caching-tutorials","page":"Developer guide","title":"Caching tutorials","text":"","category":"section"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"Tutorials in GraphNeuralNetworks.jl are written in Pluto and rendered using DemoCards.jl and PlutoStaticHTML.jl. Rendering a Pluto notebook is time and resource-consuming, especially in a CI environment. So we use the caching functionality provided by PlutoStaticHTML.jl to reduce CI time.","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"If you are contributing a new tutorial or making changes to the existing notebook, generate the docs locally before committing/pushing. For caching to work, the cache environment(your local) and the documenter CI should have the same Julia version (e.g. \"v1.9.1\", also the patch number must match). So use the documenter CI Julia version for generating docs locally.","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"julia --version # check julia version before generating docs\njulia --project=docs docs/make.jl","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"Note: Use juliaup for easy switching of Julia versions.","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"During the doc generation process, DemoCards.jl stores the cache notebooks in docs/pluto_output. So add any changes made in this folder in your git commit. Remember that every file in this folder is machine-generated and should not be edited manually.","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"git add docs/pluto_output # add generated cache","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"Check the documenter CI logs to ensure that it used the local cache:","category":"page"},{"location":"dev/","page":"Developer guide","title":"Developer guide","text":"(Image: )","category":"page"},{"location":"models/#Models","page":"Models","title":"Models","text":"","category":"section"},{"location":"models/","page":"Models","title":"Models","text":"GraphNeuralNetworks.jl provides common graph convolutional layers by which you can assemble arbitrarily deep or complex models. GNN layers are compatible with  Flux.jl ones, therefore expert Flux users are promptly able to define and train  their models. ","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"In what follows, we discuss two different styles for model creation: the explicit modeling style, more verbose but more flexible,  and the implicit modeling style based on GraphNeuralNetworks.GNNChain, more concise but less flexible.","category":"page"},{"location":"models/#Explicit-modeling","page":"Models","title":"Explicit modeling","text":"","category":"section"},{"location":"models/","page":"Models","title":"Models","text":"In the explicit modeling style, the model is created according to the following steps:","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"Define a new type for your model (GNN in the example below). Layers and submodels are fields.\nApply Flux.@layer to the new type to make it Flux's compatible (parameters' collection, gpu movement, etc...)\nOptionally define a convenience constructor for your model.\nDefine the forward pass by implementing the call method for your type.\nInstantiate the model. ","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"Here is an example of this construction:","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"using Flux, Graphs, GraphNeuralNetworks\n\nstruct GNN                                # step 1\n    conv1\n    bn\n    conv2\n    dropout\n    dense\nend\n\nFlux.@layer GNN                         # step 2\n\nfunction GNN(din::Int, d::Int, dout::Int) # step 3    \n    GNN(GCNConv(din => d),\n        BatchNorm(d),\n        GraphConv(d => d, relu),\n        Dropout(0.5),\n        Dense(d, dout))\nend\n\nfunction (model::GNN)(g::GNNGraph, x)     # step 4\n    x = model.conv1(g, x)\n    x = relu.(model.bn(x))\n    x = model.conv2(g, x)\n    x = model.dropout(x)\n    x = model.dense(x)\n    return x \nend\n\ndin, d, dout = 3, 4, 2 \nmodel = GNN(din, d, dout)                 # step 5\n\ng = rand_graph(10, 30)\nX = randn(Float32, din, 10) \n\ny = model(g, X)  # output size: (dout, g.num_nodes)\ngrad = gradient(model -> sum(model(g, X)), model)","category":"page"},{"location":"models/#Implicit-modeling-with-GNNChains","page":"Models","title":"Implicit modeling with GNNChains","text":"","category":"section"},{"location":"models/","page":"Models","title":"Models","text":"While very flexible, the way in which we defined GNN model definition in last section is a bit verbose. In order to simplify things, we provide the GraphNeuralNetworks.GNNChain type. It is very similar  to Flux's well known Chain. It allows to compose layers in a sequential fashion as Chain does, propagating the output of each layer to the next one. In addition, GNNChain  handles propagates the input graph as well, providing it as a first argument to layers subtyping the GraphNeuralNetworks.GNNLayer abstract type. ","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"Using GNNChain, the previous example becomes","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"using Flux, Graphs, GraphNeuralNetworks\n\ndin, d, dout = 3, 4, 2 \ng = rand_graph(10, 30)\nX = randn(Float32, din, 10)\n\nmodel = GNNChain(GCNConv(din => d),\n                 BatchNorm(d),\n                 x -> relu.(x),\n                 GCNConv(d => d, relu),\n                 Dropout(0.5),\n                 Dense(d, dout))","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"The GNNChain only propagates the graph and the node features. More complex scenarios, e.g. when also edge features are updated, have to be handled using the explicit definition of the forward pass. ","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"A GNNChain opportunely propagates the graph into the branches created by the Flux.Parallel layer:","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"AddResidual(l) = Parallel(+, identity, l)  # implementing a skip/residual connection\n\nmodel = GNNChain( ResGatedGraphConv(din => d, relu),\n                  AddResidual(ResGatedGraphConv(d => d, relu)),\n                  AddResidual(ResGatedGraphConv(d => d, relu)),\n                  AddResidual(ResGatedGraphConv(d => d, relu)),\n                  GlobalPooling(mean),\n                  Dense(d, dout))\n\ny = model(g, X) # output size: (dout, g.num_graphs)","category":"page"},{"location":"models/#Embedding-a-graph-in-the-model","page":"Models","title":"Embedding a graph in the model","text":"","category":"section"},{"location":"models/","page":"Models","title":"Models","text":"Sometimes it is useful to consider a specific graph as a part of a model instead of  its input. GraphNeuralNetworks.jl provides the WithGraph type to deal with this scenario.","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"chain = GNNChain(GCNConv(din => d, relu),\n                 GCNConv(d => d))\n\n\ng = rand_graph(10, 30)\n\nmodel = WithGraph(chain, g)\n\nX = randn(Float32, din, 10)\n\n# Pass only X as input, the model already contains the graph.\ny = model(X) ","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"An example of WithGraph usage is given in the graph neural ODE script in the examples folder.","category":"page"},{"location":"gsoc/#Graph-Neural-Networks-Summer-of-Code","page":"Google Summer of Code","title":"Graph Neural Networks - Summer of Code","text":"","category":"section"},{"location":"gsoc/","page":"Google Summer of Code","title":"Google Summer of Code","text":"Potential candidates to Google Summer of Code's scholarships can find out about the available projects involving GraphNeuralNetworks.jl on the dedicated page in the Julia Language website.","category":"page"},{"location":"api/pool/","page":"Pooling layers","title":"Pooling layers","text":"CurrentModule = GraphNeuralNetworks","category":"page"},{"location":"api/pool/#Pooling-Layers","page":"Pooling layers","title":"Pooling Layers","text":"","category":"section"},{"location":"api/pool/#Index","page":"Pooling layers","title":"Index","text":"","category":"section"},{"location":"api/pool/","page":"Pooling layers","title":"Pooling layers","text":"Order = [:type, :function]\nPages   = [\"pool.md\"]","category":"page"},{"location":"api/pool/#Docs","page":"Pooling layers","title":"Docs","text":"","category":"section"},{"location":"api/pool/","page":"Pooling layers","title":"Pooling layers","text":"Modules = [GraphNeuralNetworks]\nPages   = [\"layers/pool.jl\"]\nPrivate = false","category":"page"},{"location":"api/pool/#GraphNeuralNetworks.GlobalAttentionPool","page":"Pooling layers","title":"GraphNeuralNetworks.GlobalAttentionPool","text":"GlobalAttentionPool(fgate, ffeat=identity)\n\nGlobal soft attention layer from the Gated Graph Sequence Neural Networks paper\n\nmathbfu_V = sum_iin V alpha_i f_feat(mathbfx_i)\n\nwhere the coefficients alpha_i are given by a softmax_nodes operation:\n\nalpha_i = frace^f_gate(mathbfx_i)\n                sum_iin V e^f_gate(mathbfx_i)\n\nArguments\n\nfgate: The function f_gate mathbbR^D_in to mathbbR.           It is tipically expressed by a neural network.\nffeat: The function f_feat mathbbR^D_in to mathbbR^D_out.           It is tipically expressed by a neural network.\n\nExamples\n\nchin = 6\nchout = 5    \n\nfgate = Dense(chin, 1)\nffeat = Dense(chin, chout)\npool = GlobalAttentionPool(fgate, ffeat)\n\ng = Flux.batch([GNNGraph(random_regular_graph(10, 4), \n                         ndata=rand(Float32, chin, 10)) \n                for i=1:3])\n\nu = pool(g, g.ndata.x)\n\n@assert size(u) == (chout, g.num_graphs)\n\n\n\n\n\n","category":"type"},{"location":"api/pool/#GraphNeuralNetworks.GlobalPool","page":"Pooling layers","title":"GraphNeuralNetworks.GlobalPool","text":"GlobalPool(aggr)\n\nGlobal pooling layer for graph neural networks. Takes a graph and feature nodes as inputs and performs the operation\n\nmathbfu_V = square_i in V mathbfx_i\n\nwhere V is the set of nodes of the input graph and  the type of aggregation represented by square is selected by the aggr argument.  Commonly used aggregations are mean, max, and +.\n\nSee also reduce_nodes.\n\nExamples\n\nusing Flux, GraphNeuralNetworks, Graphs\n\npool = GlobalPool(mean)\n\ng = GNNGraph(erdos_renyi(10, 4))\nX = rand(32, 10)\npool(g, X) # => 32x1 matrix\n\n\ng = Flux.batch([GNNGraph(erdos_renyi(10, 4)) for _ in 1:5])\nX = rand(32, 50)\npool(g, X) # => 32x5 matrix\n\n\n\n\n\n","category":"type"},{"location":"api/pool/#GraphNeuralNetworks.Set2Set","page":"Pooling layers","title":"GraphNeuralNetworks.Set2Set","text":"Set2Set(n_in, n_iters, n_layers = 1)\n\nSet2Set layer from the paper Order Matters: Sequence to sequence for sets.\n\nFor each graph in the batch, the layer computes an output vector of size 2*n_in by iterating the following steps n_iters times:\n\nmathbfq = mathrmLSTM(mathbfq_t-1^*)\nalpha_i = fracexp(mathbfq^T mathbfx_i)sum_j=1^N exp(mathbfq^T mathbfx_j) \nmathbfr = sum_i=1^N alpha_i mathbfx_i\nmathbfq^*_t = mathbfq mathbfr\n\nwhere N is the number of nodes in the graph, LSTM is a Long-Short-Term-Memory network with n_layers layers,  input size 2*n_in and output size n_in.\n\nGiven a batch of graphs g and node features x, the layer returns a matrix of size (2*n_in, n_graphs). ```\n\n\n\n\n\n","category":"type"},{"location":"api/pool/#GraphNeuralNetworks.TopKPool","page":"Pooling layers","title":"GraphNeuralNetworks.TopKPool","text":"TopKPool(adj, k, in_channel)\n\nTop-k pooling layer.\n\nArguments\n\nadj: Adjacency matrix  of a graph.\nk: Top-k nodes are selected to pool together.\nin_channel: The dimension of input channel.\n\n\n\n\n\n","category":"type"},{"location":"api/samplers/","page":"Samplers","title":"Samplers","text":"CurrentModule = GraphNeuralNetworks","category":"page"},{"location":"api/samplers/#Samplers","page":"Samplers","title":"Samplers","text":"","category":"section"},{"location":"api/samplers/#Docs","page":"Samplers","title":"Docs","text":"","category":"section"},{"location":"api/samplers/","page":"Samplers","title":"Samplers","text":"Modules = [GraphNeuralNetworks]\nPages   = [\"samplers.jl\"]\nPrivate = false","category":"page"},{"location":"api/samplers/#GraphNeuralNetworks.NeighborLoader","page":"Samplers","title":"GraphNeuralNetworks.NeighborLoader","text":"NeighborLoader(graph; num_neighbors, input_nodes, num_layers, [batch_size])\n\nA data structure for sampling neighbors from a graph for training Graph Neural Networks (GNNs).  It supports multi-layer sampling of neighbors for a batch of input nodes, useful for mini-batch training originally introduced in \"Inductive Representation Learning on Large Graphs\" paper. [see https://arxiv.org/abs/1706.02216]\n\nFields\n\ngraph::GNNGraph: The input graph.\nnum_neighbors::Vector{Int}: A vector specifying the number of neighbors to sample per node at each GNN layer.\ninput_nodes::Vector{Int}: A vector containing the starting nodes for neighbor sampling.\nnum_layers::Int: The number of layers for neighborhood expansion (how far to sample neighbors).\nbatch_size::Union{Int, Nothing}: The size of the batch. If not specified, it defaults to the number of input_nodes.\n\nUsage\n\njulia>  loader = NeighborLoader(graph; num_neighbors=[10, 5], input_nodes=[1, 2, 3], num_layers=2)\n\njulia> batch_counter = 0\njulia> for mini_batch_gnn in loader\n            batch_counter += 1\n            println(\"Batch \", batch_counter, \": Nodes in mini-batch graph: \", nv(mini_batch_gnn))\n\n\n\n\n\n","category":"type"},{"location":"api/basic/","page":"Basic","title":"Basic","text":"CurrentModule = GraphNeuralNetworks","category":"page"},{"location":"api/basic/#Basic-Layers","page":"Basic","title":"Basic Layers","text":"","category":"section"},{"location":"api/basic/#Index","page":"Basic","title":"Index","text":"","category":"section"},{"location":"api/basic/","page":"Basic","title":"Basic","text":"Modules = [GraphNeuralNetworks]\nPages = [\"basic.md\"]","category":"page"},{"location":"api/basic/#Docs","page":"Basic","title":"Docs","text":"","category":"section"},{"location":"api/basic/","page":"Basic","title":"Basic","text":"Modules = [GraphNeuralNetworks]\nPages   = [\"layers/basic.jl\"]\nPrivate = false","category":"page"},{"location":"api/basic/#GraphNeuralNetworks.DotDecoder","page":"Basic","title":"GraphNeuralNetworks.DotDecoder","text":"DotDecoder()\n\nA graph neural network layer that  for given input graph g and node features x, returns the dot product x_i ⋅ xj on each edge. \n\nExamples\n\njulia> g = rand_graph(5, 6)\nGNNGraph:\n    num_nodes = 5\n    num_edges = 6\n\njulia> dotdec = DotDecoder()\nDotDecoder()\n\njulia> dotdec(g, rand(2, 5))\n1×6 Matrix{Float64}:\n 0.345098  0.458305  0.106353  0.345098  0.458305  0.106353\n\n\n\n\n\n","category":"type"},{"location":"api/basic/#GraphNeuralNetworks.GNNChain","page":"Basic","title":"GraphNeuralNetworks.GNNChain","text":"GNNChain(layers...)\nGNNChain(name = layer, ...)\n\nCollects multiple layers / functions to be called in sequence on given input graph and input node features. \n\nIt allows to compose layers in a sequential fashion as Flux.Chain does, propagating the output of each layer to the next one. In addition, GNNChain handles the input graph as well, providing it  as a first argument only to layers subtyping the GNNLayer abstract type. \n\nGNNChain supports indexing and slicing, m[2] or m[1:end-1], and if names are given, m[:name] == m[1] etc.\n\nExamples\n\njulia> using Flux, GraphNeuralNetworks\n\njulia> m = GNNChain(GCNConv(2=>5), \n                    BatchNorm(5), \n                    x -> relu.(x), \n                    Dense(5, 4))\nGNNChain(GCNConv(2 => 5), BatchNorm(5), #7, Dense(5 => 4))\n\njulia> x = randn(Float32, 2, 3);\n\njulia> g = rand_graph(3, 6)\nGNNGraph:\n    num_nodes = 3\n    num_edges = 6\n\njulia> m(g, x)\n4×3 Matrix{Float32}:\n    -0.795592  -0.795592  -0.795592\n    -0.736409  -0.736409  -0.736409\n    0.994925   0.994925   0.994925\n    0.857549   0.857549   0.857549\n\njulia> m2 = GNNChain(enc = m, \n                     dec = DotDecoder())\nGNNChain(enc = GNNChain(GCNConv(2 => 5), BatchNorm(5), #7, Dense(5 => 4)), dec = DotDecoder())\n\njulia> m2(g, x)\n1×6 Matrix{Float32}:\n 2.90053  2.90053  2.90053  2.90053  2.90053  2.90053\n\njulia> m2[:enc](g, x) == m(g, x)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"api/basic/#GraphNeuralNetworks.GNNLayer","page":"Basic","title":"GraphNeuralNetworks.GNNLayer","text":"abstract type GNNLayer end\n\nAn abstract type from which graph neural network layers are derived.\n\nSee also GNNChain.\n\n\n\n\n\n","category":"type"},{"location":"api/basic/#GraphNeuralNetworks.WithGraph","page":"Basic","title":"GraphNeuralNetworks.WithGraph","text":"WithGraph(model, g::GNNGraph; traingraph=false)\n\nA type wrapping the model and tying it to the graph g. In the forward pass, can only take feature arrays as inputs, returning model(g, x...; kws...).\n\nIf traingraph=false, the graph's parameters won't be part of  the trainable parameters in the gradient updates.\n\nExamples\n\ng = GNNGraph([1,2,3], [2,3,1])\nx = rand(Float32, 2, 3)\nmodel = SAGEConv(2 => 3)\nwg = WithGraph(model, g)\n# No need to feed the graph to `wg`\n@assert wg(x) == model(g, x)\n\ng2 = GNNGraph([1,1,2,3], [2,4,1,1])\nx2 = rand(Float32, 2, 4)\n# WithGraph will ignore the internal graph if fed with a new one. \n@assert wg(g2, x2) == model(g2, x2)\n\n\n\n\n\n","category":"type"},{"location":"api/temporalconv/","page":"Temporal Convolutional layers","title":"Temporal Convolutional layers","text":"CurrentModule = GraphNeuralNetworks","category":"page"},{"location":"api/temporalconv/#Temporal-Graph-Convolutional-Layers","page":"Temporal Convolutional layers","title":"Temporal Graph-Convolutional Layers","text":"","category":"section"},{"location":"api/temporalconv/","page":"Temporal Convolutional layers","title":"Temporal Convolutional layers","text":"Convolutions for time-varying graphs (temporal graphs) such as the TemporalSnapshotsGNNGraph.","category":"page"},{"location":"api/temporalconv/#Docs","page":"Temporal Convolutional layers","title":"Docs","text":"","category":"section"},{"location":"api/temporalconv/","page":"Temporal Convolutional layers","title":"Temporal Convolutional layers","text":"Modules = [GraphNeuralNetworks]\nPages   = [\"layers/temporalconv.jl\"]\nPrivate = false","category":"page"},{"location":"api/temporalconv/#GraphNeuralNetworks.A3TGCN","page":"Temporal Convolutional layers","title":"GraphNeuralNetworks.A3TGCN","text":"A3TGCN(in => out; [bias, init, init_state, add_self_loops, use_edge_weight])\n\nAttention Temporal Graph Convolutional Network (A3T-GCN) model from the paper A3T-GCN: Attention Temporal Graph Convolutional Network for Traffic Forecasting.\n\nPerforms a TGCN layer, followed by a soft attention layer.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nbias: Add learnable bias. Default true.\ninit: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the GRU layer. Default zeros32.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default false.\nuse_edge_weight: If true, consider the edge weights in the input graph (if available).                    If add_self_loops=true the new weights will be set to 1.                     This option is ignored if the edge_weight is explicitly provided in the forward pass.                    Default false.\n\nExamples\n\njulia> a3tgcn = A3TGCN(2 => 6)\nA3TGCN(2 => 6)\n\njulia> g, x = rand_graph(5, 10), rand(Float32, 2, 5);\n\njulia> y = a3tgcn(g,x);\n\njulia> size(y)\n(6, 5)\n\njulia> Flux.reset!(a3tgcn);\n\njulia> y = a3tgcn(rand_graph(5, 10), rand(Float32, 2, 5, 20));\n\njulia> size(y)\n(6, 5)\n\nwarning: Batch size changes\nFailing to call reset! when the input batch size changes can lead to unexpected behavior.\n\n\n\n\n\n","category":"type"},{"location":"api/temporalconv/#GraphNeuralNetworks.EvolveGCNO","page":"Temporal Convolutional layers","title":"GraphNeuralNetworks.EvolveGCNO","text":"EvolveGCNO(ch; bias = true, init = glorot_uniform, init_state = Flux.zeros32)\n\nEvolving Graph Convolutional Network (EvolveGCNO) layer from the paper EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs.\n\nPerfoms a Graph Convolutional layer with parameters derived from a Long Short-Term Memory (LSTM) layer across the snapshots of the temporal graph.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nbias: Add learnable bias. Default true.\ninit: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the LSTM layer. Default zeros32.\n\nExamples\n\njulia> tg = TemporalSnapshotsGNNGraph([rand_graph(10,20; ndata = rand(4,10)), rand_graph(10,14; ndata = rand(4,10)), rand_graph(10,22; ndata = rand(4,10))])\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> ev = EvolveGCNO(4 => 5)\nEvolveGCNO(4 => 5)\n\njulia> size(ev(tg, tg.ndata.x))\n(3,)\n\njulia> size(ev(tg, tg.ndata.x)[1])\n(5, 10)\n\n\n\n\n\n","category":"type"},{"location":"api/temporalconv/#GraphNeuralNetworks.DCGRU-Tuple{Any, Any, Any}","page":"Temporal Convolutional layers","title":"GraphNeuralNetworks.DCGRU","text":"DCGRU(in => out, k, n; [bias, init, init_state])\n\nDiffusion Convolutional Recurrent Neural Network (DCGRU) layer from the paper Diffusion Convolutional Recurrent Neural Network: Data-driven Traffic Forecasting.\n\nPerforms a Diffusion Convolutional layer to model spatial dependencies, followed by a Gated Recurrent Unit (GRU) cell to model temporal dependencies.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nk: Diffusion step.\nn: Number of nodes in the graph.\nbias: Add learnable bias. Default true.\ninit: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the LSTM layer. Default zeros32.\n\nExamples\n\njulia> g1, x1 = rand_graph(5, 10), rand(Float32, 2, 5);\n\njulia> dcgru = DCGRU(2 => 5, 2, g1.num_nodes);\n\njulia> y = dcgru(g1, x1);\n\njulia> size(y)\n(5, 5)\n\njulia> g2, x2 = rand_graph(5, 10), rand(Float32, 2, 5, 30);\n\njulia> z = dcgru(g2, x2);\n\njulia> size(z)\n(5, 5, 30)\n\n\n\n\n\n","category":"method"},{"location":"api/temporalconv/#GraphNeuralNetworks.GConvGRU-Tuple{Any, Any, Any}","page":"Temporal Convolutional layers","title":"GraphNeuralNetworks.GConvGRU","text":"GConvGRU(in => out, k, n; [bias, init, init_state])\n\nGraph Convolutional Gated Recurrent Unit (GConvGRU) recurrent layer from the paper Structured Sequence Modeling with Graph Convolutional Recurrent Networks.\n\nPerforms a layer of ChebConv to model spatial dependencies, followed by a Gated Recurrent Unit (GRU) cell to model temporal dependencies.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nk: Chebyshev polynomial order.\nn: Number of nodes in the graph.\nbias: Add learnable bias. Default true.\ninit: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the GRU layer. Default zeros32.\n\nExamples\n\njulia> g1, x1 = rand_graph(5, 10), rand(Float32, 2, 5);\n\njulia> ggru = GConvGRU(2 => 5, 2, g1.num_nodes);\n\njulia> y = ggru(g1, x1);\n\njulia> size(y)\n(5, 5)\n\njulia> g2, x2 = rand_graph(5, 10), rand(Float32, 2, 5, 30);\n\njulia> z = ggru(g2, x2);\n\njulia> size(z)\n(5, 5, 30)\n\n\n\n\n\n","category":"method"},{"location":"api/temporalconv/#GraphNeuralNetworks.GConvLSTM-Tuple{Any, Any, Any}","page":"Temporal Convolutional layers","title":"GraphNeuralNetworks.GConvLSTM","text":"GConvLSTM(in => out, k, n; [bias, init, init_state])\n\nGraph Convolutional Long Short-Term Memory (GConvLSTM) recurrent layer from the paper Structured Sequence Modeling with Graph Convolutional Recurrent Networks. \n\nPerforms a layer of ChebConv to model spatial dependencies, followed by a Long Short-Term Memory (LSTM) cell to model temporal dependencies.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nk: Chebyshev polynomial order.\nn: Number of nodes in the graph.\nbias: Add learnable bias. Default true.\ninit: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the LSTM layer. Default zeros32.\n\nExamples\n\njulia> g1, x1 = rand_graph(5, 10), rand(Float32, 2, 5);\n\njulia> gclstm = GConvLSTM(2 => 5, 2, g1.num_nodes);\n\njulia> y = gclstm(g1, x1);\n\njulia> size(y)\n(5, 5)\n\njulia> g2, x2 = rand_graph(5, 10), rand(Float32, 2, 5, 30);\n\njulia> z = gclstm(g2, x2);\n\njulia> size(z)\n(5, 5, 30)\n\n\n\n\n\n","category":"method"},{"location":"api/temporalconv/#GraphNeuralNetworks.TGCN-Tuple{Any}","page":"Temporal Convolutional layers","title":"GraphNeuralNetworks.TGCN","text":"TGCN(in => out; [bias, init, init_state, add_self_loops, use_edge_weight])\n\nTemporal Graph Convolutional Network (T-GCN) recurrent layer from the paper T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction.\n\nPerforms a layer of GCNConv to model spatial dependencies, followed by a Gated Recurrent Unit (GRU) cell to model temporal dependencies.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nbias: Add learnable bias. Default true.\ninit: Weights' initializer. Default glorot_uniform.\ninit_state: Initial state of the hidden stat of the GRU layer. Default zeros32.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default false.\nuse_edge_weight: If true, consider the edge weights in the input graph (if available).                    If add_self_loops=true the new weights will be set to 1.                     This option is ignored if the edge_weight is explicitly provided in the forward pass.                    Default false.\n\nExamples\n\njulia> tgcn = TGCN(2 => 6)\nRecur(\n  TGCNCell(\n    GCNConv(2 => 6, σ),                 # 18 parameters\n    GRUv3Cell(6 => 6),                  # 240 parameters\n    Float32[0.0; 0.0; … ; 0.0; 0.0;;],  # 6 parameters  (all zero)\n    2,\n    6,\n  ),\n)         # Total: 8 trainable arrays, 264 parameters,\n          # plus 1 non-trainable, 6 parameters, summarysize 1.492 KiB.\n\njulia> g, x = rand_graph(5, 10), rand(Float32, 2, 5);\n\njulia> y = tgcn(g, x);\n\njulia> size(y)\n(6, 5)\n\njulia> Flux.reset!(tgcn);\n\njulia> tgcn(rand_graph(5, 10), rand(Float32, 2, 5, 20)) |> size # batch size of 20\n(6, 5, 20)\n\nwarning: Batch size changes\nFailing to call reset! when the input batch size changes can lead to unexpected behavior.\n\n\n\n\n\n","category":"method"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"CurrentModule = GraphNeuralNetworks","category":"page"},{"location":"api/conv/#Convolutional-Layers","page":"Convolutional layers","title":"Convolutional Layers","text":"","category":"section"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"Many different types of graphs convolutional layers have been proposed in the literature. Choosing the right layer for your application could involve a lot of exploration.  Some of the most commonly used layers are the GCNConv and the GATv2Conv. Multiple graph convolutional layers are typically stacked together to create a graph neural network model (see GNNChain).","category":"page"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"The table below lists all graph convolutional layers implemented in the GraphNeuralNetworks.jl. It also highlights the presence of some additional capabilities with respect to basic message passing:","category":"page"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"Sparse Ops: implements message passing as multiplication by sparse adjacency matrix instead of the gather/scatter mechanism. This can lead to better CPU performances but it is not supported on GPU yet. \nEdge Weight: supports scalar weights (or equivalently scalar features) on edges. \nEdge Features: supports feature vectors on edges.\nHeterograph: supports heterogeneous graphs (see GNNHeteroGraph).\nTemporalSnapshotsGNNGraphs: supports temporal graphs (see TemporalSnapshotsGNNGraph) by applying the convolution layers to each snapshot independently.","category":"page"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"Layer Sparse Ops Edge Weight Edge Features Heterograph TemporalSnapshotsGNNGraphs\nAGNNConv   ✓  \nCGConv   ✓ ✓ ✓\nChebConv     ✓\nEGNNConv   ✓  \nEdgeConv    ✓ \nGATConv   ✓ ✓ ✓\nGATv2Conv   ✓ ✓ ✓\nGatedGraphConv ✓    ✓\nGCNConv ✓ ✓  ✓ \nGINConv ✓   ✓ ✓\nGMMConv   ✓  \nGraphConv ✓   ✓ ✓\nMEGNetConv   ✓  \nNNConv   ✓  \nResGatedGraphConv    ✓ ✓\nSAGEConv ✓   ✓ ✓\nSGConv ✓    ✓\nTransformerConv   ✓  ","category":"page"},{"location":"api/conv/#Docs","page":"Convolutional layers","title":"Docs","text":"","category":"section"},{"location":"api/conv/","page":"Convolutional layers","title":"Convolutional layers","text":"Modules = [GraphNeuralNetworks]\nPages   = [\"layers/conv.jl\"]\nPrivate = false","category":"page"},{"location":"api/conv/#GraphNeuralNetworks.AGNNConv","page":"Convolutional layers","title":"GraphNeuralNetworks.AGNNConv","text":"AGNNConv(; init_beta=1.0f0, trainable=true, add_self_loops=true)\n\nAttention-based Graph Neural Network layer from paper Attention-based Graph Neural Network for Semi-Supervised Learning.\n\nThe forward pass is given by\n\nmathbfx_i = sum_j in N(i) alpha_ij mathbfx_j\n\nwhere the attention coefficients alpha_ij are given by\n\nalpha_ij =frace^beta cos(mathbfx_i mathbfx_j)\n                  sum_je^beta cos(mathbfx_i mathbfx_j)\n\nwith the cosine distance defined by\n\ncos(mathbfx_i mathbfx_j) = \n  fracmathbfx_i cdot mathbfx_jlVertmathbfx_irVert lVertmathbfx_jrVert\n\nand beta a trainable parameter if trainable=true.\n\nArguments\n\ninit_beta: The initial value of beta. Default 1.0f0.\ntrainable: If true, beta is trainable. Default true.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default true.\n\nExamples:\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\n\n# create layer\nl = AGNNConv(init_beta=2.0f0)\n\n# forward pass\ny = l(g, x)   \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.CGConv","page":"Convolutional layers","title":"GraphNeuralNetworks.CGConv","text":"CGConv((in, ein) => out, act=identity; bias=true, init=glorot_uniform, residual=false)\nCGConv(in => out, ...)\n\nThe crystal graph convolutional layer from the paper Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties. Performs the operation\n\nmathbfx_i = mathbfx_i + sum_jin N(i)sigma(W_f mathbfz_ij + mathbfb_f) act(W_s mathbfz_ij + mathbfb_s)\n\nwhere mathbfz_ij  is the node and edge features concatenation  mathbfx_i mathbfx_j mathbfe_jto i  and sigma is the sigmoid function. The residual mathbfx_i is added only if residual=true and the output size is the same  as the input size.\n\nArguments\n\nin: The dimension of input node features.\nein: The dimension of input edge features. \n\nIf ein is not given, assumes that no edge features are passed as input in the forward pass.\n\nout: The dimension of output node features.\nact: Activation function.\nbias: Add learnable bias.\ninit: Weights' initializer.\nresidual: Add a residual connection.\n\nExamples\n\ng = rand_graph(5, 6)\nx = rand(Float32, 2, g.num_nodes)\ne = rand(Float32, 3, g.num_edges)\n\nl = CGConv((2, 3) => 4, tanh)\ny = l(g, x, e)    # size: (4, num_nodes)\n\n# No edge features\nl = CGConv(2 => 4, tanh)\ny = l(g, x)    # size: (4, num_nodes)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.ChebConv","page":"Convolutional layers","title":"GraphNeuralNetworks.ChebConv","text":"ChebConv(in => out, k; bias=true, init=glorot_uniform)\n\nChebyshev spectral graph convolutional layer from paper Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering.\n\nImplements\n\nX = sum^K-1_k=0  W^(k) Z^(k)\n\nwhere Z^(k) is the k-th term of Chebyshev polynomials, and can be calculated by the following recursive form:\n\nbeginaligned\nZ^(0) = X \nZ^(1) = hatL X \nZ^(k) = 2 hatL Z^(k-1) - Z^(k-2)\nendaligned\n\nwith hatL the scaled_laplacian.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nk: The order of Chebyshev polynomial.\nbias: Add learnable bias.\ninit: Weights' initializer.\n\nExamples\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(Float32, 3, g.num_nodes)\n\n# create layer\nl = ChebConv(3 => 5, 5) \n\n# forward pass\ny = l(g, x)       # size:  5 × num_nodes\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.DConv","page":"Convolutional layers","title":"GraphNeuralNetworks.DConv","text":"DConv(ch::Pair{Int, Int}, k::Int; init = glorot_uniform, bias = true)\n\nDiffusion convolution layer from the paper Diffusion Convolutional Recurrent Neural Networks: Data-Driven Traffic Forecasting.\n\nArguments\n\nch: Pair of input and output dimensions.\nk: Number of diffusion steps.\ninit: Weights' initializer. Default glorot_uniform.\nbias: Add learnable bias. Default true.\n\nExamples\n\njulia> g = GNNGraph(rand(10, 10), ndata = rand(Float32, 2, 10));\n\njulia> dconv = DConv(2 => 4, 4)\nDConv(2 => 4, 4)\n\njulia> y = dconv(g, g.ndata.x);\n\njulia> size(y)\n(4, 10)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.EGNNConv","page":"Convolutional layers","title":"GraphNeuralNetworks.EGNNConv","text":"EGNNConv((in, ein) => out; hidden_size=2in, residual=false)\nEGNNConv(in => out; hidden_size=2in, residual=false)\n\nEquivariant Graph Convolutional Layer from E(n) Equivariant Graph Neural Networks.\n\nThe layer performs the following operation:\n\nbeginaligned\nmathbfm_jto i =phi_e(mathbfh_i mathbfh_j lVertmathbfx_i-mathbfx_jrVert^2 mathbfe_jto i)\nmathbfx_i = mathbfx_i + C_isum_jinmathcalN(i)(mathbfx_i-mathbfx_j)phi_x(mathbfm_jto i)\nmathbfm_i = C_isum_jinmathcalN(i) mathbfm_jto i\nmathbfh_i = mathbfh_i + phi_h(mathbfh_i mathbfm_i)\nendaligned\n\nwhere mathbfh_i, mathbfx_i, mathbfe_jto i are invariant node features, equivariant node features, and edge features respectively. phi_e, phi_h, and phi_x are two-layer MLPs. C is a constant for normalization, computed as 1mathcalN(i).\n\nConstructor Arguments\n\nin: Number of input features for h.\nout: Number of output features for h.\nein: Number of input edge features.\nhidden_size: Hidden representation size.\nresidual: If true, add a residual connection. Only possible if in == out. Default false.\n\nForward Pass\n\nl(g, x, h, e=nothing)\n\nForward Pass Arguments:\n\ng : The graph.\nx : Matrix of equivariant node coordinates.\nh : Matrix of invariant node features.\ne : Matrix of invariant edge features. Default nothing.\n\nReturns updated h and x.\n\nExamples\n\ng = rand_graph(10, 10)\nh = randn(Float32, 5, g.num_nodes)\nx = randn(Float32, 3, g.num_nodes)\negnn = EGNNConv(5 => 6, 10)\nhnew, xnew = egnn(g, h, x)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.EdgeConv","page":"Convolutional layers","title":"GraphNeuralNetworks.EdgeConv","text":"EdgeConv(nn; aggr=max)\n\nEdge convolutional layer from paper Dynamic Graph CNN for Learning on Point Clouds.\n\nPerforms the operation\n\nmathbfx_i = square_j in N(i) nn(mathbfx_i mathbfx_j - mathbfx_i)\n\nwhere nn generally denotes a learnable function, e.g. a linear layer or a multi-layer perceptron.\n\nArguments\n\nnn: A (possibly learnable) function. \naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\n\nExamples:\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\n\n# create layer\nl = EdgeConv(Dense(2 * in_channel, out_channel), aggr = +)\n\n# forward pass\ny = l(g, x)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GATConv","page":"Convolutional layers","title":"GraphNeuralNetworks.GATConv","text":"GATConv(in => out, [σ; heads, concat, init, bias, negative_slope, add_self_loops])\nGATConv((in, ein) => out, ...)\n\nGraph attentional layer from the paper Graph Attention Networks.\n\nImplements the operation\n\nmathbfx_i = sum_j in N(i) cup i alpha_ij W mathbfx_j\n\nwhere the attention coefficients alpha_ij are given by\n\nalpha_ij = frac1z_i exp(LeakyReLU(mathbfa^T W mathbfx_i W mathbfx_j))\n\nwith z_i a normalization factor. \n\nIn case ein > 0 is given, edge features of dimension ein will be expected in the forward pass  and the attention coefficients will be calculated as  \n\nalpha_ij = frac1z_i exp(LeakyReLU(mathbfa^T W_e mathbfe_jto i W mathbfx_i W mathbfx_j))\n\nArguments\n\nin: The dimension of input node features.\nein: The dimension of input edge features. Default 0 (i.e. no edge features passed in the forward).\nout: The dimension of output node features.\nσ: Activation function. Default identity.\nbias: Learn the additive bias if true. Default true.\nheads: Number attention heads. Default 1.\nconcat: Concatenate layer output or not. If not, layer output is averaged over the heads. Default true.\nnegative_slope: The parameter of LeakyReLU.Default 0.2.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default true.\ndropout: Dropout probability on the normalized attention coefficient. Default 0.0.\n\nExamples\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(Float32, 3, g.num_nodes)\n\n# create layer\nl = GATConv(in_channel => out_channel, add_self_loops = false, bias = false; heads=2, concat=true)\n\n# forward pass\ny = l(g, x)       \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GATv2Conv","page":"Convolutional layers","title":"GraphNeuralNetworks.GATv2Conv","text":"GATv2Conv(in => out, [σ; heads, concat, init, bias, negative_slope, add_self_loops])\nGATv2Conv((in, ein) => out, ...)\n\nGATv2 attentional layer from the paper How Attentive are Graph Attention Networks?.\n\nImplements the operation\n\nmathbfx_i = sum_j in N(i) cup i alpha_ij W_1 mathbfx_j\n\nwhere the attention coefficients alpha_ij are given by\n\nalpha_ij = frac1z_i exp(mathbfa^T LeakyReLU(W_2 mathbfx_i + W_1 mathbfx_j))\n\nwith z_i a normalization factor.\n\nIn case ein > 0 is given, edge features of dimension ein will be expected in the forward pass  and the attention coefficients will be calculated as  \n\nalpha_ij = frac1z_i exp(mathbfa^T LeakyReLU(W_3 mathbfe_jto i + W_2 mathbfx_i + W_1 mathbfx_j))\n\nArguments\n\nin: The dimension of input node features.\nein: The dimension of input edge features. Default 0 (i.e. no edge features passed in the forward).\nout: The dimension of output node features.\nσ: Activation function. Default identity.\nbias: Learn the additive bias if true. Default true.\nheads: Number attention heads. Default 1.\nconcat: Concatenate layer output or not. If not, layer output is averaged over the heads. Default true.\nnegative_slope: The parameter of LeakyReLU.Default 0.2.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default true.\ndropout: Dropout probability on the normalized attention coefficient. Default 0.0.\n\nExamples\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\nein = 3\ng = GNNGraph(s, t)\nx = randn(Float32, 3, g.num_nodes)\n\n# create layer\nl = GATv2Conv((in_channel, ein) => out_channel, add_self_loops = false)\n\n# edge features\ne = randn(Float32, ein, length(s))\n\n# forward pass\ny = l(g, x, e)    \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GCNConv","page":"Convolutional layers","title":"GraphNeuralNetworks.GCNConv","text":"GCNConv(in => out, σ=identity; [bias, init, add_self_loops, use_edge_weight])\n\nGraph convolutional layer from paper Semi-supervised Classification with Graph Convolutional Networks.\n\nPerforms the operation\n\nmathbfx_i = sum_jin N(i) a_ij W mathbfx_j\n\nwhere a_ij = 1  sqrtN(i)N(j) is a normalization factor computed from the node degrees. \n\nIf the input graph has weighted edges and use_edge_weight=true, than a_ij will be computed as\n\na_ij = frace_jto isqrtsum_j in N(i)  e_jto i sqrtsum_i in N(j)  e_ito j\n\nThe input to the layer is a node feature array X of size (num_features, num_nodes) and optionally an edge weight vector.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nσ: Activation function. Default identity.\nbias: Add learnable bias. Default true.\ninit: Weights' initializer. Default glorot_uniform.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default false.\nuse_edge_weight: If true, consider the edge weights in the input graph (if available).                    If add_self_loops=true the new weights will be set to 1.                     This option is ignored if the edge_weight is explicitly provided in the forward pass.                    Default false.\n\nForward\n\n(::GCNConv)(g::GNNGraph, x, edge_weight = nothing; norm_fn = d -> 1 ./ sqrt.(d), conv_weight = nothing) -> AbstractMatrix\n\nTakes as input a graph g, a node feature matrix x of size [in, num_nodes], and optionally an edge weight vector. Returns a node feature matrix of size  [out, num_nodes].\n\nThe norm_fn parameter allows for custom normalization of the graph convolution operation by passing a function as argument.  By default, it computes frac1sqrtd i.e the inverse square root of the degree (d) of each node in the graph.  If conv_weight is an AbstractMatrix of size [out, in], then the convolution is performed using that weight matrix instead of the weights stored in the model.\n\nExamples\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(Float32, 3, g.num_nodes)\n\n# create layer\nl = GCNConv(3 => 5) \n\n# forward pass\ny = l(g, x)       # size:  5 × num_nodes\n\n# convolution with edge weights and custom normalization function\nw = [1.1, 0.1, 2.3, 0.5]\ncustom_norm_fn(d) = 1 ./ sqrt.(d + 1)  # Custom normalization function\ny = l(g, x, w; norm_fn = custom_norm_fn)\n\n# Edge weights can also be embedded in the graph.\ng = GNNGraph(s, t, w)\nl = GCNConv(3 => 5, use_edge_weight=true) \ny = l(g, x) # same as l(g, x, w) \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GINConv","page":"Convolutional layers","title":"GraphNeuralNetworks.GINConv","text":"GINConv(f, ϵ; aggr=+)\n\nGraph Isomorphism convolutional layer from paper How Powerful are Graph Neural Networks?.\n\nImplements the graph convolution\n\nmathbfx_i = f_Thetaleft((1 + epsilon) mathbfx_i + sum_j in N(i) mathbfx_j right)\n\nwhere f_Theta typically denotes a learnable function, e.g. a linear layer or a multi-layer perceptron.\n\nArguments\n\nf: A (possibly learnable) function acting on node features. \nϵ: Weighting factor.\n\nExamples:\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\n\n# create dense layer\nnn = Dense(in_channel, out_channel)\n\n# create layer\nl = GINConv(nn, 0.01f0, aggr = mean)\n\n# forward pass\ny = l(g, x)  \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GMMConv","page":"Convolutional layers","title":"GraphNeuralNetworks.GMMConv","text":"GMMConv((in, ein) => out, σ=identity; K=1, bias=true, init=glorot_uniform, residual=false)\n\nGraph mixture model convolution layer from the paper Geometric deep learning on graphs and manifolds using mixture model CNNs Performs the operation\n\nmathbfx_i = mathbfx_i + frac1N(i) sum_jin N(i)frac1Ksum_k=1^K mathbfw_k(mathbfe_jto i) odot Theta_k mathbfx_j\n\nwhere w^a_k(e^a) for feature a and kernel k is given by\n\nw^a_k(e^a) = exp(-frac12(e^a - mu^a_k)^T (Sigma^-1)^a_k(e^a - mu^a_k))\n\nTheta_k mu^a_k (Sigma^-1)^a_k are learnable parameters.\n\nThe input to the layer is a node feature array x of size (num_features, num_nodes) and edge pseudo-coordinate array e of size (num_features, num_edges) The residual mathbfx_i is added only if residual=true and the output size is the same  as the input size.\n\nArguments\n\nin: Number of input node features.\nein: Number of input edge features.\nout: Number of output features.\nσ: Activation function. Default identity.\nK: Number of kernels. Default 1.\nbias: Add learnable bias. Default true.\ninit: Weights' initializer. Default glorot_uniform.\nresidual: Residual conncetion. Default false.\n\nExamples\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s,t)\nnin, ein, out, K = 4, 10, 7, 8 \nx = randn(Float32, nin, g.num_nodes)\ne = randn(Float32, ein, g.num_edges)\n\n# create layer\nl = GMMConv((nin, ein) => out, K=K)\n\n# forward pass\nl(g, x, e)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GatedGraphConv","page":"Convolutional layers","title":"GraphNeuralNetworks.GatedGraphConv","text":"GatedGraphConv(out, num_layers; aggr=+, init=glorot_uniform)\n\nGated graph convolution layer from Gated Graph Sequence Neural Networks.\n\nImplements the recursion\n\nbeginaligned\nmathbfh^(0)_i = mathbfx_i mathbf0 \nmathbfh^(l)_i = GRU(mathbfh^(l-1)_i square_j in N(i) W mathbfh^(l-1)_j)\nendaligned\n\nwhere mathbfh^(l)_i denotes the l-th hidden variables passing through GRU. The dimension of input mathbfx_i needs to be less or equal to out.\n\nArguments\n\nout: The dimension of output features.\nnum_layers: The number of recursion steps.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\ninit: Weight initialization function.\n\nExamples:\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nout_channel = 5\nnum_layers = 3\ng = GNNGraph(s, t)\n\n# create layer\nl = GatedGraphConv(out_channel, num_layers)\n\n# forward pass\ny = l(g, x)   \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.GraphConv","page":"Convolutional layers","title":"GraphNeuralNetworks.GraphConv","text":"GraphConv(in => out, σ=identity; aggr=+, bias=true, init=glorot_uniform)\n\nGraph convolution layer from Reference: Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks.\n\nPerforms:\n\nmathbfx_i = W_1 mathbfx_i + square_j in mathcalN(i) W_2 mathbfx_j\n\nwhere the aggregation type is selected by aggr.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nσ: Activation function.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\nbias: Add learnable bias.\ninit: Weights' initializer.\n\nExamples\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(Float32, 3, g.num_nodes)\n\n# create layer\nl = GraphConv(in_channel => out_channel, relu, bias = false, aggr = mean)\n\n# forward pass\ny = l(g, x)       \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.MEGNetConv","page":"Convolutional layers","title":"GraphNeuralNetworks.MEGNetConv","text":"MEGNetConv(ϕe, ϕv; aggr=mean)\nMEGNetConv(in => out; aggr=mean)\n\nConvolution from Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals paper. In the forward pass, takes as inputs node features x and edge features e and returns updated features x' and e' according to \n\nbeginaligned\nmathbfe_ito j  = phi_e(mathbfx_i  mathbfx_j  mathbfe_ito j)\nmathbfx_i  = phi_v(mathbfx_i square_jin mathcalN(i)mathbfe_jto i)\nendaligned\n\naggr defines the aggregation to be performed.\n\nIf the neural networks ϕe and  ϕv are not provided, they will be constructed from the in and out arguments instead as multi-layer perceptron with one hidden layer and relu  activations.\n\nExamples\n\ng = rand_graph(10, 30)\nx = randn(Float32, 3, 10)\ne = randn(Float32, 3, 30)\nm = MEGNetConv(3 => 3)\nx′, e′ = m(g, x, e)\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.NNConv","page":"Convolutional layers","title":"GraphNeuralNetworks.NNConv","text":"NNConv(in => out, f, σ=identity; aggr=+, bias=true, init=glorot_uniform)\n\nThe continuous kernel-based convolutional operator from the  Neural Message Passing for Quantum Chemistry paper.  This convolution is also known as the edge-conditioned convolution from the  Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs paper.\n\nPerforms the operation\n\nmathbfx_i = W mathbfx_i + square_j in N(i) f_Theta(mathbfe_jto i)mathbfx_j\n\nwhere f_Theta  denotes a learnable function (e.g. a linear layer or a multi-layer perceptron). Given an input of batched edge features e of size (num_edge_features, num_edges),  the function f will return an batched matrices array whose size is (out, in, num_edges). For convenience, also functions returning a single (out*in, num_edges) matrix are allowed.\n\nArguments\n\nin: The dimension of input node features.\nout: The dimension of output node features.\nf: A (possibly learnable) function acting on edge features.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\nσ: Activation function.\nbias: Add learnable bias.\ninit: Weights' initializer.\n\nExamples:\n\nn_in = 3\nn_in_edge = 10\nn_out = 5\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\n\n# create dense layer\nnn = Dense(n_in_edge => n_out * n_in)\n\n# create layer\nl = NNConv(n_in => n_out, nn, tanh, bias = true, aggr = +)\n\nx = randn(Float32, n_in, g.num_nodes)\ne = randn(Float32, n_in_edge, g.num_edges)\n\n# forward pass\ny = l(g, x, e)  \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.ResGatedGraphConv","page":"Convolutional layers","title":"GraphNeuralNetworks.ResGatedGraphConv","text":"ResGatedGraphConv(in => out, act=identity; init=glorot_uniform, bias=true)\n\nThe residual gated graph convolutional operator from the Residual Gated Graph ConvNets paper.\n\nThe layer's forward pass is given by\n\nmathbfx_i = actbig(Umathbfx_i + sum_j in N(i) eta_ij V mathbfx_jbig)\n\nwhere the edge gates eta_ij are given by\n\neta_ij = sigmoid(Amathbfx_i + Bmathbfx_j)\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nact: Activation function.\ninit: Weight matrices' initializing function. \nbias: Learn an additive bias if true.\n\nExamples:\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\n\n# create layer\nl = ResGatedGraphConv(in_channel => out_channel, tanh, bias = true)\n\n# forward pass\ny = l(g, x)  \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.SAGEConv","page":"Convolutional layers","title":"GraphNeuralNetworks.SAGEConv","text":"SAGEConv(in => out, σ=identity; aggr=mean, bias=true, init=glorot_uniform)\n\nGraphSAGE convolution layer from paper Inductive Representation Learning on Large Graphs.\n\nPerforms:\n\nmathbfx_i = W cdot mathbfx_i square_j in mathcalN(i) mathbfx_j\n\nwhere the aggregation type is selected by aggr.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nσ: Activation function.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\nbias: Add learnable bias.\ninit: Weights' initializer.\n\nExamples:\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\n\n# create layer\nl = SAGEConv(in_channel => out_channel, tanh, bias = false, aggr = +)\n\n# forward pass\ny = l(g, x)   \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.SGConv","page":"Convolutional layers","title":"GraphNeuralNetworks.SGConv","text":"SGConv(int => out, k=1; [bias, init, add_self_loops, use_edge_weight])\n\nSGC layer from Simplifying Graph Convolutional Networks Performs operation\n\nH^K = (tildeD^-12 tildeA tildeD^-12)^K X Theta\n\nwhere tildeA is A + I.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nk : Number of hops k. Default 1.\nbias: Add learnable bias. Default true.\ninit: Weights' initializer. Default glorot_uniform.\nadd_self_loops: Add self loops to the graph before performing the convolution. Default false.\nuse_edge_weight: If true, consider the edge weights in the input graph (if available).                    If add_self_loops=true the new weights will be set to 1. Default false.\n\nExamples\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(Float32, 3, g.num_nodes)\n\n# create layer\nl = SGConv(3 => 5; add_self_loops = true) \n\n# forward pass\ny = l(g, x)       # size:  5 × num_nodes\n\n# convolution with edge weights\nw = [1.1, 0.1, 2.3, 0.5]\ny = l(g, x, w)\n\n# Edge weights can also be embedded in the graph.\ng = GNNGraph(s, t, w)\nl = SGConv(3 => 5, add_self_loops = true, use_edge_weight=true) \ny = l(g, x) # same as l(g, x, w) \n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.TAGConv","page":"Convolutional layers","title":"GraphNeuralNetworks.TAGConv","text":"TAGConv(in => out, k=3; bias=true, init=glorot_uniform, add_self_loops=true, use_edge_weight=false)\n\nTAGConv layer from Topology Adaptive Graph Convolutional Networks. This layer extends the idea of graph convolutions by applying filters that adapt to the topology of the data.  It performs the operation:\n\nH^K = sum_k=0^K (D^-12 A D^-12)^k X Theta_k\n\nwhere A is the adjacency matrix of the graph, D is the degree matrix, X is the input feature matrix, and Theta_k is a unique weight matrix for each hop k.\n\nArguments\n\nin: Number of input features.\nout: Number of output features.\nk: Maximum number of hops to consider. Default is 3.\nbias: Whether to include a learnable bias term. Default is true.\ninit: Initialization function for the weights. Default is glorot_uniform.\nadd_self_loops: Whether to add self-loops to the adjacency matrix. Default is true.\nuse_edge_weight: If true, edge weights are considered in the computation (if available). Default is false.\n\nExamples\n\n# Example graph data\ns = [1, 1, 2, 3]\nt = [2, 3, 1, 1]\ng = GNNGraph(s, t)  # Create a graph\nx = randn(Float32, 3, g.num_nodes)  # Random features for each node\n\n# Create a TAGConv layer\nl = TAGConv(3 => 5, k=3; add_self_loops=true)\n\n# Apply the TAGConv layer\ny = l(g, x)  # Output size: 5 × num_nodes\n\n\n\n\n\n","category":"type"},{"location":"api/conv/#GraphNeuralNetworks.TransformerConv","page":"Convolutional layers","title":"GraphNeuralNetworks.TransformerConv","text":"TransformerConv((in, ein) => out; [heads, concat, init, add_self_loops, bias_qkv,\n    bias_root, root_weight, gating, skip_connection, batch_norm, ff_channels]))\n\nThe transformer-like multi head attention convolutional operator from the  Masked Label Prediction: Unified Message Passing Model for Semi-Supervised  Classification paper, which also considers  edge features. It further contains options to also be configured as the transformer-like convolutional operator from the  Attention, Learn to Solve Routing Problems! paper, including a successive feed-forward network as well as skip layers and batch normalization.\n\nThe layer's basic forward pass is given by\n\nx_i = W_1x_i + sum_jin N(i) alpha_ij (W_2 x_j + W_6e_ij)\n\nwhere the attention scores are\n\nalpha_ij = mathrmsoftmaxleft(frac(W_3x_i)^T(W_4x_j+\nW_6e_ij)sqrtdright)\n\nOptionally, a combination of the aggregated value with transformed root node features  by a gating mechanism via\n\nx_i = beta_i W_1 x_i + (1 - beta_i) underbraceleft(sum_j in mathcalN(i)\nalpha_ij W_2 x_j right)_=m_i\n\nwith\n\nbeta_i = textrmsigmoid(W_5^top  W_1 x_i m_i W_1 x_i - m_i )\n\ncan be performed.\n\nArguments\n\nin: Dimension of input features, which also corresponds to the dimension of    the output features.\nein: Dimension of the edge features; if 0, no edge features will be used.\nout: Dimension of the output.\nheads: Number of heads in output. Default 1.\nconcat: Concatenate layer output or not. If not, layer output is averaged   over the heads. Default true.\ninit: Weight matrices' initializing function. Default glorot_uniform.\nadd_self_loops: Add self loops to the input graph. Default false.\nbias_qkv: If set, bias is used in the key, query and value transformations for nodes.   Default true.\nbias_root: If set, the layer will also learn an additive bias for the root when root    weight is used. Default true.\nroot_weight: If set, the layer will add the transformed root node features   to the output. Default true.\ngating: If set, will combine aggregation and transformed root node features by a   gating mechanism. Default false.\nskip_connection: If set, a skip connection will be made from the input and    added to the output. Default false.\nbatch_norm: If set, a batch normalization will be applied to the output. Default false.\nff_channels: If positive, a feed-forward NN is appended, with the first having the given   number of hidden nodes; this NN also gets a skip connection and batch normalization    if the respective parameters are set. Default: 0.\n\nExamples\n\nN, in_channel, out_channel = 4, 3, 5\nein, heads = 2, 3\ng = GNNGraph([1,1,2,4], [2,3,1,1])\nl = TransformerConv((in_channel, ein) => in_channel; heads, gating = true, bias_qkv = true)\nx = rand(Float32, in_channel, N)\ne = rand(Float32, ein, g.num_edges)\nl(g, x, e)\n\n\n\n\n\n","category":"type"},{"location":"#GraphNeuralNetworks-Monorepo","page":"Home","title":"GraphNeuralNetworks Monorepo","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is the documentation page for GraphNeuralNetworks.jl, a graph neural network library written in Julia and based on the deep learning framework Flux.jl. GraphNeuralNetworks.jl is largely inspired by PyTorch Geometric, Deep Graph Library, and GeometricFlux.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"GraphNeuralNetwork.jl: Package that contains stateful graph convolutional layers based on the machine learning framework Flux.jl. This is fronted package for Flux users. It depends on GNNlib.jl, GNNGraphs.jl, and Flux.jl packages.\nImplements common graph convolutional layers.\nSupports computations on batched graphs. \nEasy to define custom layers.\nCUDA support.\nIntegration with Graphs.jl.\nExamples of node, edge, and graph level machine learning tasks. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Usage examples on real datasets can be found in the examples folder. ","category":"page"}]
}
